<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.450">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Ming Zhao">
<meta name="dcterms.date" content="2024-08-02">

<title>Neural Networks</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
q { quotes: "“" "”" "‘" "’"; }
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="Intro_to_Neural_Networks_files/libs/clipboard/clipboard.min.js"></script>
<script src="Intro_to_Neural_Networks_files/libs/quarto-html/quarto.js"></script>
<script src="Intro_to_Neural_Networks_files/libs/quarto-html/popper.min.js"></script>
<script src="Intro_to_Neural_Networks_files/libs/quarto-html/tippy.umd.min.js"></script>
<script src="Intro_to_Neural_Networks_files/libs/quarto-html/anchor.min.js"></script>
<link href="Intro_to_Neural_Networks_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="Intro_to_Neural_Networks_files/libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="Intro_to_Neural_Networks_files/libs/bootstrap/bootstrap.min.js"></script>
<link href="Intro_to_Neural_Networks_files/libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="Intro_to_Neural_Networks_files/libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" integrity="sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script>
<script type="application/javascript">define('jquery', [],function() {return window.jQuery;})</script>

  <script>window.backupDefine = window.define; window.define = undefined;</script><script src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js"></script>
  <script>document.addEventListener("DOMContentLoaded", function () {
 var mathElements = document.getElementsByClassName("math");
 var macros = [];
 for (var i = 0; i < mathElements.length; i++) {
  var texText = mathElements[i].firstChild;
  if (mathElements[i].tagName == "SPAN") {
   katex.render(texText.data, mathElements[i], {
    displayMode: mathElements[i].classList.contains('display'),
    throwOnError: false,
    macros: macros,
    fleqn: false
   });
}}});
  </script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css">

</head>

<body>

<div id="quarto-content" class="page-columns page-rows-contents page-layout-article toc-left">
<div id="quarto-sidebar-toc-left" class="sidebar toc-left">
  <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#introduction" id="toc-introduction" class="nav-link active" data-scroll-target="#introduction">Introduction</a></li>
  <li><a href="#fundamentals-of-neural-networks" id="toc-fundamentals-of-neural-networks" class="nav-link" data-scroll-target="#fundamentals-of-neural-networks">Fundamentals of Neural Networks</a>
  <ul class="collapse">
  <li><a href="#tensor-where-it-starts" id="toc-tensor-where-it-starts" class="nav-link" data-scroll-target="#tensor-where-it-starts">Tensor: where it starts</a></li>
  <li><a href="#layers-the-building-blocks-of-deep-learning" id="toc-layers-the-building-blocks-of-deep-learning" class="nav-link" data-scroll-target="#layers-the-building-blocks-of-deep-learning">Layers: the building blocks of deep learning</a></li>
  <li><a href="#activation-functions-beyond-the-linearity" id="toc-activation-functions-beyond-the-linearity" class="nav-link" data-scroll-target="#activation-functions-beyond-the-linearity">Activation functions: beyond the linearity</a></li>
  <li><a href="#loss-function-and-optimizer-less-is-what-we-want" id="toc-loss-function-and-optimizer-less-is-what-we-want" class="nav-link" data-scroll-target="#loss-function-and-optimizer-less-is-what-we-want">Loss function and optimizer: less is what we want</a></li>
  <li><a href="#models-networks-of-layers" id="toc-models-networks-of-layers" class="nav-link" data-scroll-target="#models-networks-of-layers">Models: networks of layers</a></li>
  <li><a href="#anatomy-of-a-neural-network" id="toc-anatomy-of-a-neural-network" class="nav-link" data-scroll-target="#anatomy-of-a-neural-network">Anatomy of a neural network</a></li>
  </ul></li>
  <li><a href="#exploring-neural-network" id="toc-exploring-neural-network" class="nav-link" data-scroll-target="#exploring-neural-network">Exploring Neural Network</a>
  <ul class="collapse">
  <li><a href="#data-preparation" id="toc-data-preparation" class="nav-link" data-scroll-target="#data-preparation">Data Preparation</a></li>
  <li><a href="#network-architecture" id="toc-network-architecture" class="nav-link" data-scroll-target="#network-architecture">Network Architecture</a></li>
  <li><a href="#compilation" id="toc-compilation" class="nav-link" data-scroll-target="#compilation">Compilation</a></li>
  <li><a href="#training-the-network" id="toc-training-the-network" class="nav-link" data-scroll-target="#training-the-network">Training the Network</a></li>
  <li><a href="#the-model" id="toc-the-model" class="nav-link" data-scroll-target="#the-model">The Model</a></li>
  </ul></li>
  </ul>
</nav>
</div>
<div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
</div>
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Neural Networks</h1>
</div>



<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Ming Zhao </p>
          </div>
  </div>
    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">August 2, 2024</p>
    </div>
  </div>
  
    
  </div>
  

</header>

<p>
<a href="https://colab.research.google.com/drive/14CECv-8O2u4HXnzv3KXJtZnrgnVs2s3Y" target="_blank"><img src="https://camo.githubusercontent.com/f5e0d0538a9c2972b5d413e0ace04cecd8efd828d133133933dfffec282a4e1b/68747470733a2f2f636f6c61622e72657365617263682e676f6f676c652e636f6d2f6173736574732f636f6c61622d62616467652e737667" alt="Colab" data-canonical-src="https://colab.research.google.com/assets/colab-badge.svg" style="max-width: 100%;"></a>
</p>
<section id="introduction" class="level1">
<h1>Introduction</h1>
<p><strong>Introduction to Artificial Intelligence</strong> Artificial intelligence (AI) has captured widespread attention, frequently discussed in various media beyond just technical outlets. The journey of AI began in the 1950s with computer scientists exploring the potential for computers to <q>think.</q> AI aims to automate tasks traditionally performed by humans, encompassing several approaches including both learning and non-learning methods.</p>
<p><strong>Early AI and Symbolic AI</strong> Initially, AI was dominated by Symbolic AI up until the late 1980s. This approach involved programmers creating explicit sets of rules to manipulate knowledge. However, Symbolic AI struggled with complex problems due to its inflexibility and limited scope, leading to the exploration of alternative methods.</p>
<p><span class="math display">\begin{align*}
\left.
\begin{aligned}
\text{Rules} \longrightarrow \\
\text{Data} \longrightarrow
\end{aligned} \right|
&amp;\text{Classical programming} \longrightarrow \text{Answer} \\
\\
\left.
\begin{aligned}
\text{Data} \longrightarrow \\
\text{Answer} \longrightarrow
\end{aligned} \right|
&amp;\text{Machine learning} \longrightarrow \text{Rules}
\end{align*}</span></p>
<p><strong>Rise of Machine Learning</strong> As the limitations of Symbolic AI became apparent, machine learning emerged as a new paradigm. Unlike traditional programming, which relies on hard-coded rules, machine learning enables computers to learn from data. This shift allows systems to develop their own rules by finding patterns in data, which is more effective for tasks with complex or less defined rules. The success of machine learning has been propelled by advances in computing power and the availability of large datasets.</p>
<p><strong>Introduction and Explanation of Deep Learning</strong> Within machine learning, deep learning represents a further specialization, characterized by models that process data through many layers (hence <q>deep</q>) of neural networks. These layers construct increasingly abstract representations of data, which are not explicitly programmed but learned from vast amounts of training data. Deep learning excels at identifying intricate structures in high-dimensional data, making it a powerful tool for many AI applications.</p>
<p><strong>Educational Approach</strong> Our educational approach focuses on building intuitive understanding of these concepts without delving too deeply into technical complexities. This strategy is designed to help learners grasp the fundamental principles of deep learning and apply them to real-world problems effectively.</p>
</section>
<section id="fundamentals-of-neural-networks" class="level1">
<h1>Fundamentals of Neural Networks</h1>
<p>Let’s begin by exploring a simple feedforward neural network with three layers: an input layer, a hidden layer (with two nodes or neurons), and an output layer, detailed in the <a href="https://github.com/ming-zhao/ming-zhao.github.io/blob/master/AIML_for_Business/res/Neural_Networks/data/SimpleNeuralNetwork.xlsx?raw=true">Excel file</a>. The Excel model demonstrates how even a straightforward neural network, like this one, can exhibit nonlinear behavior based on the input value changes. This characteristic is fundamental to understanding how neural networks can model complex functions despite their seemingly simple structures.</p>
<p>The network’s structure is illustrated in <a href="#fig-simpleNN">Figure&nbsp;1</a> below.</p>
<div id="fig-simpleNN" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<center>
<img src="https://github.com/ming-zhao/ming-zhao.github.io/raw/master/AIML_for_Business/res/Neural_Networks/figures/simpleNN.png" width="700" class="figure-img">
</center>
<figcaption class="figure-caption">Figure&nbsp;1: Neural Network in Excel</figcaption>
</figure>
</div>
<p>The tabset below provides detailed explanations of the neural network’s layers.</p>
<div class="tabset-margin-container"></div><div class="panel-tabset">
<ul class="nav nav-tabs" role="tablist"><li class="nav-item" role="presentation"><a class="nav-link active" id="tabset-1-1-tab" data-bs-toggle="tab" data-bs-target="#tabset-1-1" role="tab" aria-controls="tabset-1-1" aria-selected="true" href="">Input Layer</a></li><li class="nav-item" role="presentation"><a class="nav-link" id="tabset-1-2-tab" data-bs-toggle="tab" data-bs-target="#tabset-1-2" role="tab" aria-controls="tabset-1-2" aria-selected="false" href="">Hidden Layer</a></li><li class="nav-item" role="presentation"><a class="nav-link" id="tabset-1-3-tab" data-bs-toggle="tab" data-bs-target="#tabset-1-3" role="tab" aria-controls="tabset-1-3" aria-selected="false" href="">Output Layer</a></li></ul>
<div class="tab-content">
<div id="tabset-1-1" class="tab-pane active" role="tabpanel" aria-labelledby="tabset-1-1-tab">
<p>It consists of a single node represented as <q>input x</q>, which represents the input data for the network.</p>
</div>
<div id="tabset-1-2" class="tab-pane" role="tabpanel" aria-labelledby="tabset-1-2-tab">
<ul>
<li>Two neurons comprise this layer. The input node connects to these neurons with weights <code>w11</code> and <code>w12</code>. These weights determine the input’s influence on the hidden neurons.</li>
<li>Each neuron in the hidden layer includes a bias term (<code>b11</code> and <code>b12</code>), which is added to the weighted input before applying the activation function.</li>
<li>The neurons utilize a logistic activation function, symbolized by <code>L</code>, to introduce non-linearity. The outcomes of this layer are <code>z1 = L(w11*x + b11)</code> and <code>z2 = L(w12*x + b12)</code>.</li>
</ul>
</div>
<div id="tabset-1-3" class="tab-pane" role="tabpanel" aria-labelledby="tabset-1-3-tab">
<ul>
<li>The outputs <code>z1</code> and <code>z2</code> from the hidden layer feed into the output node, with weights <code>w10</code> and <code>w20</code>.</li>
<li>An additional bias <code>b</code> is incorporated before the logistic function is applied to generate the final output <code>y = L(w10*z1 + w20*z2 + b)</code>.</li>
</ul>
<p>The network’s final output <code>y</code> is the culmination of processing the input <code>x</code> through the layers, with each neuron outputting the logistic function’s result when applied to the sum of its weighted inputs and bias. This structure allows the neural network to approximate non-linear functions and make predictions or classifications based on the input <code>x</code>.</p>
</div>
</div>
</div>
<section id="tensor-where-it-starts" class="level2">
<h2 class="anchored" data-anchor-id="tensor-where-it-starts">Tensor: where it starts</h2>
<p>Tensors serve as the foundational data structures for neural networks, where they are better known as multidimensional arrays. The essence of a tensor is characterized by its dimensionality, which directly corresponds to the number of indices required to pinpoint a specific scalar value within its structure. In simpler terms, tensors can be visualized across several dimensions:</p>
<div id="fig-tensor" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<center>
<img src="https://github.com/ming-zhao/ming-zhao.github.io/raw/master/AIML_for_Business/res/Neural_Networks/figures/tensor.png" width="500" class="figure-img">
</center>
<figcaption class="figure-caption">Figure&nbsp;2: Tensor</figcaption>
</figure>
</div>
<ul>
<li><strong>Scalars</strong>, which are zero-dimensional tensors, represent singular values without any axes.</li>
<li><strong>Vectors</strong> represent one-dimensional tensors, akin to a straight line of elements, each identifiable by a single index.</li>
<li><strong>Matrices</strong> take the form of two-dimensional tensors, arrayed in rows and columns, requiring two indices for location identification.</li>
</ul>
<p>To contextualize the utility of tensors in real-world applications, consider the following examples:</p>
<ul>
<li><strong>Vector data</strong> in machine learning often appears as 2D tensors with a shape denoted by (samples, features), where each sample is a vector of features.</li>
<li><strong>Timeseries or sequence data</strong> are typically represented as 3D tensors shaped (samples, timesteps, features), useful in applications like stock price predictions or speech recognition where the sequence of data points is crucial.</li>
<li><strong>Image processing</strong> uses 4D tensors to store data, commonly shaped (samples, height, width, channels) or (samples, channels, height, width), depending on the format preferred, which effectively captures the spatial hierarchy and coloring of multiple images in a batch.</li>
<li><strong>Video data</strong> extends this concept into 5D tensors, either shaped (samples, frames, height, width, channels) or (samples, frames, channels, height, width), accommodating multiple frames per sample to analyze movements and changes over time.</li>
</ul>
<p>Through these structures, tensors allow neural networks to interpret and process a wide range of complex data types systematically and efficiently, making them indispensable in the field of deep learning.</p>
</section>
<section id="layers-the-building-blocks-of-deep-learning" class="level2">
<h2 class="anchored" data-anchor-id="layers-the-building-blocks-of-deep-learning">Layers: the building blocks of deep learning</h2>
<p>A layer in a neural network comprises numerous neurons or nodes, each designed to identify and learn specific features from the input data. These layers sequentially process input tensors, transforming them into output tensors that reflect increasingly abstract representations of the data, as shown in the figure.</p>
<div id="fig-layers_and_weights" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<center>
<img src="https://github.com/ming-zhao/ming-zhao.github.io/raw/master/AIML_for_Business/res/Neural_Networks/figures/NN_layers.jpg" width="500" class="figure-img">
</center>
<figcaption class="figure-caption">Figure&nbsp;3: Layers and Weights</figcaption>
</figure>
</div>
<p>The efficacy of each layer hinges on its weights—denoted as <span class="math inline">w_i</span> in our simplified Excel model. These weights are pivotal as they adjust the influence of input features on the output during the learning process. Through training, the network fine-tunes these weights to minimize errors and enhance performance, enabling the neural network to derive meaningful patterns and insights from complex data.</p>
<p>Ultimately, each neural network layer acts as a sophisticated data-processing module, essential for extracting features that inform the network’s final output, which is used to make predictions or classifications.</p>
</section>
<section id="activation-functions-beyond-the-linearity" class="level2">
<h2 class="anchored" data-anchor-id="activation-functions-beyond-the-linearity">Activation functions: beyond the linearity</h2>
<p>Many operations in the neural networks is expressed as a linear transformation—scaling and offset—followed by an activation function, e.g., <code>z1 = L(w11*x + b11)</code> in the Excel model, where <code>L</code> is a logistic activation function</p>
<div id="fig-activation" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<center>
<img src="https://github.com/ming-zhao/ming-zhao.github.io/raw/master/AIML_for_Business/res/Neural_Networks/figures/activation.jpg" width="700" class="figure-img">
</center>
<figcaption class="figure-caption">Figure&nbsp;4: Activation Function</figcaption>
</figure>
</div>
<p>The linear transformation is typically conducted as matrix multiplication, providing the groundwork for more complex operations.</p>
<p>Activation functions are critical and may sound complex; however, the current go-to function is the ReLU, or rectified linear unit. Despite its intimidating name, ReLU is simply <span class="math inline">\max(0,x)</span>, outputting zero for negative inputs and the input value itself for non-negative inputs. The accompanying graphs depict common activation functions and demonstrate how they modify input values into outputs.</p>
<div id="fig-examples_of_activation_funcs" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<center>
<img src="https://github.com/ming-zhao/ming-zhao.github.io/raw/master/AIML_for_Business/res/Neural_Networks/figures/activation2.jpg" width="600" class="figure-img">
</center>
<figcaption class="figure-caption">Figure&nbsp;5: Examples of Activation Functions</figcaption>
</figure>
</div>
<p>Another very important activation function <em>softmax</em>, which is slightly more complex mathematically. It outputs a set of values ranging between 0 and 1, summing to 1 (probabilities!). Think of the softmax function like a talent show judge who must decide which performer stands out the most. It looks at all the scores (or values) and boosts the highest one so it’s clear who the winner is. In a neural network that’s trying to categorize things, the softmax function helps pick the most likely category the network believes the input belongs to.</p>
<p>Without activation functions, a neural network would be nothing more than a linear regression model, unable to capture the nonlinearity inherent in complex data relationships. Effective activation functions share a common trait:</p>
<ul>
<li>They possess at least one responsive range, wherein significant changes to the input yield substantial changes in the output—crucial for the network’s learning phase.</li>
<li>Many also have a less responsive or saturated range, where input variations have minimal impact on the output.</li>
</ul>
</section>
<section id="loss-function-and-optimizer-less-is-what-we-want" class="level2">
<h2 class="anchored" data-anchor-id="loss-function-and-optimizer-less-is-what-we-want">Loss function and optimizer: less is what we want</h2>
<p>A loss function (or cost function) is a function that computes a single numerical value that the learning process will attempt to minimize. A loss function is like a scoreboard for our neural network game. It keeps track of the score to help the network get better at making predictions. It does this by looking at what the network predicts and comparing it to the correct answers, seeing how far off it is.</p>
<p>The optimizer determines how the network will be updated based on the loss function. It implements a specific variant of stochastic gradient descent (SGD). An optimizer is like a coach. It uses the score from the loss function to guide the network on how to change its strategy (or update its weights) to play the game better next time, aiming to get a higher score by making more accurate predictions.</p>
</section>
<section id="models-networks-of-layers" class="level2">
<h2 class="anchored" data-anchor-id="models-networks-of-layers">Models: networks of layers</h2>
<p>A deep learning model is like a complex web made up of many layers. Each layer has a bunch of tiny processing units called nodes or neurons. In a certain type of network called a feedforward neural network (FNN), information travels in just one direction, forward, from the input layer, through the hidden layers, to the output layer. FNNs are great for sorting things into categories (classification) and making predictions (regression).</p>
<p>On the other hand, a recurrent neural network (RNN) works differently. It has special loops in its connections that let it remember past information. This memory helps RNNs handle data that follows a sequence, like sentences in a conversation, music notes in a song, or stock prices over time. Because of this memory, RNNs can understand the sequence and context, which is really useful for making predictions in tasks like language translation or stock price forecasting.</p>
<p>In essence, the way a deep learning model is built - whether it’s an FNN or an RNN - matters a lot because it decides what the model can do best.</p>
</section>
<section id="anatomy-of-a-neural-network" class="level2">
<h2 class="anchored" data-anchor-id="anatomy-of-a-neural-network">Anatomy of a neural network</h2>
<p>We examine a simple neural network featuring 3 hidden layers. This network is trained using various scattered input points derived from a set of functions, such as <span class="math inline">x^2</span>, <span class="math inline">\sin(x)</span>, <span class="math inline">\text{abs}(x)</span> and <span class="math inline">\text{heaviside}(x)</span>, which are displayed on the left side of the figure below. The network’s output, which consists of the predicted functions based on the learning, is depicted on the right side of the figure. The central part of the figure illustrates the neural network’s architecture.</p>
<div id="fig-anatomy" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<center>
<img src="https://github.com/ming-zhao/ming-zhao.github.io/raw/master/AIML_for_Business/res/Neural_Networks/figures/hidden.jpg" width="600" class="figure-img">
</center>
<figcaption class="figure-caption">Figure&nbsp;6: Anatomy of a Neural Network</figcaption>
</figure>
</div>
<p><a href="#fig-hidden_layers">Figure&nbsp;7</a> displays the features captured by the hidden layers. Essentially, the neural network learns from the blue dots (denoted as <q>func</q> in the figure legend) and infers that the final curve (represented by the red solid line, or the <q>predicted curve</q>) is a weighted sum of three dashed curves generated by the hidden layers. It is observed that the predicted curve aligns closely with the true function, indicating that the neural network effectively learns from the scattered input points.</p>
<div class="cell" data-outputid="9c8f2d12-7576-47f1-ccea-2fc4d65766e0" data-execution_count="2">
<div class="cell-output cell-output-display">
<div id="fig-hidden_layers" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="Intro_to_Neural_Networks_files/figure-html/fig-hidden_layers-output-1.png" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">Figure&nbsp;7: Hidden Layers</figcaption>
</figure>
</div>
</div>
</div>
</section>
</section>
<section id="exploring-neural-network" class="level1">
<h1>Exploring Neural Network</h1>
<p>In this section, we explore neural networks using the MNIST dataset. Our objective is to build a simple Feedforward Neural Network (FNN) that can recognize digits from images. The MNIST dataset is a widely-used benchmark for image classification tasks. It comprises 60,000 training images and 10,000 test images of handwritten digits ranging from 0 to 9. Each image is in grayscale and has a resolution of 28x28 pixels.</p>
<div class="cell" data-outputid="c9913d5e-5bee-44ae-dbad-2975c19e4955" data-execution_count="2">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> tensorflow.keras.datasets <span class="im">import</span> mnist</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>(train_images, train_labels), (test_images, test_labels) <span class="op">=</span> mnist.load_data()</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'training images:</span><span class="sc">{}</span><span class="st">, test images:</span><span class="sc">{}</span><span class="st">'</span>.<span class="bu">format</span>(train_images.shape, test_images.shape))</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> showimg(data):</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>    idx  <span class="op">=</span> <span class="dv">3</span></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>    span <span class="op">=</span> <span class="dv">5</span></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> data<span class="op">==</span><span class="st">'train'</span>:</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>        images <span class="op">=</span> train_images</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>        labels <span class="op">=</span> train_labels</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> data<span class="op">==</span><span class="st">'test'</span>:</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>        images <span class="op">=</span> test_images</span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>        labels <span class="op">=</span> test_labels</span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>    plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>,<span class="dv">2</span>))</span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">5</span>):</span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a>        plt.subplot(<span class="dv">1</span>, <span class="dv">5</span>, i <span class="op">+</span> <span class="dv">1</span>)</span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a>        digit <span class="op">=</span> images[idx<span class="op">+</span>i]</span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a>        plt.imshow(digit, cmap<span class="op">=</span>plt.cm.binary)</span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a>        plt.title(<span class="st">'Index:</span><span class="sc">{}</span><span class="st">, Label:</span><span class="sc">{}</span><span class="st">'</span>.<span class="bu">format</span>(idx<span class="op">+</span>i, labels[idx<span class="op">+</span>i]), fontsize <span class="op">=</span> <span class="dv">12</span>)</span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a>    plt.show()</span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a>showimg(<span class="st">'train'</span>)</span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a>showimg(<span class="st">'test'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>training images:(60000, 28, 28), test images:(10000, 28, 28)</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="Intro_to_Neural_Networks_files/figure-html/cell-3-output-2.png" class="img-fluid"></p>
</div>
<div class="cell-output cell-output-display">
<p><img src="Intro_to_Neural_Networks_files/figure-html/cell-3-output-3.png" class="img-fluid"></p>
</div>
</div>
<p>The first row displays five images from the training dataset, while the second row features five images from the testing dataset. Each image is marked with an index that shows its position within the respective training or testing set, along with a label indicating the digit the image represents. Our objective is to learn how these labels correspond to the images using the training set and then use this understanding to accurately predict the labels for each image in the testing set.</p>
<p>To facilitate a clear demonstration, we have chosen to use Python package Keras for constructing our neural network. Keras is renowned for its simple and intuitive coding structure, making it an ideal choice for this task. Our aim is to classify each image into one of ten categories, numbered 0 through 9.</p>
<section id="data-preparation" class="level2">
<h2 class="anchored" data-anchor-id="data-preparation">Data Preparation</h2>
<p>To feed our neural network the right kind of data, we need to reshape it first:</p>
<div id="fig-data_reshape" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<center>
<img src="https://github.com/ming-zhao/ming-zhao.github.io/raw/master/AIML_for_Business/res/Neural_Networks/figures/mnist_1.jpg" width="700" class="figure-img">
</center>
<figcaption class="figure-caption">Figure&nbsp;8: Data Reshape</figcaption>
</figure>
</div>
<div class="tabset-margin-container"></div><div class="panel-tabset">
<ul class="nav nav-tabs" role="tablist"><li class="nav-item" role="presentation"><a class="nav-link active" id="tabset-2-1-tab" data-bs-toggle="tab" data-bs-target="#tabset-2-1" role="tab" aria-controls="tabset-2-1" aria-selected="true" href="">Image Reshape</a></li><li class="nav-item" role="presentation"><a class="nav-link" id="tabset-2-2-tab" data-bs-toggle="tab" data-bs-target="#tabset-2-2" role="tab" aria-controls="tabset-2-2" aria-selected="false" href="">Label Reshape</a></li></ul>
<div class="tab-content">
<div id="tabset-2-1" class="tab-pane active" role="tabpanel" aria-labelledby="tabset-2-1-tab">
<p>In the MNIST dataset, each image is a grid of 28x28 pixels, and each pixel appears as a tiny square in the figure. These pixels contain numbers between 0 and 255 that represent how light or dark each pixel is. To prepare the images for our neural network, we turn this grid into a single long list, or vector, with 784 numbers (because 28 multiplied by 28 gives us 784). We also adjust these numbers to be between 0 and 1 by dividing them by 255, which helps the network process the images more effectively.</p>
</div>
<div id="tabset-2-2" class="tab-pane" role="tabpanel" aria-labelledby="tabset-2-2-tab">
<p>For the labels, which are the actual numbers that the images represent, we have a list with either 60,000 entries for the training set or 10,000 for the testing set. These numbers vary from 0 to 9. We use a so-called one-hot encoding to turn each number into a ten-element list, where only one element is <q>1</q> (indicating the digit), and the rest are <q>0’s. So, the number ’2</q> becomes [0, 0, 1, 0, …, 0], a vector of size 10.</p>
</div>
</div>
</div>
<p>So, for our training set of 60,000 images and the testing set of 10,000 images, we end up with two big lists. The training list is 60,000 rows long and 784 columns wide, and the testing list is 10,000 rows by 784 columns. We do the same for our labels, creating lists that are 60,000x10 for training and 10,000x10 for testing, where each row has one <q>1</q> and nine ’0’s to show what digit the image is.</p>
<p>The data preparation steps are carried out using the following Python code:</p>
<pre class="{python}"><code>train_images_reshape = train_images.reshape((60000, 28 * 28))
train_images_reshape = train_images_reshape.astype('float32') / 255
test_images_reshape = test_images.reshape((10000, 28 * 28))
test_images_reshape = test_images_reshape.astype('float32') / 255

train_labels_cat = to_categorical(train_labels)
test_labels_cat = to_categorical(test_labels)</code></pre>
</section>
<section id="network-architecture" class="level2">
<h2 class="anchored" data-anchor-id="network-architecture">Network Architecture</h2>
<p>At the heart of neural networks is the layer—a data-processing module that functions as a filter for information. Essentially, a layer takes in data and helps to extract and refine features from it, which are representations of the data in a form that is more meaningful for the task at hand.</p>
<p>Deep learning involves stringing together these layers to progressively distill and refine data. You can think of a deep learning model as a sophisticated sieve, where each layer acts as a filter that focuses on increasingly detailed aspects of the data.</p>
<p>The Python code below constructs such a model using Keras:</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a>network <span class="op">=</span> models.Sequential()</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>network.add(layers.Dense(<span class="dv">512</span>, activation<span class="op">=</span><span class="st">'relu'</span>, input_shape<span class="op">=</span>(<span class="dv">28</span> <span class="op">*</span> <span class="dv">28</span>,)))</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>network.add(layers.Dense(<span class="dv">10</span>, activation<span class="op">=</span><span class="st">'softmax'</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>In this code:</p>
<ul>
<li>We start by initializing a new sequential network.</li>
<li>Next, we add a densely connected layer of 512 neurons that uses the ReLU (rectified linear unit) activation function. The input_shape=(28 * 28,) indicates that each input will be a flattened array of 28 by 28 pixels.</li>
<li>Finally, we add a 10-neuron dense layer, which uses the softmax activation function. This layer will output a probability distribution over 10 classes, corresponding to the 10 possible digits in the MNIST dataset.</li>
</ul>
<div id="nte-anatomy" class="callout callout-style-default callout-note callout-titled" title="Anatomy of Neural Network for Digit Classification">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-1-contents" aria-controls="callout-1" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Anatomy of Neural Network for Digit Classification
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-1" class="callout-1-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>This figure provides a high level visualization to outline the process of a simple neural network learning to classify handwritten digits from the MNIST dataset.</p>
<center>
<img src="https://github.com/ming-zhao/ming-zhao.github.io/raw/master/AIML_for_Business/res/Neural_Networks/figures/mnist_2.jpg" width="700">
</center>
<ul>
<li><p><strong>Input Image</strong>: The journey begins with a 28x28 pixel representation of a handwritten digit from the MNIST dataset.</p></li>
<li><p><strong>Layer 1 Input</strong>: Each digit image is unrolled into a 784-element vector (since 28 multiplied by 28 equals 784) to serve as the input for the neural network.</p></li>
<li><p><strong>Layer 1 Weight</strong>: In the first dense layer, we have 512 neurons, each connected to every input pixel, resulting in 784 weights per neuron. These weights are akin to the importance the neuron assigns to each pixel when trying to learn the patterns in the images. Intriguingly, each neuron’s set of 784 weights can be reshaped back into a 28x28 matrix and visualized similarly to the original digit images, representing the specific features that neuron is detecting. This layer transforms the input by applying a weighted sum of the inputs, followed by a non-linear activation function (ReLU - Rectified Linear Unit).</p></li>
<li><p><strong>Layer 1 Output &amp; Layer 2 Input</strong>: The processed data, known as filter scores, are passed from the first dense layer to the second layer. This transformation is the neural network’s first attempt at understanding the input digit.</p></li>
<li><p><strong>Layer 2 Weight</strong>: The second layer, composed of 10 neurons, aligns with the 10 possible digit classes, ranging from 0 to 9. Every neuron in this layer has 512 weights, which it uses to transform the 512-dimensional filter scores from the previous layer into a single score that represents the likelihood of the input image matching a particular digit class.</p></li>
<li><p><strong>Layer 2 Output</strong>: This layer outputs 10 scores, each indicating the likelihood that the input image matches one of the digit classes.</p></li>
<li><p><strong>Softmax Activation</strong>: Finally, the softmax activation function converts these scores into a probability distribution, ensuring that the probabilities sum to 1, allowing us to select the digit class with the highest probability as our prediction.</p></li>
</ul>
<p>The network architecture specified in the provided code segment is visualized step by step in the figure, showing the flow from the initial input to the final output that classifies the digit.</p>
</div>
</div>
</div>
</section>
<section id="compilation" class="level2">
<h2 class="anchored" data-anchor-id="compilation">Compilation</h2>
<p>Before we training our neural network to recognize digits, we need to set up a few things. It’s like prepping a player before the big game. We need to:</p>
<ul>
<li><p>Choose an optimizer: This is like the coach for the network. It helps the network improve step by step. In our code, we’re using <q>rmsprop</q>, which is a smart coach that adjusts the player’s (network’s) moves to perform better over time.</p></li>
<li><p>Pick a loss function: This is how the network knows if it’s winning or losing. It’s like the scoreboard. The <q>categorical_crossentropy</q> we’ve chosen is perfect for when the answers are one choice out of many, like picking which digit is in the image.</p></li>
<li><p>Decide on metrics: This is what we want to keep an eye on. We’ve picked <q>accuracy</q> because we want to see how often the network gets the right answer.</p></li>
</ul>
<p>And here’s how we write it down in our Python playbook:</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a>network.<span class="bu">compile</span>(optimizer<span class="op">=</span><span class="st">'rmsprop'</span>, <span class="co"># rmsprop = Root Mean Squared Propagation</span></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>                loss<span class="op">=</span><span class="st">'categorical_crossentropy'</span>,</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>                metrics<span class="op">=</span>[<span class="st">'accuracy'</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>By running this code, we tell our neural network to get ready. We’ve given it the rules of the game (loss function), a coach (optimizer), and told it to focus on getting a high score (accuracy).</p>
</section>
<section id="training-the-network" class="level2">
<h2 class="anchored" data-anchor-id="training-the-network">Training the Network</h2>
<p>We use the following command to train the network:</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a>network.fit(train_images_reshape, train_labels_cat, </span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>            epochs<span class="op">=</span><span class="dv">5</span>, batch_size<span class="op">=</span><span class="dv">128</span>, verbose<span class="op">=</span><span class="dv">1</span>)<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>This command initiates training, where the network iterates over the training data in mini-batches of 128 samples, completing 5 full cycles (each cycle over all the training data is called an <em>epoch</em>). In each epoch, the network computes the gradient of the weights (essentially the direction and amount by which the weights should be adjusted) concerning the loss function on the batch and updates the weights accordingly. After these 5 epochs, the network will have performed 5 <span class="math inline">\times</span> ceil(60000 <span class="math inline">\div</span> 128) gradient updates, totaling 2345 updates.</p>
<p>The size of the batch significantly impacts learning. A sufficiently large batch size provides a stable estimate of the gradient of the full dataset. By sampling from your dataset, you can approximate the gradient while significantly reducing computational costs.</p>
</section>
<section id="the-model" class="level2">
<h2 class="anchored" data-anchor-id="the-model">The Model</h2>
<p>We have compiled all the code snippets together to present the complete model as follows:</p>
<div class="cell" data-outputid="ad4b50a9-d6ad-492a-8e11-86050d4b310b" data-execution_count="3">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> os</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> tensorflow.keras <span class="im">import</span> models, layers</span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> tensorflow.keras <span class="im">import</span> backend <span class="im">as</span> K</span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> tensorflow.keras.utils <span class="im">import</span> to_categorical, get_file</span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a><span class="op">%</span>matplotlib inline</span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a>mdl_url <span class="op">=</span> (<span class="st">"https://raw.githubusercontent.com/ming-zhao/ming-zhao.github.io"</span></span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a>           <span class="st">"/master/AIML_for_Business/"</span>)</span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-13"><a href="#cb7-13" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> build_model():</span>
<span id="cb7-14"><a href="#cb7-14" aria-hidden="true" tabindex="-1"></a>    model <span class="op">=</span> models.Sequential(name<span class="op">=</span><span class="st">'mnist_simple'</span>)</span>
<span id="cb7-15"><a href="#cb7-15" aria-hidden="true" tabindex="-1"></a>    model.add(layers.Input(shape<span class="op">=</span>(<span class="dv">28</span> <span class="op">*</span> <span class="dv">28</span>,)))</span>
<span id="cb7-16"><a href="#cb7-16" aria-hidden="true" tabindex="-1"></a>    model.add(layers.Dense(<span class="dv">512</span>, activation<span class="op">=</span><span class="st">'relu'</span>))</span>
<span id="cb7-17"><a href="#cb7-17" aria-hidden="true" tabindex="-1"></a>    model.add(layers.Dense(<span class="dv">10</span>, activation<span class="op">=</span><span class="st">'softmax'</span>))</span>
<span id="cb7-18"><a href="#cb7-18" aria-hidden="true" tabindex="-1"></a>    model.<span class="bu">compile</span>(optimizer<span class="op">=</span><span class="st">'rmsprop'</span>, loss<span class="op">=</span><span class="st">'categorical_crossentropy'</span>, metrics<span class="op">=</span>[<span class="st">'acc'</span>])</span>
<span id="cb7-19"><a href="#cb7-19" aria-hidden="true" tabindex="-1"></a>    model.fit(train_images_reshape, train_labels_cat, epochs<span class="op">=</span><span class="dv">5</span>, batch_size<span class="op">=</span><span class="dv">128</span>, verbose<span class="op">=</span><span class="dv">1</span>)<span class="op">;</span></span>
<span id="cb7-20"><a href="#cb7-20" aria-hidden="true" tabindex="-1"></a>    model.save(<span class="st">'./model/mnist_simple.keras'</span>)</span>
<span id="cb7-21"><a href="#cb7-21" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> model</span>
<span id="cb7-22"><a href="#cb7-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-23"><a href="#cb7-23" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> keras.datasets <span class="im">import</span> mnist</span>
<span id="cb7-24"><a href="#cb7-24" aria-hidden="true" tabindex="-1"></a>(train_images, train_labels), (test_images, test_labels) <span class="op">=</span> mnist.load_data()</span>
<span id="cb7-25"><a href="#cb7-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-26"><a href="#cb7-26" aria-hidden="true" tabindex="-1"></a>train_images_reshape <span class="op">=</span> train_images.reshape((<span class="dv">60000</span>, <span class="dv">28</span> <span class="op">*</span> <span class="dv">28</span>))</span>
<span id="cb7-27"><a href="#cb7-27" aria-hidden="true" tabindex="-1"></a>train_images_reshape <span class="op">=</span> train_images_reshape.astype(<span class="st">'float32'</span>) <span class="op">/</span> <span class="dv">255</span></span>
<span id="cb7-28"><a href="#cb7-28" aria-hidden="true" tabindex="-1"></a>test_images_reshape <span class="op">=</span> test_images.reshape((<span class="dv">10000</span>, <span class="dv">28</span> <span class="op">*</span> <span class="dv">28</span>))</span>
<span id="cb7-29"><a href="#cb7-29" aria-hidden="true" tabindex="-1"></a>test_images_reshape <span class="op">=</span> test_images_reshape.astype(<span class="st">'float32'</span>) <span class="op">/</span> <span class="dv">255</span></span>
<span id="cb7-30"><a href="#cb7-30" aria-hidden="true" tabindex="-1"></a>train_labels_cat <span class="op">=</span> to_categorical(train_labels)</span>
<span id="cb7-31"><a href="#cb7-31" aria-hidden="true" tabindex="-1"></a>test_labels_cat <span class="op">=</span> to_categorical(test_labels)</span>
<span id="cb7-32"><a href="#cb7-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-33"><a href="#cb7-33" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> build_model()</span>
<span id="cb7-34"><a href="#cb7-34" aria-hidden="true" tabindex="-1"></a><span class="co"># model = models.load_model(get_file(origin=os.path.join(mdl_url, 'mnist_simple.keras')));</span></span>
<span id="cb7-35"><a href="#cb7-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-36"><a href="#cb7-36" aria-hidden="true" tabindex="-1"></a>model.summary()</span>
<span id="cb7-37"><a href="#cb7-37" aria-hidden="true" tabindex="-1"></a>test_loss, test_acc <span class="op">=</span> model.evaluate(test_images_reshape, test_labels_cat, verbose<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb7-38"><a href="#cb7-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-39"><a href="#cb7-39" aria-hidden="true" tabindex="-1"></a>predicted <span class="op">=</span> np.argmax(model.predict(test_images_reshape, verbose<span class="op">=</span><span class="dv">0</span>), axis<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb7-40"><a href="#cb7-40" aria-hidden="true" tabindex="-1"></a>result <span class="op">=</span> <span class="bu">abs</span>(predicted <span class="op">-</span> test_labels)</span>
<span id="cb7-41"><a href="#cb7-41" aria-hidden="true" tabindex="-1"></a>misclassified <span class="op">=</span> np.where(result<span class="op">&gt;</span><span class="dv">0</span>)[<span class="dv">0</span>]</span>
<span id="cb7-42"><a href="#cb7-42" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Test accuracy is </span><span class="sc">{}</span><span class="st">%'</span>.<span class="bu">format</span>(<span class="bu">round</span>(test_acc<span class="op">*</span><span class="dv">100</span>,<span class="dv">2</span>)))</span>
<span id="cb7-43"><a href="#cb7-43" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Out of 10000 testing images, </span><span class="sc">{}</span><span class="st"> misclassified images.</span><span class="ch">\n</span><span class="st">'</span>.<span class="bu">format</span>(misclassified.shape[<span class="dv">0</span>]))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 1/5
469/469 ━━━━━━━━━━━━━━━━━━━━ 2s 3ms/step - acc: 0.8710 - loss: 0.4424
Epoch 2/5
469/469 ━━━━━━━━━━━━━━━━━━━━ 2s 3ms/step - acc: 0.9654 - loss: 0.1209
Epoch 3/5
469/469 ━━━━━━━━━━━━━━━━━━━━ 1s 3ms/step - acc: 0.9795 - loss: 0.0712
Epoch 4/5
469/469 ━━━━━━━━━━━━━━━━━━━━ 1s 3ms/step - acc: 0.9855 - loss: 0.0491
Epoch 5/5
469/469 ━━━━━━━━━━━━━━━━━━━━ 2s 3ms/step - acc: 0.9896 - loss: 0.0371
Test accuracy is 98.07%
Out of 10000 testing images, 193 misclassified images.
</code></pre>
</div>
<div class="cell-output cell-output-display">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold">Model: "mnist_simple"</span>
</pre>
</div>
<div class="cell-output cell-output-display">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓
┃<span style="font-weight: bold"> Layer (type)                    </span>┃<span style="font-weight: bold"> Output Shape           </span>┃<span style="font-weight: bold">       Param # </span>┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩
│ dense (<span style="color: #0087ff; text-decoration-color: #0087ff">Dense</span>)                   │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">512</span>)            │       <span style="color: #00af00; text-decoration-color: #00af00">401,920</span> │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ dense_1 (<span style="color: #0087ff; text-decoration-color: #0087ff">Dense</span>)                 │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">10</span>)             │         <span style="color: #00af00; text-decoration-color: #00af00">5,130</span> │
└─────────────────────────────────┴────────────────────────┴───────────────┘
</pre>
</div>
<div class="cell-output cell-output-display">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold"> Total params: </span><span style="color: #00af00; text-decoration-color: #00af00">814,102</span> (3.11 MB)
</pre>
</div>
<div class="cell-output cell-output-display">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold"> Trainable params: </span><span style="color: #00af00; text-decoration-color: #00af00">407,050</span> (1.55 MB)
</pre>
</div>
<div class="cell-output cell-output-display">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold"> Non-trainable params: </span><span style="color: #00af00; text-decoration-color: #00af00">0</span> (0.00 B)
</pre>
</div>
<div class="cell-output cell-output-display">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold"> Optimizer params: </span><span style="color: #00af00; text-decoration-color: #00af00">407,052</span> (1.55 MB)
</pre>
</div>
</div>
<p>The output illustrates the layers of the model and the number of their parameters. It also shows the model’s accuracy on the test data. As we can see, some images are misclassified. Let’s examine those images.</p>
<div class="cell" data-execution_count="4">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a>index <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a><span class="co"># print('Examples of misclassified images {}-{}'.format(index, index+4))</span></span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">13</span>,<span class="dv">3</span>))</span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">5</span>):</span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a>    plt.subplot(<span class="dv">1</span>, <span class="dv">5</span>, i <span class="op">+</span> <span class="dv">1</span>)</span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a>    idx <span class="op">=</span> misclassified[i<span class="op">+</span>index]</span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a>    digit <span class="op">=</span> test_images[idx]</span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a>    plt.imshow(digit, cmap<span class="op">=</span>plt.cm.binary)</span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a>    plt.title(<span class="st">'Predicted:</span><span class="sc">{}</span><span class="st">, Label:</span><span class="sc">{}</span><span class="st">'</span>.<span class="bu">format</span>(predicted[idx], test_labels[idx]), fontsize <span class="op">=</span> <span class="dv">12</span>)</span>
<span id="cb9-10"><a href="#cb9-10" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div id="fig-misclassified" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="Intro_to_Neural_Networks_files/figure-html/fig-misclassified-output-1.png" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">Figure&nbsp;9: Examples of misclassified images</figcaption>
</figure>
</div>
</div>
</div>
<p><a href="#fig-misclassified">Figure&nbsp;9</a> displays five misclassified images. For instance, at the top of all the 5 images, it shows <q>predicted: X, Label: Y</q> for some X <span class="math inline">\neq</span> Y — indicating that the label is Y, but our model predicted X.</p>
<div class="cell" data-execution_count="5">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Get the weights of all layers</span></span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>weights <span class="op">=</span> [layer.get_weights() <span class="cf">for</span> layer <span class="kw">in</span> model.layers]</span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Reshape the weights into 28x28 images</span></span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a>weight <span class="op">=</span> np.reshape(weights[<span class="dv">0</span>][<span class="dv">0</span>], (<span class="dv">28</span>, <span class="dv">28</span>, <span class="dv">512</span>))</span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true" tabindex="-1"></a><span class="co"># print('plot weights of the first layer as a heatmap')</span></span>
<span id="cb10-8"><a href="#cb10-8" aria-hidden="true" tabindex="-1"></a>fig, axs <span class="op">=</span> plt.subplots(nrows<span class="op">=</span><span class="dv">16</span>, ncols<span class="op">=</span><span class="dv">32</span>, figsize<span class="op">=</span>(<span class="dv">32</span>,<span class="dv">16</span>))</span>
<span id="cb10-9"><a href="#cb10-9" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">16</span>):</span>
<span id="cb10-10"><a href="#cb10-10" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">32</span>):</span>
<span id="cb10-11"><a href="#cb10-11" aria-hidden="true" tabindex="-1"></a>        axs[i,j].imshow(weight[:,:,i<span class="op">*</span><span class="dv">32</span><span class="op">+</span>j], cmap<span class="op">=</span><span class="st">'gray'</span>)</span>
<span id="cb10-12"><a href="#cb10-12" aria-hidden="true" tabindex="-1"></a>        axs[i,j].axis(<span class="st">'off'</span>)</span>
<span id="cb10-13"><a href="#cb10-13" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div id="fig-first_layer_heatmap" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="Intro_to_Neural_Networks_files/figure-html/fig-first_layer_heatmap-output-1.png" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">Figure&nbsp;10: Plot weights of the first layer as a heatmap</figcaption>
</figure>
</div>
</div>
</div>
<p><a href="#fig-first_layer_heatmap">Figure&nbsp;10</a> is the heatmap of the first layer’s weights. This layer contains 512 neurons, resulting in 512 images. Recall that the code snippet below added this layer to the model:</p>
<div class="sourceCode" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a>network.add(layers.Dense(<span class="dv">512</span>, activation<span class="op">=</span><span class="st">'relu'</span>, input_shape<span class="op">=</span>(<span class="dv">28</span> <span class="op">*</span> <span class="dv">28</span>,)))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Each image in <a href="#fig-first_layer_heatmap">Figure&nbsp;10</a> represents the specific features that a neuron is detecting. For more details, see Anatomy of Neural Network for Digit Classification: Layer 1 Weight.</p>
<p><a href="#fig-more_layer_heatmap">Figure&nbsp;11</a> aggregates the weights of both layers into a heatmap. While the aggregated weights plots don’t take into account activation functions and bias terms, they do provide some insight into how the neural network operates.</p>
<p>Consider the first image in <a href="#fig-more_layer_heatmap">Figure&nbsp;11</a>, which demonstrates how an image is classified as the digit 0. According to the color scale on the right, yellow or light green indicates high values, suggesting that pixels in these areas significantly contribute to classifying an image as 0. Conversely, dark colors indicate low values (or even negative), showing that pixels in these areas do not support the classification as 0. For instance, the center of the 0 image displays strongly negative values, indicating that any color in the center of the image is less likely to be classified as 0.</p>
<div class="cell" data-execution_count="6">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="co"># print('plot aggregated weights of both layers as a heatmap')</span></span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Get aggregated weights and reshape</span></span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a>weight <span class="op">=</span> (weights[<span class="dv">0</span>][<span class="dv">0</span>] <span class="op">@</span> weights[<span class="dv">1</span>][<span class="dv">0</span>]).reshape(<span class="dv">28</span>, <span class="dv">28</span>, <span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a>fig, axes <span class="op">=</span> plt.subplots(nrows<span class="op">=</span><span class="dv">1</span>, ncols<span class="op">=</span><span class="dv">10</span>, figsize<span class="op">=</span>(<span class="dv">18</span>,<span class="dv">10</span>))</span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i, ax <span class="kw">in</span> <span class="bu">enumerate</span>(axes):</span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a>    ax.set_title(i)</span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a>    im <span class="op">=</span> ax.imshow(weight[:,:,i], cmap<span class="op">=</span><span class="st">'viridis'</span>)</span>
<span id="cb12-8"><a href="#cb12-8" aria-hidden="true" tabindex="-1"></a>    ax.axis(<span class="st">'off'</span>)</span>
<span id="cb12-9"><a href="#cb12-9" aria-hidden="true" tabindex="-1"></a>fig.colorbar(im, ax<span class="op">=</span>axes, fraction<span class="op">=</span><span class="fl">0.006</span>, pad<span class="op">=</span><span class="fl">0.01</span>)</span>
<span id="cb12-10"><a href="#cb12-10" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div id="fig-more_layer_heatmap" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="Intro_to_Neural_Networks_files/figure-html/fig-more_layer_heatmap-output-1.png" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">Figure&nbsp;11: Plot aggregated weights of both layers as a heatmap</figcaption>
</figure>
</div>
</div>
</div>
</section>
</section>

</main>
<!-- /main column -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->



</body></html>
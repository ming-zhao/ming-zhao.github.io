{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "title: Untitled\n",
    "author: Developer\n",
    "date: \"{{ datetime.now().strftime('%Y-%m-%d %H:%M:%S') }}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext pretty_jupyter\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embedding\n",
    "\n",
    "Machine learning models only work with a numeric representation of input data. Suppose we have a training corpus with three  sentences:\n",
    "\n",
    "- \"the dog saw a cat\", \n",
    "- \"the dog chased the cat\", \n",
    "- \"the cat climbed a tree\".\n",
    "\n",
    "The corpus vocabulary has eight words, which are listed alphabetically.\n",
    "\n",
    "|index | vocabulary | \n",
    "| --- | ---  |\n",
    "|1 |a   | \n",
    "|2 |cat |\n",
    "|3 |chased |\n",
    "|4 |climbed |\n",
    "|5 |dog |\n",
    "|6 |saw |\n",
    "|7 |the |\n",
    "|8 |tree |\n",
    "\n",
    "## Tokenization\n",
    "\n",
    "The different units into which you can break down text are called tokens. The token can be a word (e.g. \"cat\"), or a phrase (e.g., \"the cat\", or \"a cat\"). Eventually, we will transform each token to a numeric vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The count of words OrderedDict([('the', 4), ('dog', 2), ('saw', 1), ('a', 2), ('cat', 3), ('chased', 1), ('climbed', 1), ('tree', 1)])\n",
      "The sequences generated from text are :  [[1, 3, 5, 4, 2], [1, 3, 6, 1, 2], [1, 2, 7, 4, 8]]\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "texts = [\"the dog saw a cat\",\n",
    "         \"the dog chased the cat\",\n",
    "         \"the cat climbed a tree\"]\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "print(\"The count of words\",tokenizer.word_counts)\n",
    "print(\"The sequences generated from text are : \",sequences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The example includes 8 tokens (one for each word) and assign an integer to each token (e.g. dog = 3).\n",
    "\n",
    "## One-hot encoding\n",
    "\n",
    "One-hot encoding is the most common, most basic way to turn a token into a vector. It is a binary vector of the size of the vocabulary where the vector has 1 for the index of the word, and 0 elsewhere. For example, \"cat\" has index 2 in the alphabetical order of the vocabulary. Then, the one-hot encoding of the word \"cat\" can be\n",
    "$$\\underbrace{[0,1,0,0,0,0,0,0]}_{8}$$\n",
    "and, the sentence \"the dog saw a cat\" can be represented as a $5 \\times 8$ matrix\n",
    "\\begin{align*}\n",
    "\\left[\n",
    "\\begin{array}{*{8}c}\n",
    "0,0,0,0,0,0,1,0 \\\\\n",
    "0,0,0,0,1,0,0,0\\\\\n",
    "0,0,0,0,0,1,0,0\\\\\n",
    "1,0,0,0,0,0,0,0\\\\\n",
    "0,1,0,0,0,0,0,0\n",
    "\\end{array} \\right]\n",
    "\\end{align*}\n",
    "since the sentence has 5 words and the corpus vocabulary has 8 words.\n",
    "\n",
    "One-hot encoding has two drawbacks for natural language processing (NLP) tasks:\n",
    "\n",
    "- The dimensionality of the vector space becomes very high and sparse because each vector has the size of the vocabulary.\n",
    "\n",
    "- There is no connection between words with similar meanings. For example, it is impossible to know that \"dog\" and \"cat\" are both pets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding layers\n",
    "\n",
    "A popular and powerful way to associate a vector with a word is the use of dense word vectors, also called word embeddings.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/ming-zhao/ming-zhao.github.io/master/intro_to_analytics/Sequential_Models/figures/word_embedding.jpg\" width=\"800\">\n",
    "\n",
    "Given input texts, a word embedding model constructs a numeric vector for each word, resulting in a numeric representation of the input texts.\n",
    "\n",
    "A word embedding model is built from a training corpus. Initially, we represent the training texts by simply using frequency encoding or one-hot encoding. We can think the embedding model as a mathematical model that involving many caculations. After sending the numeric representation of the trainning text, embedding model \n",
    "\n",
    "\n",
    "Machine learning models don't take raw text as input and they only work with numeric tensors. In natural language processing (NLP), word embedding is a projection of a word into meaningful vectors of real numbers.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "One-hot encoding is a binary vector of the size of the vocabulary where the vector is all zeros, but has 1 for the index of the word. \n",
    "We can also encode the sentence based on the frequency of each word's occurrence. Then, the sentence “the dog saw a cat” becomes an array\n",
    "$$[4, 2, 1, 2, 3]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis of IMDB Movie Reviews\n",
    "\n",
    "The IMDB dataset is a commonly used dataset for machine learning tutorials related to text and language. It contains 50,000 movie reviews, with 25,000 in the training set and 25,000 in the testing set, collected from IMDB. Each review in the dataset has been labeled with a binary sentiment: positive (1) or negative (0). The following code loads the IMDB data:\n",
    "```python\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "```\n",
    "The raw data contains the text of each movie review. Keras provides a built-in function that replaces the raw text in both the training and testing datasets with integers based on the frequency of each word's occurrence in the entire training dataset. For instance the integer \"3\" encodes the 3rd most frequent word in the data. \n",
    "\n",
    "The resulting `x_train` is a numpy array containing 25,000 lists, and only the `num_words` most frequent words are kept. Each list may have a different length due to different review lengths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example of dictionary:\n",
      " {11: 'this', 12: 'that', 13: 'was', 14: 'as', 15: 'for'}\n",
      "Review:\n",
      " ? this film was just brilliant casting location scenery story direction everyone's really suited the part they played and you could just imagine being there robert ? is an amazing actor and now the same being director ? father came from the same scottish island as myself so i loved the fact there was a real connection with this film the witty remarks throughout the film were great it was just brilliant so much that i bought the film as soon as it was released for ? and would recommend it to everyone to watch and the fly fishing was amazing really cried at the end it was so sad and you know what they say if you cry at a film it must have been good and this definitely was also ? to the two little boy's that played the ? of norman and paul they were just brilliant children are often left out of the ? list i think because the stars that play them all grown up are such a big profile for the whole film but these children are amazing and should be praised for what they have done don't you think the whole story was so lovely because it was true and was someone's life after all that was shared with us all\n",
      "--------------------------\n",
      "Number of words: 218\n",
      "sentiment: 1\n"
     ]
    }
   ],
   "source": [
    "def show_review(index):\n",
    "    decoded_review = ' '.join([reverse_word_index.get(i - 3, '?') for i in x_train[index]])\n",
    "    print('Review:\\n', decoded_review)\n",
    "    print('--------------------------')\n",
    "    print('Number of words:', len(decoded_review.split()))\n",
    "    print('sentiment:', y_train[0])\n",
    "    # print('Indices for all ?s:', [x_train[index][i] for i, w in enumerate(decoded_review.split()) if w == '?'])\n",
    "\n",
    "from keras.datasets import imdb\n",
    "\n",
    "max_features = 10000\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "\n",
    "word_index = imdb.get_word_index()\n",
    "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n",
    "\n",
    "print(\"Example of dictionary:\\n\",dict(sorted(reverse_word_index.items())[10:15]))\n",
    "show_review(0)\n",
    "# print(\"Decode review x_train[0][:10]]:\\n\",\n",
    "#       ' '.join([reverse_word_index.get(i - 3, '?') for i in x_train[0][:10]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before training a machine learning model, the dataset needs to be preprocessed, such as removing frequently occurring words that do not contribute much to the meaning of the text, such as stopwords like \"the,\" \"and,\" or \"a\". Additionally, most machine learning algorithms expect to see the same number of features, i.e., the same length for each list.\n",
    "\n",
    "To achieve this, the pad_sequences function sets the maximum number of words in each review (i.e., maxlen):\n",
    "```python\n",
    "x_train = pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = pad_sequences(x_test, maxlen=maxlen)\n",
    "```\n",
    "For reviews that have fewer than maxlen words, we pad them with \"0\", while for reviews that have more than maxlen words, we truncate them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import imdb\n",
    "from keras.utils import pad_sequences\n",
    "\n",
    "max_features = 10000\n",
    "maxlen = 20\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "x_train = pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = pad_sequences(x_test, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.datasets import imdb\n",
    "from keras import models, layers\n",
    "from keras.utils import pad_sequences\n",
    "from keras import backend as K\n",
    "\n",
    "max_features = 10000\n",
    "maxlen = 20\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "x_train = pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = pad_sequences(x_test, maxlen=maxlen)\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Embedding(input_dim=max_features, output_dim=8, input_length=maxlen))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
    "history = model.fit(x_train, y_train, epochs=10, batch_size=32, validation_split=0.2, verbose=0)\n",
    "\n",
    "# show model summary\n",
    "model.summary()\n",
    "\n",
    "# show fitting information\n",
    "vars(history)\n",
    "\n",
    "# Create a Keras model with input and all layers' output\n",
    "layer_outputs = [layer.output for layer in model.layers]\n",
    "model_with_output = keras.Model(inputs=model.input, outputs=layer_outputs)\n",
    "\n",
    "# Compute the outputs of all layers for the input tensor\n",
    "outputs = model_with_output(x_train)\n",
    "weights = [layer.get_weights() for layer in model.layers]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are max_features (10,000) words in the vocabulary. The embedding layer converts each word into a fixed-length vector of a defined size (8 in this case). The length of the input sequence for the Embedding layer is `maxlen`, which is the length of each x_train[i] sequence. As shown in the figure, the output of the first review is a 20x8 tensor where each word (e.g., 4) is converted to a vector in the `weights` of the embedding layer (e.g., weights[4]).\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/ming-zhao/ming-zhao.github.io/master/intro_to_analytics/Sequential_Models/figures/embedding.jpg\" width=\"600\">\n",
    "\n",
    "The Flatten layer is used to convert a multi-dimensional tensor into a single-dimensional tensor by removing all dimensions except for one.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/ming-zhao/ming-zhao.github.io/master/intro_to_analytics/Sequential_Models/figures/flatten.jpg\" width=\"700\">\n",
    "\n",
    "The Dense layer produces the final output for classification.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/ming-zhao/ming-zhao.github.io/master/intro_to_analytics/Sequential_Models/figures/dense.jpg\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "<!-- {a, cat, chased, climbed, dog, saw, the, tree} -->\n",
    "\n",
    "Once ordered alphabetically, each word can be referenced by its index. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[WinError 182] The operating system cannot run %1. Error loading \"C:\\Users\\mzhao\\Anaconda3\\envs\\py39\\lib\\site-packages\\torch\\lib\\shm.dll\" or one of its dependencies.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnn\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptim\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01moptim\u001b[39;00m\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\py39\\lib\\site-packages\\torch\\__init__.py:128\u001b[0m\n\u001b[0;32m    126\u001b[0m     err \u001b[38;5;241m=\u001b[39m ctypes\u001b[38;5;241m.\u001b[39mWinError(last_error)\n\u001b[0;32m    127\u001b[0m     err\u001b[38;5;241m.\u001b[39mstrerror \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m Error loading \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdll\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m or one of its dependencies.\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m--> 128\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m err\n\u001b[0;32m    129\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m res \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    130\u001b[0m     is_loaded \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[1;31mOSError\u001b[0m: [WinError 182] The operating system cannot run %1. Error loading \"C:\\Users\\mzhao\\Anaconda3\\envs\\py39\\lib\\site-packages\\torch\\lib\\shm.dll\" or one of its dependencies."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "# from torchtext.datasets import IMDB\n",
    "\n",
    "# Load the IMDB dataset\n",
    "max_features = 10000\n",
    "maxlen = 20\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=10000)\n",
    "\n",
    "from keras.utils import pad_sequences\n",
    "x_train = pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = pad_sequences(x_test, maxlen=maxlen)\n",
    "\n",
    "# Define a custom dataset class\n",
    "class IMDBDataset(Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        self.x = [torch.LongTensor(sample) for sample in x]\n",
    "        self.y = torch.FloatTensor(y)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx], self.y[idx]\n",
    "\n",
    "# Pad the sequences to a fixed length\n",
    "# x_train = pad_sequence(x_train, batch_first=True, padding_value=0)\n",
    "# x_test = pad_sequence(x_test, batch_first=True, padding_value=0)\n",
    "\n",
    "# Create the model\n",
    "class IMDBModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(IMDBModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(num_embeddings=max_features, embedding_dim=8)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc = nn.Linear(in_features=maxlen*8, out_features=1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "model = IMDBModel()\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.RMSprop(model.parameters())\n",
    "\n",
    "# Define the dataset and data loaders\n",
    "train_dataset = IMDBDataset(x_train, y_train)\n",
    "test_dataset = IMDBDataset(x_test, y_test)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(10):\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels.unsqueeze(1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "    epoch_loss = running_loss / len(train_dataset)\n",
    "    print(f\"Epoch {epoch+1} loss: {epoch_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pretrained Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 88582 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "imdb_dir = \"C:\\\\Users\\\\mzhao\\\\Downloads\\\\aclImdb\\\\aclImdb\"\n",
    "test_dir = os.path.join(imdb_dir, 'train')\n",
    "\n",
    "labels = []\n",
    "texts = []\n",
    "\n",
    "for label_type in ['neg', 'pos']:\n",
    "    dir_name = os.path.join(test_dir, label_type)\n",
    "    for fname in sorted(os.listdir(dir_name)):\n",
    "        if fname[-4:] == '.txt':\n",
    "            f = open(os.path.join(dir_name, fname), encoding=\"utf8\")\n",
    "            texts.append(f.read())\n",
    "            f.close()\n",
    "            if label_type == 'neg':\n",
    "                labels.append(0)\n",
    "            else:\n",
    "                labels.append(1)\n",
    "\n",
    "maxlen = 100  # We will cut reviews after 100 words\n",
    "training_samples = 200\n",
    "validation_samples = 10000\n",
    "max_words = 10000  # We will only consider the top 10,000 words in the dataset\n",
    "\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "data = pd.read_csv(\"retail.csv\")\n",
    "\n",
    "customers = data.CustomerID.unique().tolist()\n",
    "random.shuffle(customers)\n",
    "customers_train = [customers[i] for i in range(round(0.9*len(customers)))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list to capture purchase history of the customers\n",
    "train_data = []\n",
    "longest = 0\n",
    "# populate the list with the product codes\n",
    "for i in customers_train:\n",
    "    temp = data[data[\"CustomerID\"] == i][\"StockCode\"].tolist()\n",
    "    if len(temp) > longest:\n",
    "        longest = len(temp)\n",
    "    train_data.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "model = Word2Vec(sentences=train_data, \n",
    "                 vector_size=100, \n",
    "                 window=longest, \n",
    "                 min_count=1, \n",
    "                 workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "catalog = data[['StockCode','Description']].drop_duplicates()\n",
    "catalog = pd.Series(catalog.Description.values,index=catalog.StockCode).to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fancy font birthday card,  [ 4.0704374  -0.22106545 -0.59359145  1.9778492  -3.4744737   0.15816163\n",
      " -0.4869984  -0.09155636  0.7313011  -1.1964443   2.2655969   0.40903243\n",
      "  2.300708    2.2600214  -0.18437724 -2.5046859   0.84504133  2.1785254\n",
      " -2.2461796  -0.41344282  1.146358    0.7075168   1.168436    1.6376145\n",
      " -1.3849007   0.1322     -1.8882684  -3.2134023  -2.7833428   3.2070143\n",
      " -1.1483601   3.3133862  -0.48263362  0.27444142 -1.8520232   1.4065834\n",
      " -0.37015316  0.5616905   1.4450984  -3.695895   -1.3664227   0.99329466\n",
      "  0.37704632 -1.3385558  -1.4899579  -1.6486493   0.33262435 -8.018287\n",
      "  0.61036426  0.31012124 -1.4512694   0.79294133  0.7805817  -0.8608317\n",
      "  0.3281135  -0.6267116  -2.502889    0.28152987 -2.4738226  -0.0701296\n",
      " -1.494042   -0.5871988  -1.7151927  -0.04570224  0.6937349  -0.283201\n",
      "  2.7495005  -4.2997017  -2.798139    1.2615929   1.7535899   2.2932553\n",
      " -0.44363186 -1.8016801   2.965545    1.6557103  -2.1493094   0.2981701\n",
      " -3.0025198  -3.365834    3.8598619  -3.5739481  -2.3316126   0.04437663\n",
      " -6.1700883   0.6805624  -1.6111788   2.1927068  -0.09581266 -0.87568\n",
      "  0.9798162  -0.22551404 -2.7736611   1.5798463   1.4444796  -3.6038978\n",
      " -0.20610172  0.1652321  -0.1938224   1.7076657 ]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('swallows greeting card', '22030', 0.9560685753822327),\n",
       " ('robot birthday card', '22037', 0.8443895578384399),\n",
       " ('card billboard font', '22983', 0.8401292562484741),\n",
       " ('banquet birthday  card  ', '22026', 0.8324193954467773),\n",
       " ('wrap gingham rose ', '22986', 0.8145104050636292),\n",
       " ('card cat and tree ', '22718', 0.8039556741714478),\n",
       " ('wrap billboard fonts design', '22985', 0.7852078676223755),\n",
       " ('card wedding day', '22715', 0.776096761226654),\n",
       " ('wrap vintage leaf design', '23232', 0.7712549567222595),\n",
       " ('fancy fonts birthday wrap', '21497', 0.7692080736160278)]"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item = '21506'\n",
    "print(catalog[item], model.wv.get_vector(item))\n",
    "# model.wv.most_similar('90019A', topn=10)\n",
    "# print(data[data[\"CustomerID\"] == 13408][\"StockCode\"].tolist())\n",
    "[(catalog[i],i,s) for (i,s) in model.wv.most_similar(item, topn=10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>InvoiceNo</th>\n",
       "      <th>StockCode</th>\n",
       "      <th>Description</th>\n",
       "      <th>Quantity</th>\n",
       "      <th>InvoiceDate</th>\n",
       "      <th>UnitPrice</th>\n",
       "      <th>CustomerID</th>\n",
       "      <th>Country</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>253</th>\n",
       "      <td>536394</td>\n",
       "      <td>21506</td>\n",
       "      <td>fancy font birthday card,</td>\n",
       "      <td>24</td>\n",
       "      <td>12/1/2010 10:39</td>\n",
       "      <td>0.42</td>\n",
       "      <td>13408</td>\n",
       "      <td>United Kingdom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>254</th>\n",
       "      <td>536394</td>\n",
       "      <td>22633</td>\n",
       "      <td>hand warmer union jack</td>\n",
       "      <td>96</td>\n",
       "      <td>12/1/2010 10:39</td>\n",
       "      <td>1.85</td>\n",
       "      <td>13408</td>\n",
       "      <td>United Kingdom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>255</th>\n",
       "      <td>536394</td>\n",
       "      <td>22866</td>\n",
       "      <td>hand warmer scotty dog design</td>\n",
       "      <td>96</td>\n",
       "      <td>12/1/2010 10:39</td>\n",
       "      <td>1.85</td>\n",
       "      <td>13408</td>\n",
       "      <td>United Kingdom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>256</th>\n",
       "      <td>536394</td>\n",
       "      <td>22865</td>\n",
       "      <td>hand warmer owl design</td>\n",
       "      <td>96</td>\n",
       "      <td>12/1/2010 10:39</td>\n",
       "      <td>1.85</td>\n",
       "      <td>13408</td>\n",
       "      <td>United Kingdom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>257</th>\n",
       "      <td>536394</td>\n",
       "      <td>22632</td>\n",
       "      <td>hand warmer red retrospot</td>\n",
       "      <td>96</td>\n",
       "      <td>12/1/2010 10:39</td>\n",
       "      <td>1.85</td>\n",
       "      <td>13408</td>\n",
       "      <td>United Kingdom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>406824</th>\n",
       "      <td>581587</td>\n",
       "      <td>22613</td>\n",
       "      <td>pack of 20 spaceboy napkins</td>\n",
       "      <td>12</td>\n",
       "      <td>12/9/2011 12:50</td>\n",
       "      <td>0.85</td>\n",
       "      <td>12680</td>\n",
       "      <td>France</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>406825</th>\n",
       "      <td>581587</td>\n",
       "      <td>22899</td>\n",
       "      <td>children's apron dolly girl</td>\n",
       "      <td>6</td>\n",
       "      <td>12/9/2011 12:50</td>\n",
       "      <td>2.10</td>\n",
       "      <td>12680</td>\n",
       "      <td>France</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>406826</th>\n",
       "      <td>581587</td>\n",
       "      <td>23254</td>\n",
       "      <td>childrens cutlery dolly girl</td>\n",
       "      <td>4</td>\n",
       "      <td>12/9/2011 12:50</td>\n",
       "      <td>4.15</td>\n",
       "      <td>12680</td>\n",
       "      <td>France</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>406827</th>\n",
       "      <td>581587</td>\n",
       "      <td>23255</td>\n",
       "      <td>childrens cutlery circus parade</td>\n",
       "      <td>4</td>\n",
       "      <td>12/9/2011 12:50</td>\n",
       "      <td>4.15</td>\n",
       "      <td>12680</td>\n",
       "      <td>France</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>406828</th>\n",
       "      <td>581587</td>\n",
       "      <td>22138</td>\n",
       "      <td>baking set 9 piece retrospot</td>\n",
       "      <td>3</td>\n",
       "      <td>12/9/2011 12:50</td>\n",
       "      <td>4.95</td>\n",
       "      <td>12680</td>\n",
       "      <td>France</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>52438 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       InvoiceNo StockCode                      Description  Quantity  \\\n",
       "253       536394     21506       fancy font birthday card,         24   \n",
       "254       536394     22633           hand warmer union jack        96   \n",
       "255       536394     22866    hand warmer scotty dog design        96   \n",
       "256       536394     22865           hand warmer owl design        96   \n",
       "257       536394     22632        hand warmer red retrospot        96   \n",
       "...          ...       ...                              ...       ...   \n",
       "406824    581587     22613      pack of 20 spaceboy napkins        12   \n",
       "406825    581587     22899     children's apron dolly girl          6   \n",
       "406826    581587     23254    childrens cutlery dolly girl          4   \n",
       "406827    581587     23255  childrens cutlery circus parade         4   \n",
       "406828    581587     22138    baking set 9 piece retrospot          3   \n",
       "\n",
       "            InvoiceDate  UnitPrice  CustomerID         Country  \n",
       "253     12/1/2010 10:39       0.42       13408  United Kingdom  \n",
       "254     12/1/2010 10:39       1.85       13408  United Kingdom  \n",
       "255     12/1/2010 10:39       1.85       13408  United Kingdom  \n",
       "256     12/1/2010 10:39       1.85       13408  United Kingdom  \n",
       "257     12/1/2010 10:39       1.85       13408  United Kingdom  \n",
       "...                 ...        ...         ...             ...  \n",
       "406824  12/9/2011 12:50       0.85       12680          France  \n",
       "406825  12/9/2011 12:50       2.10       12680          France  \n",
       "406826  12/9/2011 12:50       4.15       12680          France  \n",
       "406827  12/9/2011 12:50       4.15       12680          France  \n",
       "406828  12/9/2011 12:50       4.95       12680          France  \n",
       "\n",
       "[52438 rows x 8 columns]"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation = data[~data['CustomerID'].isin(customers_train)]\n",
    "validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.test.utils import common_texts\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "model = Word2Vec(sentences=common_texts, vector_size=100, window=5, min_count=1, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['human', 'interface', 'computer'],\n",
       " ['survey', 'user', 'computer', 'system', 'response', 'time'],\n",
       " ['eps', 'user', 'interface', 'system'],\n",
       " ['system', 'human', 'system', 'eps'],\n",
       " ['user', 'response', 'time'],\n",
       " ['trees'],\n",
       " ['graph', 'trees'],\n",
       " ['graph', 'minors', 'trees'],\n",
       " ['graph', 'minors', 'survey']]"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "common_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package word2vec_sample to\n",
      "[nltk_data]     C:\\Users\\mzhao\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping models\\word2vec_sample.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# nltk.download('abc')\n",
    "# nltk.download('punkt')\n",
    "nltk.download('word2vec_sample')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('universities', 0.7003918290138245),\n",
       " ('faculty', 0.6780907511711121),\n",
       " ('undergraduate', 0.6587096452713013)]"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.data import find\n",
    "word2vec_sample = str(find('models/word2vec_sample/pruned.word2vec.txt'))\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format(word2vec_sample, binary=False)\n",
    "model.most_similar(positive=['university'], topn = 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN\n",
    "## Simple RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, None, 8)           80000     \n",
      "                                                                 \n",
      " simple_rnn (SimpleRNN)      (None, 16)                400       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 80,400\n",
      "Trainable params: 80,400\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "max_features = 10000\n",
    "maxlen = 20\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "\n",
    "x_train = pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = pad_sequences(x_test, maxlen=maxlen)\n",
    "\n",
    "keras.backend.clear_session()\n",
    "model = models.Sequential()\n",
    "model.add(layers.Embedding(10000, 8))\n",
    "model.add(layers.SimpleRNN(16))\n",
    "\n",
    "# show model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Keras model with input and all layers' output\n",
    "layer_outputs = [layer.output for layer in model.layers]\n",
    "model_with_output = keras.Model(inputs=model.input, outputs=layer_outputs)\n",
    "\n",
    "# Compute the outputs of all layers for the input tensor\n",
    "outputs = model_with_output(x_train)\n",
    "weights = [layer.get_weights() for layer in model.layers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(32,), dtype=float32, numpy=\n",
       "array([ 0.06555703, -0.03856479,  0.07951305, -0.16726314, -0.07529365,\n",
       "        0.02212742, -0.02026786, -0.04387271,  0.01329855, -0.02792788,\n",
       "        0.12781449, -0.10273641,  0.00038305,  0.08441611, -0.08616555,\n",
       "        0.07417635,  0.03580396, -0.06360581,  0.10678901,  0.03336492,\n",
       "        0.07783552, -0.03432852,  0.02323227,  0.06389533, -0.03654022,\n",
       "        0.08909026, -0.01203921, -0.00381233, -0.03260984, -0.11244239,\n",
       "        0.02235624, -0.05727508], dtype=float32)>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# weights[0][0][x_train[0][1]], outputs[0][0]\n",
    "outputs[1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8,) (8, 16) (16, 16) (16,)\n"
     ]
    }
   ],
   "source": [
    "print(outputs[0][0][0].shape, W.shape, U.shape, b.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.05197031,  0.04177374, -0.0147732 , -0.0227857 ,  0.13540761,\n",
       "       -0.08274685, -0.02358452,  0.22519965, -0.14050737,  0.04962772,\n",
       "        0.11025104,  0.05457537,  0.0192558 , -0.13512526,  0.0694317 ,\n",
       "       -0.07166684])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W = weights[1][0]\n",
    "U = weights[1][1]\n",
    "b = weights[1][2]\n",
    "output_features = 16\n",
    "state_t = np.zeros((output_features,))\n",
    "for input_t in outputs[0][0]:\n",
    "    output_t = np.tanh(np.dot(input_t.numpy(), W) + np.dot(state_t, U) + b)\n",
    "    state_t = output_t\n",
    "state_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(32,), dtype=float32, numpy=\n",
       " array([-0.14333315, -0.47907102, -0.22011378, -0.02202254,  0.09401781,\n",
       "         0.219764  , -0.594706  ,  0.13566796, -0.49044338,  0.5106683 ,\n",
       "        -0.19827935, -0.5135692 , -0.42115653, -0.4450957 ,  0.06052525,\n",
       "         0.36579034,  0.10193661,  0.11326604, -0.5279614 ,  0.34483153,\n",
       "         0.2393107 ,  0.36822978, -0.21346939, -0.48492825,  0.02432239,\n",
       "         0.06428306, -0.14286458,  0.30609712, -0.15044975, -0.4312444 ,\n",
       "        -0.53776294,  0.35020414], dtype=float32)>,\n",
       " <tf.Tensor: shape=(32,), dtype=float32, numpy=\n",
       " array([-0.41544935,  0.17709664,  0.2055597 ,  0.41529933, -0.04139323,\n",
       "        -0.2449441 ,  0.6667583 , -0.5129134 ,  0.14518976, -0.6080745 ,\n",
       "         0.56856453,  0.32077798,  0.3311706 ,  0.39054462, -0.53130054,\n",
       "        -0.28791022, -0.48670012, -0.1805276 ,  0.31555927, -0.27831638,\n",
       "        -0.02016427, -0.48700204,  0.22928694,  0.3068616 , -0.46142042,\n",
       "        -0.375262  , -0.04258059,  0.10469109, -0.12640603,  0.766074  ,\n",
       "         0.76509035,  0.17206685], dtype=float32)>)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs[1][0], outputs[1][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([25000, 32]), (32, 32), (32,), TensorShape([20, 32]), 3)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs[1].shape, weights[1][0].shape, weights[1][2].shape, outputs[0][0].shape, len(weights[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[0]\n",
    "np.all(np.array([weights[0][0][i] for i in x_train[0]]) == outputs[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Failed to convert a NumPy array to a Tensor (Unsupported object type list).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m model\u001b[38;5;241m.\u001b[39madd(layers\u001b[38;5;241m.\u001b[39mDense(\u001b[38;5;241m1\u001b[39m, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msigmoid\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m      6\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrmsprop\u001b[39m\u001b[38;5;124m'\u001b[39m, loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbinary_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m, metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124macc\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m----> 7\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m acc \u001b[38;5;241m=\u001b[39m history\u001b[38;5;241m.\u001b[39mhistory[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124macc\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     10\u001b[0m val_acc \u001b[38;5;241m=\u001b[39m history\u001b[38;5;241m.\u001b[39mhistory[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_acc\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\py39\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\py39\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py:102\u001b[0m, in \u001b[0;36mconvert_to_eager_tensor\u001b[1;34m(value, ctx, dtype)\u001b[0m\n\u001b[0;32m    100\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m dtypes\u001b[38;5;241m.\u001b[39mas_dtype(dtype)\u001b[38;5;241m.\u001b[39mas_datatype_enum\n\u001b[0;32m    101\u001b[0m ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m--> 102\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEagerTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mValueError\u001b[0m: Failed to convert a NumPy array to a Tensor (Unsupported object type list)."
     ]
    }
   ],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Embedding(10000, 32))\n",
    "model.add(layers.SimpleRNN(32))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
    "history = model.fit(x_train, y_train, epochs=10, batch_size=128, validation_split=0.2, verbose=0)\n",
    "\n",
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs, acc, 'bo', label='Training')\n",
    "plt.plot(epochs, val_acc, 'r', label='Validation')\n",
    "plt.title('Training and validation accuracy')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs, loss, 'bo', label='Training')\n",
    "plt.plot(epochs, val_loss, 'r', label='Validation')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend(bbox_to_anchor=(1.02, 0.2), loc=2, borderaxespad=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "157/157 [==============================] - 5s 15ms/step - loss: 0.5799 - acc: 0.6869 - val_loss: 0.5117 - val_acc: 0.7396\n",
      "Epoch 2/2\n",
      "157/157 [==============================] - 2s 12ms/step - loss: 0.4421 - acc: 0.7922 - val_loss: 0.5264 - val_acc: 0.7500\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.datasets import imdb\n",
    "from keras import models, layers\n",
    "from keras.utils import pad_sequences\n",
    "from keras import backend as K\n",
    "\n",
    "max_features = 10000\n",
    "maxlen = 20\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "x_train = pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = pad_sequences(x_test, maxlen=maxlen)\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Embedding(max_features, 32))\n",
    "model.add(layers.LSTM(32))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
    "history = model.fit(x_train, y_train, epochs=2, batch_size=128, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Keras model with input and all layers' output\n",
    "layer_outputs = [layer.output for layer in model.layers]\n",
    "model_with_output = keras.Model(inputs=model.input, outputs=layer_outputs)\n",
    "\n",
    "# Compute the outputs of all layers for the input tensor\n",
    "outputs = model_with_output(x_train)\n",
    "weights = [layer.get_weights() for layer in model.layers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((32, 128), (32, 128), (128,), 3)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights[1][0].shape,weights[1][1].shape,weights[1][2].shape, len(weights[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(32,), dtype=float32, numpy=\n",
       "array([ 0.1506226 , -0.27912652, -0.26049507, -0.26200834, -0.21119316,\n",
       "       -0.17897682, -0.30369914,  0.2579301 , -0.14385766, -0.18712118,\n",
       "       -0.09781732,  0.14265549,  0.21217372,  0.33847484,  0.1827306 ,\n",
       "        0.1037639 , -0.26946834,  0.3291054 , -0.22862698, -0.18016145,\n",
       "       -0.00615121, -0.03012956, -0.19730377, -0.06631082,  0.06824851,\n",
       "       -0.08964282, -0.12964779,  0.11516143,  0.19629063, -0.3004803 ,\n",
       "        0.2790725 ,  0.34915867], dtype=float32)>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs[1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.15062273, -0.27912668, -0.26049519, -0.26200847, -0.21119327,\n",
       "        -0.17897692, -0.30369921,  0.25793025, -0.14385778, -0.18712128,\n",
       "        -0.09781735,  0.14265556,  0.21217385,  0.33847495,  0.18273075,\n",
       "         0.10376399, -0.26946848,  0.32910555, -0.22862713, -0.18016153,\n",
       "        -0.00615121, -0.03012959, -0.1973039 , -0.06631087,  0.06824854,\n",
       "        -0.08964286, -0.12964788,  0.11516147,  0.19629076, -0.3004805 ,\n",
       "         0.27907263,  0.34915878]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "def hard_sigmoid(x):\n",
    "    return np.clip(0.2 * x + 0.5, 0, 1)\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "n = 1\n",
    "units = 32\n",
    "Wi = weights[1][0][:, 0:units]\n",
    "Wf = weights[1][0][:, units:2 * units]\n",
    "Wc = weights[1][0][:, 2 * units:3 * units]\n",
    "Wo = weights[1][0][:, 3 * units:]\n",
    "\n",
    "# (5, 20) units, units * 4\n",
    "Ui = weights[1][1][:, 0:units]\n",
    "Uf = weights[1][1][:, units:2 * units]\n",
    "Uc = weights[1][1][:, 2 * units:3 * units]\n",
    "Uo = weights[1][1][:, 3 * units:]\n",
    "\n",
    "# (20,) units * 4\n",
    "bi = weights[1][2][0:units]\n",
    "bf = weights[1][2][units:2 * units]\n",
    "bc = weights[1][2][2 * units:3 * units]\n",
    "bo = weights[1][2][3 * units:]\n",
    "\n",
    "# ht_1 = np.zeros(n * units).reshape(units,-1)\n",
    "# Ct_1 = np.zeros(n * units).reshape(units,-1)\n",
    "\n",
    "ht_1 = np.zeros(n * units).reshape(-1, units)\n",
    "Ct_1 = np.zeros(n * units).reshape(-1, units)\n",
    "\n",
    "results = []\n",
    "for xt in outputs[0][0]:\n",
    "    ft = sigmoid(np.dot(xt, Wf) + np.dot(ht_1, Uf) + bf)  # forget gate\n",
    "    it = sigmoid(np.dot(xt, Wi) + np.dot(ht_1, Ui) + bi)  # input gate\n",
    "    ot = sigmoid(np.dot(xt, Wo) + np.dot(ht_1, Uo) + bo)  # output gate\n",
    "    Ct = ft * Ct_1 + it * np.tanh(np.dot(xt, Wc) + np.dot(ht_1, Uc) + bc)\n",
    "    \n",
    "    ht = ot * np.tanh(Ct)\n",
    "    ht_1 = ht  # hidden state, previous memory state\n",
    "    Ct_1 = Ct  # cell state, previous carry state\n",
    "\n",
    "    results.append(ht)\n",
    "results[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Ct.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention\n",
    "\n",
    "https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading lines...\n",
      "Read 135842 sentence pairs\n",
      "Trimmed to 10599 sentence pairs\n",
      "Counting words...\n",
      "Counted words:\n",
      "fra 4345\n",
      "eng 2803\n",
      "['je me suis amendee .', 'i m reformed .']\n"
     ]
    }
   ],
   "source": [
    "import unicodedata\n",
    "import re\n",
    "import random\n",
    "\n",
    "SOS_token = 0\n",
    "EOS_token = 1\n",
    "\n",
    "class Lang:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {0: \"SOS\", 1: \"EOS\"}\n",
    "        self.n_words = 2  # Count SOS and EOS\n",
    "\n",
    "    def addSentence(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.addWord(word)\n",
    "\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1\n",
    "            \n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "\n",
    "# Lowercase, trim, and remove non-letter characters\n",
    "\n",
    "\n",
    "def normalizeString(s):\n",
    "    s = unicodeToAscii(s.lower().strip())\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
    "    return s\n",
    "\n",
    "def readLangs(lang1, lang2, reverse=False):\n",
    "    print(\"Reading lines...\")\n",
    "\n",
    "    # Read the file and split into lines\n",
    "    lines = open('data/%s-%s.txt' % (lang1, lang2), encoding='utf-8').\\\n",
    "        read().strip().split('\\n')\n",
    "\n",
    "    # Split every line into pairs and normalize\n",
    "    pairs = [[normalizeString(s) for s in l.split('\\t')] for l in lines]\n",
    "\n",
    "    # Reverse pairs, make Lang instances\n",
    "    if reverse:\n",
    "        pairs = [list(reversed(p)) for p in pairs]\n",
    "        input_lang = Lang(lang2)\n",
    "        output_lang = Lang(lang1)\n",
    "    else:\n",
    "        input_lang = Lang(lang1)\n",
    "        output_lang = Lang(lang2)\n",
    "\n",
    "    return input_lang, output_lang, pairs\n",
    "\n",
    "MAX_LENGTH = 10\n",
    "\n",
    "eng_prefixes = (\n",
    "    \"i am \", \"i m \",\n",
    "    \"he is\", \"he s \",\n",
    "    \"she is\", \"she s \",\n",
    "    \"you are\", \"you re \",\n",
    "    \"we are\", \"we re \",\n",
    "    \"they are\", \"they re \"\n",
    ")\n",
    "\n",
    "\n",
    "def filterPair(p):\n",
    "    return len(p[0].split(' ')) < MAX_LENGTH and \\\n",
    "        len(p[1].split(' ')) < MAX_LENGTH and \\\n",
    "        p[1].startswith(eng_prefixes)\n",
    "\n",
    "\n",
    "def filterPairs(pairs):\n",
    "    return [pair for pair in pairs if filterPair(pair)]\n",
    "\n",
    "def prepareData(lang1, lang2, reverse=False):\n",
    "    input_lang, output_lang, pairs = readLangs(lang1, lang2, reverse)\n",
    "    print(\"Read %s sentence pairs\" % len(pairs))\n",
    "    pairs = filterPairs(pairs)\n",
    "    print(\"Trimmed to %s sentence pairs\" % len(pairs))\n",
    "    print(\"Counting words...\")\n",
    "    for pair in pairs:\n",
    "        input_lang.addSentence(pair[0])\n",
    "        output_lang.addSentence(pair[1])\n",
    "    print(\"Counted words:\")\n",
    "    print(input_lang.name, input_lang.n_words)\n",
    "    print(output_lang.name, output_lang.n_words)\n",
    "    return input_lang, output_lang, pairs\n",
    "\n",
    "\n",
    "input_lang, output_lang, pairs = prepareData('eng', 'fra', True)\n",
    "print(random.choice(pairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def indexesFromSentence(lang, sentence):\n",
    "    return [lang.word2index[word] for word in sentence.split(' ')]\n",
    "\n",
    "def tensorFromSentence(lang, sentence):\n",
    "    indexes = indexesFromSentence(lang, sentence)\n",
    "    indexes.append(EOS_token)\n",
    "    return torch.tensor(indexes, dtype=torch.long).view(-1, 1)\n",
    "\n",
    "def tensorsFromPair(pair):\n",
    "    input_tensor = tensorFromSentence(input_lang, pair[0])\n",
    "    target_tensor = tensorFromSentence(output_lang, pair[1])\n",
    "    return (input_tensor, target_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "MAX_LENGTH = 10\n",
    "\n",
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        output = embedded\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size)\n",
    "    \n",
    "class AttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, dropout_p=0.1, max_length=MAX_LENGTH):\n",
    "        super(AttnDecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.dropout_p = dropout_p\n",
    "        self.max_length = max_length\n",
    "\n",
    "        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
    "        self.attn = nn.Linear(self.hidden_size * 2, self.max_length)\n",
    "        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
    "        self.dropout = nn.Dropout(self.dropout_p)\n",
    "        self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n",
    "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
    "\n",
    "    def forward(self, input, hidden, encoder_outputs):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        embedded = self.dropout(embedded)\n",
    "        attn_weights = F.softmax(\n",
    "            self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1)\n",
    "        attn_applied = torch.bmm(attn_weights.unsqueeze(0),\n",
    "                                 encoder_outputs.unsqueeze(0))\n",
    "#         print('Attn:')\n",
    "#         print(embedded.shape, hidden.shape, torch.cat((embedded[0], hidden[0]), 1).shape)\n",
    "#         print(self.attn(torch.cat((embedded[0], hidden[0]), 1)).shape, attn_weights.shape, encoder_outputs.shape)\n",
    "#         print(attn_applied.shape)\n",
    "        output = torch.cat((embedded[0], attn_applied[0]), 1)\n",
    "        output = self.attn_combine(output).unsqueeze(0)\n",
    "\n",
    "        output = F.relu(output)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        \n",
    "\n",
    "\n",
    "        output = F.log_softmax(self.out(output[0]), dim=1)\n",
    "        return output, hidden, attn_weights\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size)\n",
    "    \n",
    "teacher_forcing_ratio = 0.5\n",
    "\n",
    "def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, \n",
    "          max_length=MAX_LENGTH):\n",
    "    encoder_hidden = encoder.initHidden()\n",
    "\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    input_length = input_tensor.size(0)\n",
    "    target_length = target_tensor.size(0)\n",
    "\n",
    "    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "\n",
    "    loss = 0\n",
    "\n",
    "    for ei in range(input_length):\n",
    "        encoder_output, encoder_hidden = encoder(\n",
    "            input_tensor[ei], encoder_hidden)\n",
    "        encoder_outputs[ei] = encoder_output[0, 0]\n",
    "\n",
    "    decoder_input = torch.tensor([[SOS_token]], device=device)\n",
    "\n",
    "    decoder_hidden = encoder_hidden\n",
    "\n",
    "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "\n",
    "    if use_teacher_forcing:\n",
    "        # Teacher forcing: Feed the target as the next input\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            loss += criterion(decoder_output, target_tensor[di])\n",
    "            decoder_input = target_tensor[di]  # Teacher forcing\n",
    "\n",
    "    else:\n",
    "        # Without teacher forcing: use its own predictions as the next input\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            topv, topi = decoder_output.topk(1)\n",
    "            decoder_input = topi.squeeze().detach()  # detach from history as input\n",
    "\n",
    "            loss += criterion(decoder_output, target_tensor[di])\n",
    "            if decoder_input.item() == EOS_token:\n",
    "                break\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return loss.item() / target_length \n",
    "\n",
    "import time\n",
    "import math\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.switch_backend('agg')\n",
    "import matplotlib.ticker as ticker\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def showPlot(points):\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots()\n",
    "    # this locator puts ticks at regular intervals\n",
    "    loc = ticker.MultipleLocator(base=0.2)\n",
    "    ax.yaxis.set_major_locator(loc)\n",
    "    plt.plot(points)\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))\n",
    "\n",
    "def trainIters(encoder, decoder, n_iters, print_every=1000, plot_every=100, learning_rate=0.01):\n",
    "    start = time.time()\n",
    "    plot_losses = []\n",
    "    print_loss_total = 0  # Reset every print_every\n",
    "    plot_loss_total = 0  # Reset every plot_every\n",
    "\n",
    "    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
    "    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
    "    training_pairs = [tensorsFromPair(random.choice(pairs))\n",
    "                      for i in range(n_iters)]\n",
    "    criterion = nn.NLLLoss()\n",
    "\n",
    "    for iter in range(1, n_iters + 1):\n",
    "        training_pair = training_pairs[iter - 1]\n",
    "        input_tensor = training_pair[0]\n",
    "        target_tensor = training_pair[1]\n",
    "\n",
    "        loss = train(input_tensor, target_tensor, encoder,\n",
    "                     decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "        print_loss_total += loss\n",
    "        plot_loss_total += loss\n",
    "\n",
    "        if iter % print_every == 0:\n",
    "            print_loss_avg = print_loss_total / print_every\n",
    "            print_loss_total = 0\n",
    "            print('%s (%d %d%%) %.4f' % (timeSince(start, iter / n_iters),\n",
    "                                         iter, iter / n_iters * 100, print_loss_avg))\n",
    "\n",
    "        if iter % plot_every == 0:\n",
    "            plot_loss_avg = plot_loss_total / plot_every\n",
    "            plot_losses.append(plot_loss_avg)\n",
    "            plot_loss_total = 0\n",
    "\n",
    "    showPlot(plot_losses)\n",
    "    \n",
    "hidden_size = 256\n",
    "encoder1 = EncoderRNN(input_lang.n_words, hidden_size).to(device)\n",
    "attn_decoder1 = AttnDecoderRNN(hidden_size, output_lang.n_words, dropout_p=0.1).to(device)\n",
    "\n",
    "trainIters(encoder1, attn_decoder1, 10000, print_every=5000)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> je suis maintenant pret tom .\n",
      "= i m ready now tom .\n",
      "< he he is . <EOS>\n",
      "\n",
      "> tu es encore jeune .\n",
      "= you re still green .\n",
      "< he he is . <EOS>\n",
      "\n",
      "> vous allez me manquer .\n",
      "= i m going to miss you .\n",
      "< he he is . <EOS>\n",
      "\n",
      "> vous n etes pas tres ordonne .\n",
      "= you re not very tidy .\n",
      "< he he is . <EOS>\n",
      "\n",
      "> je me trouve juste derriere lui .\n",
      "= i m right behind him .\n",
      "< he he is . <EOS>\n",
      "\n",
      "> nous y sommes pretes .\n",
      "= we re ready for this .\n",
      "< he he is . <EOS>\n",
      "\n",
      "> vous etes surmenee .\n",
      "= you re overworked .\n",
      "< he he is . <EOS>\n",
      "\n",
      "> je suis plus elegante que vous .\n",
      "= i m smarter than you .\n",
      "< he he is . <EOS>\n",
      "\n",
      "> nous nous en sortons tres bien .\n",
      "= we re doing great .\n",
      "< he he is . <EOS>\n",
      "\n",
      "> je suis plutot heureux .\n",
      "= i m fairly happy .\n",
      "< he he is . <EOS>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def evaluate(encoder, decoder, sentence, max_length=MAX_LENGTH):\n",
    "    with torch.no_grad():\n",
    "        input_tensor = tensorFromSentence(input_lang, sentence)\n",
    "        input_length = input_tensor.size()[0]\n",
    "        encoder_hidden = encoder.initHidden()\n",
    "\n",
    "        encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "\n",
    "        for ei in range(input_length):\n",
    "            encoder_output, encoder_hidden = encoder(input_tensor[ei],\n",
    "                                                     encoder_hidden)\n",
    "            encoder_outputs[ei] += encoder_output[0, 0]\n",
    "\n",
    "        decoder_input = torch.tensor([[SOS_token]], device=device)  # SOS\n",
    "\n",
    "        decoder_hidden = encoder_hidden\n",
    "\n",
    "        decoded_words = []\n",
    "        decoder_attentions = torch.zeros(max_length, max_length)\n",
    "\n",
    "        for di in range(max_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            decoder_attentions[di] = decoder_attention.data\n",
    "            topv, topi = decoder_output.data.topk(1)\n",
    "            if topi.item() == EOS_token:\n",
    "                decoded_words.append('<EOS>')\n",
    "                break\n",
    "            else:\n",
    "                decoded_words.append(output_lang.index2word[topi.item()])\n",
    "\n",
    "            decoder_input = topi.squeeze().detach()\n",
    "\n",
    "        return decoded_words, decoder_attentions[:di + 1]\n",
    "    \n",
    "def evaluateRandomly(encoder, decoder, n=10):\n",
    "    for i in range(n):\n",
    "        pair = random.choice(pairs)\n",
    "        print('>', pair[0])\n",
    "        print('=', pair[1])\n",
    "        output_words, attentions = evaluate(encoder, decoder, pair[0])\n",
    "        output_sentence = ' '.join(output_words)\n",
    "        print('<', output_sentence)\n",
    "        print('')    \n",
    "        \n",
    "evaluateRandomly(encoder1, attn_decoder1)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pair:\n",
      "['j ai ans .', 'i m .'] tensor([[2],\n",
      "        [3],\n",
      "        [4],\n",
      "        [5],\n",
      "        [1]]) tensor([[2],\n",
      "        [3],\n",
      "        [4],\n",
      "        [1]])\n",
      "length: 5 4\n",
      "torch.Size([10, 256]) tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "ei= 0\n",
      "Encoder\n",
      "torch.Size([1]) tensor([2]) torch.Size([1, 1, 256])\n",
      "torch.Size([1, 1, 256]) tensor(True)\n",
      "ei= 1\n",
      "Encoder\n",
      "torch.Size([1]) tensor([3]) torch.Size([1, 1, 256])\n",
      "torch.Size([1, 1, 256]) tensor(True)\n",
      "ei= 2\n",
      "Encoder\n",
      "torch.Size([1]) tensor([4]) torch.Size([1, 1, 256])\n",
      "torch.Size([1, 1, 256]) tensor(True)\n",
      "ei= 3\n",
      "Encoder\n",
      "torch.Size([1]) tensor([5]) torch.Size([1, 1, 256])\n",
      "torch.Size([1, 1, 256]) tensor(True)\n",
      "ei= 4\n",
      "Encoder\n",
      "torch.Size([1]) tensor([1]) torch.Size([1, 1, 256])\n",
      "torch.Size([1, 1, 256]) tensor(True)\n",
      "11111111\n",
      "torch.Size([10, 256])\n",
      "Attn:\n",
      "torch.Size([1, 1, 256]) torch.Size([1, 1, 256]) torch.Size([1, 512])\n",
      "torch.Size([1, 10]) torch.Size([1, 10]) torch.Size([10, 256])\n",
      "torch.Size([1, 1, 256])\n",
      "torch.Size([1, 1, 256])\n",
      "torch.Size([1, 1, 256]) torch.Size([1, 1, 256])\n",
      "check1: tensor(True)\n",
      "di 0\n",
      "torch.Size([1, 2803]) tensor([[-7.5620]], grad_fn=<TopkBackward0>)\n",
      "Attn:\n",
      "torch.Size([1, 1, 256]) torch.Size([1, 1, 256]) torch.Size([1, 512])\n",
      "torch.Size([1, 10]) torch.Size([1, 10]) torch.Size([10, 256])\n",
      "torch.Size([1, 1, 256])\n",
      "torch.Size([1, 1, 256])\n",
      "torch.Size([1, 1, 256]) torch.Size([1, 1, 256])\n",
      "check1: tensor(True)\n",
      "di 1\n",
      "torch.Size([1, 2803]) tensor([[-7.6206]], grad_fn=<TopkBackward0>)\n",
      "Attn:\n",
      "torch.Size([1, 1, 256]) torch.Size([1, 1, 256]) torch.Size([1, 512])\n",
      "torch.Size([1, 10]) torch.Size([1, 10]) torch.Size([10, 256])\n",
      "torch.Size([1, 1, 256])\n",
      "torch.Size([1, 1, 256])\n",
      "torch.Size([1, 1, 256]) torch.Size([1, 1, 256])\n",
      "check1: tensor(True)\n",
      "di 2\n",
      "torch.Size([1, 2803]) tensor([[-7.6017]], grad_fn=<TopkBackward0>)\n",
      "Attn:\n",
      "torch.Size([1, 1, 256]) torch.Size([1, 1, 256]) torch.Size([1, 512])\n",
      "torch.Size([1, 10]) torch.Size([1, 10]) torch.Size([10, 256])\n",
      "torch.Size([1, 1, 256])\n",
      "torch.Size([1, 1, 256])\n",
      "torch.Size([1, 1, 256]) torch.Size([1, 1, 256])\n",
      "check1: tensor(True)\n",
      "di 3\n",
      "torch.Size([1, 2803]) tensor([[-7.6512]], grad_fn=<TopkBackward0>)\n"
     ]
    }
   ],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        output = embedded\n",
    "        print('Encoder')\n",
    "        print(input.shape, input, output.shape)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        print(output.shape, (output==hidden).all())\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size)\n",
    "    \n",
    "class AttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, dropout_p=0.1, max_length=MAX_LENGTH):\n",
    "        super(AttnDecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.dropout_p = dropout_p\n",
    "        self.max_length = max_length\n",
    "\n",
    "        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
    "        self.attn = nn.Linear(self.hidden_size * 2, self.max_length)\n",
    "        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
    "        self.dropout = nn.Dropout(self.dropout_p)\n",
    "        self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n",
    "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
    "\n",
    "    def forward(self, input, hidden, encoder_outputs):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        embedded = self.dropout(embedded)\n",
    "        attn_weights = F.softmax(\n",
    "            self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1)\n",
    "        attn_applied = torch.bmm(attn_weights.unsqueeze(0),\n",
    "                                 encoder_outputs.unsqueeze(0))\n",
    "        print('Attn:')\n",
    "        print(embedded.shape, hidden.shape, torch.cat((embedded[0], hidden[0]), 1).shape)\n",
    "        print(self.attn(torch.cat((embedded[0], hidden[0]), 1)).shape, attn_weights.shape, encoder_outputs.shape)\n",
    "        print(attn_applied.shape)\n",
    "        output = torch.cat((embedded[0], attn_applied[0]), 1)\n",
    "        output = self.attn_combine(output).unsqueeze(0)\n",
    "        print(output.shape)\n",
    "        output = F.relu(output)\n",
    "        print(output.shape, hidden.shape)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        print('check1:', (output==hidden).all())\n",
    "        output = F.log_softmax(self.out(output[0]), dim=1)\n",
    "        return output, hidden, attn_weights\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size)\n",
    "\n",
    "hidden_size = 256\n",
    "input_lang_n_words = 4345\n",
    "output_lang_n_words = 2803\n",
    "encoder = EncoderRNN(input_lang_n_words, hidden_size)\n",
    "decoder = AttnDecoderRNN(hidden_size, output_lang_n_words, dropout_p=0.1)\n",
    "n_iters = 1\n",
    "learning_rate=0.01\n",
    "# trainIters(encoder1, attn_decoder1, 1, print_every=5000)\n",
    "\n",
    "plot_losses = []\n",
    "print_loss_total = 0  # Reset every print_every\n",
    "plot_loss_total = 0  # Reset every plot_every\n",
    "\n",
    "encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
    "decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
    "training_pairs = [tensorsFromPair(random.choice(pairs))\n",
    "                  for i in range(n_iters)]\n",
    "training_pairs = [tensorsFromPair(pairs[i]) for i in range(n_iters)]\n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "# pairs = language french english pair\n",
    "# training_pairs = tensor (random.choice from pairs)\n",
    "\n",
    "for iter in range(1, n_iters + 1):\n",
    "    training_pair = training_pairs[iter - 1]\n",
    "    input_tensor = training_pair[0]\n",
    "    target_tensor = training_pair[1]\n",
    "    # print(pairs[0], training_pairs)\n",
    "    # loss = train(input_tensor, target_tensor, encoder,\n",
    "    #               decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "    \n",
    "    print(\"pair:\")\n",
    "    print(pairs[0], input_tensor, target_tensor)\n",
    "    max_length = 10\n",
    "\n",
    "    encoder_hidden = encoder.initHidden()\n",
    "    # print(encoder_hidden.shape)\n",
    "\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    input_length = input_tensor.size(0)\n",
    "    target_length = target_tensor.size(0)\n",
    "    print(\"length:\", input_length, target_length)\n",
    "    encoder_outputs = torch.zeros(max_length, encoder.hidden_size)\n",
    "    print(encoder_outputs.shape, encoder_outputs)\n",
    "    loss = 0\n",
    "    # encoder_outputs includes hidden states\n",
    "    for ei in range(input_length):\n",
    "        print('ei=', ei)\n",
    "        encoder_output, encoder_hidden = encoder(\n",
    "            input_tensor[ei], encoder_hidden)\n",
    "        # GRU/RNN encoder_output = encoder_hidden\n",
    "        # print(ei, encoder_output, encoder_hidden)\n",
    "        encoder_outputs[ei] = encoder_output[0, 0]\n",
    "        # print(ei, input_tensor[ei], encoder_hidden.shape, encoder_output.shape)\n",
    "    # print(SOS_token)\n",
    "    decoder_input = torch.tensor([[SOS_token]])\n",
    "    decoder_input = torch.tensor(0)\n",
    "    decoder_hidden = encoder_hidden\n",
    "    print(\"11111111\")\n",
    "    \n",
    "    print(encoder_outputs.shape)\n",
    "    # print(encoder_outputs[4], encoder_hidden)\n",
    "    teacher_forcing_ratio = 0.5\n",
    "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "    use_teacher_forcing = False\n",
    "\n",
    "    # encoder_outputs never update\n",
    "    if use_teacher_forcing:\n",
    "        # Teacher forcing: Feed the target as the next input\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            loss += criterion(decoder_output, target_tensor[di])\n",
    "            decoder_input = target_tensor[di]  # Teacher forcing\n",
    "\n",
    "    else:\n",
    "        # Without teacher forcing: use its own predictions as the next input\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            topv, topi = decoder_output.topk(1)\n",
    "            #print(di, decoder_output, topv, topi, decoder_input)\n",
    "            #print(decoder_attention.sum())\n",
    "            decoder_input = topi.squeeze().detach()  # detach from history as input\n",
    "            print('di', di)\n",
    "            print(decoder_output.shape, topv)\n",
    "            loss += criterion(decoder_output, target_tensor[di])\n",
    "            if decoder_input.item() == EOS_token:\n",
    "                break\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    loss = loss.item() / target_length  \n",
    "    print_loss_total += loss\n",
    "    plot_loss_total += loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q\n",
      " tensor([[-0.7758,  0.7220],\n",
      "        [ 0.1698, -0.3775],\n",
      "        [ 0.7690,  0.4455]])\n",
      "K\n",
      " tensor([[ 1.4530,  1.0118],\n",
      "        [ 1.2450,  0.7545],\n",
      "        [-0.3294,  1.0256]])\n",
      "V\n",
      " tensor([[ 0.7514,  0.4281],\n",
      "        [-1.6614,  1.0047],\n",
      "        [-0.3136, -1.3628]])\n",
      "Values\n",
      " tensor([[-0.3694, -0.4792],\n",
      "        [-0.4340,  0.1339],\n",
      "        [-0.3340,  0.3340]])\n",
      "Attention\n",
      " tensor([[0.2146, 0.2109, 0.5745],\n",
      "        [0.3510, 0.3667, 0.2823],\n",
      "        [0.4536, 0.3735, 0.1728]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAm8AAAE6CAYAAABAnbVRAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy88F64QAAAACXBIWXMAAA9hAAAPYQGoP6dpAAB9/0lEQVR4nO3dd1xT1/sH8E9kg4AMWYqASFUEJxXBuuoerauOOqlWW0fVqnXUWtDW2dbRWrW2zjprHbVurIpa0SqKEzcqKogLAspMzu8Pf+TrzTnADSQS5Hm/Xnm9zJOTm3OT3HC89znPUTDGGAghhBBCSKlQrqQ7QAghhBBC5KPBGyGEEEJIKUKDN0IIIYSQUoQGb4QQQgghpQgN3gghhBBCShEavBFCCCGElCI0eCOEEEIIKUVo8EYIIYQQUorQ4I0QQgghpBShwVspsWrVKigUCs3N1NQUlStXxkcffYT79+8b5DUVCgUiIiI09y9fvoyIiAjcvn2baxsWFgZvb2+D9KMotPv+Jrl9+zYUCgVWrVqliUVEREChUJRcp0qBw4cPQ6FQ4M8//yy0rS7fZ7nfNWP8jLT7nvc7IzrGSxtvb2+EhYWVdDcIMQjTku4A0c3KlStRo0YNZGRk4MiRI5g1axaioqJw4cIF2NjY6PW1oqOjUblyZc39y5cvY9q0aWjevDn3h23q1KkYPXq0Xl+fyPfxxx+jXbt2Jd2NN0ZZ/T537NgR0dHRcHd3L+muFNu2bdtgZ2dX0t0gxCBo8FbKBAQEICgoCADQokULqFQqfPPNN9i+fTv69u2r19dq1KiR7La+vr56fW2im8qVK0sG2mVVRkYGLC0ti32Gq6x+nytWrIiKFSuWdDf0ol69eiXdBUIMhi6blnJ5A6w7d+4AADIzMzF58mT4+PjA3NwclSpVwogRI5CSkiJ53sGDB9G8eXM4OTnBysoKVapUQffu3fHixQtNm1cvqaxatQo9evQA8HLQmHf5Nu/Snegyk9y+eHt7o1OnTti7dy/q168PKysr1KhRAytWrJC0e/ToEYYPHw5/f3+UL18eLi4uePfdd3H06NEiv3/Z2dn49ttvUaNGDVhYWKBixYr46KOP8OjRoyL1EQDu37+PoUOHwtPTE+bm5vDw8MAHH3yAhw8fatrcvXsX/fr1g4uLCywsLFCzZk388MMPUKvVkm09ePAAPXv2hK2tLezt7dGrVy8kJSVxrym6JKdLn48dO4aQkBBYWlqiUqVKmDp1Kn777TfZl9B27NiBkJAQWFtbw9bWFq1bt0Z0dLTm8e3bt0OhUOCff/7hnrtkyRIoFAqcP39eEzt9+jTef/99ODo6wtLSEvXq1cMff/wheV7eJb79+/dj0KBBqFixIqytrZGVlVVgX3NycjBlyhR4eHjAzs4OrVq1wtWrVyVtRN9npVKJIUOGwMnJCeXLl0e7du1w7do14Wvs2rULdevWhYWFBXx8fPD9998L2zHGsHjxYtStWxdWVlZwcHDABx98gFu3bknaNW/eHAEBATh16hSaNGkCa2trVK1aFbNnz+a+MyJy+y66bJr32tHR0QgNDYWVlRW8vb2xcuVKzb7Wr18f1tbWCAwMxN69e7ntXr9+HX369JF833/++WdJm7zL2hs2bCj08zl79iw6deqk2Z6Hhwc6duyIe/fuadqILpvKOe7y0hK+//57zJs3Dz4+PihfvjxCQkJw4sQJyfZu3bqF3r17w8PDAxYWFnB1dUXLli0RGxtb4OdBSLExUiqsXLmSAWCnTp2SxBcuXMgAsGXLljG1Ws3atm3LTE1N2dSpU9n+/fvZ999/z2xsbFi9evVYZmYmY4yx+Ph4ZmlpyVq3bs22b9/ODh8+zNatW8f69+/Pnj17ptk2ABYeHs4YYyw5OZnNnDmTAWA///wzi46OZtHR0Sw5OZkxxtjAgQOZl5eX5rly+8IYY15eXqxy5crM39+frVmzhu3bt4/16NGDAWBRUVGadleuXGHDhg1jGzduZIcPH2Y7d+5kgwcPZuXKlWOHDh2SvC+v9j0/KpWKtWvXjtnY2LBp06axyMhI9ttvv7FKlSoxf39/9uLFC537eO/ePebu7s6cnZ3ZvHnz2IEDB9imTZvYoEGDWFxcnOa9rFSpEqtYsSJbunQp27t3Lxs5ciQDwIYNG6bZ1osXL1jNmjWZvb09++mnn9i+ffvYqFGjWJUqVRgAtnLlSk3b8PBwpn04y+3zuXPnmKWlJatduzbbuHEj27FjB+vQoQPz9vZmAFh8fHyB7+O6desYANamTRu2fft2tmnTJtagQQNmbm7Ojh49yhhjLCcnh7m4uLC+fftyz2/YsCGrX7++5v7BgweZubk5a9KkCdu0aRPbu3cvCwsL4/Y575ioVKkSGzp0KNuzZw/7888/WW5urrCfhw4dYgCYt7c369u3L9u1axfbsGEDq1KlCvPz85M8T/R9btGiBbOwsGAzZsxg+/fvZ+Hh4axq1arcd+3AgQPMxMSEvfPOO2zr1q1s8+bN7O2339Z8bq8aMmQIMzMzY+PGjWN79+5l69evZzVq1GCurq4sKSlJ065Zs2bMycmJ+fn5saVLl7LIyEg2fPhwBoCtXr26wM9Hl77nvaevfuZ5r129enW2fPlytm/fPtapUycGgE2bNo0FBgayDRs2sN27d7NGjRoxCwsLdv/+fc3zL126xOzt7VlgYCBbs2YN279/Pxs3bhwrV64ci4iI0PnzSU9PZ05OTiwoKIj98ccfLCoqim3atIl9+umn7PLly5rteXl5sYEDB2ruyz3u4uPjNf1o164d2759O9u+fTsLDAxkDg4OLCUlRdO2evXqrFq1auz3339nUVFRbMuWLWzcuHHc7xEh+kaDt1Ii70f1xIkTLCcnh6WlpbGdO3eyihUrMltbW5aUlMT27t3LALC5c+dKnrtp0ybNAI8xxv78808GgMXGxhb4mto/7Js3b2YAhD9M2n/s5PaFsZc/spaWluzOnTuaWEZGBnN0dGSffPJJvv3Lzc1lOTk5rGXLlqxr164F9l1kw4YNDADbsmWLJH7q1CkGgC1evFjnPg4aNIiZmZlJ/ohomzRpEgPATp48KYkPGzaMKRQKdvXqVcYYY0uWLGEA2F9//SVpN2TIENmDNzl97tGjB7OxsWGPHj3SxFQqFfP39y908KZSqZiHhwcLDAxkKpVKE09LS2MuLi4sNDRUExs7diyzsrKS/PG7fPkyA8B++uknTaxGjRqsXr16LCcnR/JanTp1Yu7u7prXyTsmBgwYkG//XpU3OOjQoYMk/scffzAALDo6WhPT/j7v2bOHAWALFy6UPHfGjBncdy04OJh5eHiwjIwMTUypVDJHR0fJZxQdHc0AsB9++EGyzYSEBGZlZcUmTJigiTVr1kz4nfH392dt27YtcL916Xt+gzcA7PTp05rYkydPmImJCbOyspIM1GJjYxkA9uOPP2pibdu2ZZUrV2apqamS1x85ciSztLRkT58+ZYzJ/3xOnz7NALDt27cXuN/agze5x13e4C0wMFAyoP/vv/8YALZhwwbGGGOPHz9mANiCBQsK7AchhkCXTUuZRo0awczMDLa2tujUqRPc3NywZ88euLq64uDBgwDAXSro0aMHbGxsNJes6tatC3NzcwwdOhSrV6/mLtHog9y+5Klbty6qVKmiuW9paYm33npLczk4z9KlS1G/fn1YWlrC1NQUZmZm+OeffxAXF6dzH3fu3IkKFSrgvffeQ25uruZWt25duLm54fDhwzr3cc+ePWjRogVq1qyZ7+sePHgQ/v7+aNiwoSQeFhYGxpjmvTt06BBsbW3x/vvvS9r16dNH9j7K6XNUVBTeffddODs7a2LlypVDz549C93+1atX8eDBA/Tv3x/lyv3v56R8+fLo3r07Tpw4obkUP2jQIGRkZGDTpk2aditXroSFhYVmn27cuIErV65o8jdf/Vw6dOiAxMRE7hJa9+7dZb8fALj3s3bt2gDAfddedejQIQDg8kq1P4vnz5/j1KlT6NatGywtLTVxW1tbvPfee5K2O3fuhEKhQL9+/ST76ebmhjp16nDfPzc3N+47U7t27QL7rUvfC+Lu7o4GDRpo7js6OsLFxQV169aFh4eHJp73vX81jeOff/5B165dYW1tzX2emZmZ3KXIwj6fatWqwcHBARMnTsTSpUtx+fJlWfsg97jL07FjR5iYmOTbD0dHR/j6+uK7777DvHnzcPbsWVmXsAnRBxq8lTJr1qzBqVOncPbsWTx48ADnz59H48aNAQBPnjyBqakpl3CsUCjg5uaGJ0+eAHiZjH3gwAG4uLhgxIgR8PX1ha+vLxYuXKi3fsrtSx4nJyduGxYWFsjIyNDcnzdvHoYNG4bg4GBs2bIFJ06cwKlTp9CuXTtJO7kePnyIlJQUmJubw8zMTHJLSkrC48ePde7jo0ePCp048OTJE+Fsvrw/gnnvzZMnT+Dq6sq1c3NzK3zndOhzfq8jimnL62t++6NWq/Hs2TMAQK1atfD2229rcqVUKhXWrl2Lzp07w9HREQA0eYHjx4/nPpPhw4cDAPe56DozUvs9sbCwAIACv0N532ft52p/Fs+ePYNarRZ+Rtqxhw8fgjEGV1dXbl9PnDhRpO9fcfpekLzP51Xm5uZc3NzcHMDLQVvea+fm5uKnn37i9rFDhw4A+M+zsM/H3t4eUVFRqFu3Lr788kvUqlULHh4eCA8PR05OTr77IPe4k9uPvBzOtm3bYu7cuahfvz4qVqyIUaNGIS0tLd9+EKIPNNu0lKlZs6Zmtqk2Jycn5Obm4tGjR5JBE2MMSUlJePvttzWxJk2aoEmTJlCpVDh9+jR++uknjBkzBq6urujdu3ex+6lLX+Rau3YtmjdvjiVLlkjiRf2hdHZ2hpOTkzDBGnh5tkRXFStWlCRNizg5OSExMZGLP3jwQNOvvHb//fcf1040YaE4nJycJJMpdHmdvD9w+e1PuXLl4ODgoIl99NFHGD58OOLi4nDr1i0kJibio48+0jyet++TJ09Gt27dhK9ZvXp1yf3XUTst7/v85MkTyR917ffIwcEBCoVC+N5px5ydnaFQKHD06FHNwOBVopgh+24IDg4OMDExQf/+/TFixAhhGx8fH523GxgYiI0bN4IxhvPnz2PVqlWYPn06rKysMGnSJOFz5B53uvDy8sLy5csBANeuXcMff/yBiIgIZGdnY+nSpTpvjxC56MzbG6Rly5YAXg5yXrVlyxY8f/5c8/irTExMEBwcrJn5debMmXy3L+cMRXH6UhiFQsH9QTt//rxkVqMuOnXqhCdPnkClUiEoKIi7aQ8S5Gjfvj0OHTrEXdp7VcuWLXH58mXuvV6zZg0UCgVatGgB4OWs3rS0NOzYsUPSbv369Tr3qyDNmjXDwYMHJWdA1Go1Nm/eXOhzq1evjkqVKmH9+vVgjGniz58/x5YtWzQzUPN8+OGHsLS0xKpVq7Bq1SpUqlQJbdq0kWzPz88P586dE34mQUFBRRpUF1feZ7Ju3TpJXPuzsLGxQcOGDbF161bN2Sfg5X8w/v77b0nbTp06gTGG+/fvC/czMDDwtfbdEKytrdGiRQucPXsWtWvXFu6n6IyiXAqFAnXq1MH8+fNRoUKFAn+/5B53RfXWW2/hq6++QmBgYIH9IEQf6MzbG6R169Zo27YtJk6cCKVSicaNG+P8+fMIDw9HvXr10L9/fwAv88YOHjyIjh07okqVKsjMzNSUj2jVqlW+2w8ICAAALFu2DLa2trC0tISPj4/wx1duX3TRqVMnfPPNNwgPD0ezZs1w9epVTJ8+HT4+PsjNzdV5e71798a6devQoUMHjB49Gg0bNoSZmRnu3buHQ4cOoXPnzujatatO25w+fTr27NmDpk2b4ssvv0RgYCBSUlKwd+9ejB07FjVq1MDnn3+ONWvWoGPHjpg+fTq8vLywa9cuLF68GMOGDcNbb70FABgwYADmz5+PAQMGYMaMGfDz88Pu3buxb98+nfe1IFOmTMHff/+Nli1bYsqUKbCyssLSpUvx/PlzAJDksmkrV64c5s6di759+6JTp0745JNPkJWVhe+++w4pKSmYPXu2pH2FChXQtWtXrFq1CikpKRg/fjy3/V9++QXt27dH27ZtERYWhkqVKuHp06eIi4vDmTNnZA0q9a1NmzZo2rQpJkyYgOfPnyMoKAj//vsvfv/9d67tN998g3bt2qF169YYN24cVCoV5syZAxsbGzx9+lTTrnHjxhg6dCg++ugjnD59Gk2bNoWNjQ0SExNx7NgxBAYGYtiwYa+174awcOFCvPPOO2jSpAmGDRsGb29vpKWl4caNG/j777+5XLPC7Ny5E4sXL0aXLl1QtWpVMMawdetWpKSkoHXr1vk+T+5xJ9f58+cxcuRI9OjRA35+fjA3N8fBgwdx/vz5fM/+EaI3JTZVgugkv1Ih2jIyMtjEiROZl5cXMzMzY+7u7mzYsGGSEiDR0dGsa9euzMvLi1lYWDAnJyfWrFkztmPHDsm2IJixuWDBAubj48NMTEwkMx61Z+fJ7QtjL2eFdezYkduXZs2asWbNmmnuZ2VlsfHjx7NKlSoxS0tLVr9+fbZ9+3bha4v6LpKTk8O+//57VqdOHWZpacnKly/PatSowT755BN2/fp1nfvI2MvZgoMGDWJubm7MzMyMeXh4sJ49e7KHDx9q2ty5c4f16dOHOTk5MTMzM1a9enX23XffSWZsMvay9Ej37t1Z+fLlma2tLevevTs7fvy47Nmmcvt89OhRFhwczCwsLJibmxv74osv2Jw5cxgAyezQ/Gzfvp0FBwczS0tLZmNjw1q2bMn+/fdfYdv9+/czAAwAu3btmrDNuXPnWM+ePZmLiwszMzNjbm5u7N1332VLly7VtJF7TOTJm824efNmSTxvduGr76foO5WSksIGDRrEKlSowKytrVnr1q3ZlStXhN+1HTt2sNq1azNzc3NWpUoVNnv2bOFnxBhjK1asYMHBwczGxoZZWVkxX19fNmDAAMnszmbNmrFatWpxzxX1U0Ru3/ObbSp67fy+XwDYiBEjJLH4+Hg2aNAgVqlSJWZmZsYqVqzIQkND2bfffqtpI/fzuXLlCvvwww+Zr68vs7KyYvb29qxhw4Zs1apVXP9enW3KmLzjLu/1vvvuO+G+5b1fDx8+ZGFhYaxGjRrMxsaGlS9fntWuXZvNnz8/33I1hOiLgrFXrnUQQsj/a9OmDW7fvp1vIVpCCCElgy6bEkIwduxY1KtXD56ennj69CnWrVuHyMhITTI2IYQQ40GDN0IIVCoVvv76ayQlJUGhUMDf3x+///47+vXrV9JdI4QQooUumxJCCCGElCJGXyrkyJEjeO+99+Dh4QGFQoHt27eXdJcIIYQQUgoVZUwRFRWFBg0awNLSElWrVhXW8NuyZQv8/f1hYWEBf39/bNu2zQC9/x+jH7w9f/4cderUwaJFi0q6K4QQQggpxXQdU8THx6NDhw5o0qQJzp49iy+//BKjRo3Cli1bNG2io6PRq1cv9O/fH+fOnUP//v3Rs2dPnDx50lC7UboumyoUCmzbtg1dunQp6a4QQgghpBSTM6aYOHEiduzYIVk/+9NPP8W5c+c0BeJ79eoFpVKJPXv2aNq0a9cODg4O2LBhg0H6/sZNWMjKykJWVpbmvlqtxtOnT+Hk5PRaltEhhBBCSjPGGNLS0uDh4VFgkW5DyczMRHZ2tuz2jDHu77uFhYVelpiLjo6WrAIDAG3btsXy5cuRk5MDMzMzREdH4/PPP+faLFiwoNivn583bvA2a9YsTJs2raS7QQghhJRqCQkJqFy58mt9zczMTNjY2ECtVst+Tvny5ZGeni6JhYeHIyIiotj9SUpKgqurqyTm6uqK3NxcPH78GO7u7vm2MeT6wW/c4G3y5MkYO3as5n5qaiqqVKmCu3fvwM7OThN/7733ueempKRI7m/dupVrc6H9B1zscHwKF5u6ZzoXm/73aS726nVzAPjxxx+5Nn6H/uBiC37m1/P8fFQoF7vahO/v6NGjJfd79uzJtZnSrg4Xi+gQzsVaVnXgYgG7pcsXde/enWtToUIFLvb3X9u52LqqwVzsSTZ/UH8W+6fk/uBJ33JtYmNjuZj22qsAkPk1v7TNugPxXGz6sr7SNimOXJt58+ZxMdHSOV3LJfDbHyX93Ad08OXaKCbx37MBAwZwsYYNG3KxX8JHc7EfGkhLg1Sx4n8iut84wcU6d+7MxZRKJRf766+/JPdjmr/HtTlyL42LfR05m4uFbznGxURJwnnr9ubx2ssvC/XTL6e42LjxTbnYhQaduJj2/7gBcCVWJjStxrWZ1mUmF2tTg19q7q0t/BqkH3wgPa5dXFy4Nts2b+Ria3wbc7G0XP54Gn5+OxcbMGaK5P7ly5e5NqL1UtMmjOViG6LucrFvVnzExVY/tJLcX7hwIdfmq6++4mIds65zsenj+N/zwZ2lS2JljfqSbzN4MBcLDeV/a3+e9AkXm/t2GBerZmMmuf/+1X+5Nu+9xx8Xr15RyiP6vp9q0pGLHU+UDmymHPyBazNlXaTm39nZ2Vi5cmWJrCOcnZ0NtVoNNzc3WWf91Go1kpKSkJCQIPkbr4+zbnm0z+rlZZu9Ghe1MeTVvjdu8JbfqVI7OzvJB2tqyu+6iYmJ5L7oi2uj1QYALBT8F8zOxppvJ+iX9pfTxsaGa2NrYcbFzAVzTWwtzLnYq4uC5/eawvdL1H/BforeD+33Tft9BcTv/6ufTx4rBf9cS8HxYGdbXnLfzIx/z0Q/BOXLl+dipmZ830Tvt52VpbSvWVZcG9FrWlnx7ezK8Z+B9muWF/RLIei/6DVF74f2ewYAllqfsXU5/v0XfU5yjieA/26Iti88nsoX7XgC+GPK1tywx5Oob3Y2/Gde1OMJ4N/b4hxPOYI/MHaC19T+Dok+X9HxpBb0TXg8WVtyMUtLaUyfxxPAH1NmMo8nc3P+uyHneAL477zc40m0frPouXKOKdHxJNqnkkw1Mjc3lz14A/i/8fri5ubGnUFLTk6GqampZl3v/Npon43TJ6OfbUoIIYSQsqVcuXKyb4YUEhKCyMhISWz//v0ICgrS/IcmvzaiM7T6YvRn3tLT03Hjxg3N/fj4eMTGxsLR0RFVqlQpwZ4RQgghxBAMNTArbEwxefJk3L9/H2vWrAHwcmbpokWLMHbsWAwZMgTR0dFYvny5ZBbp6NGj0bRpU8yZMwedO3fGX3/9hQMHDuDYMT61Q1+MfvB2+vRptGjRQnM/L59t4MCBWLVqlezt1KhRU/JFmDJlCtemn1Wi5P4kNz7fanAnPnelz0Z+/cdaPXpwsbfffpuL3TmxX3J/7lt8Ll6yJf8xLVHzOVjvvvsuF0uJ5PczJiZGcv9s43Zcm69+4PPsZh35jotN2cbXsRmstZ+//vor1+atfWu42DBT/r2d9AWfd3SxKZ+j5/W2dN/79+/PtVk9nM/L+rIe/363r87nHX2dyOd5NW7bVnJflHd07+4dLraqYgAX+zVbxcUWJUr/J9dnzFSuTYwgX1FUdDJjPJ/fNrIKv+9zf5fmHa14yuc0enl5cbHwcD4fsgfjv6OTnRtI7n/StTrXptdm/niqIdjPd955h4vdOb6Li82q3k1yP9mav2y6KOcGFxMdT+m7I7jY+fPnudh/b7eU3I+Yy+f/zY7m87cmrD/ExcIaNeJiK1askNz3+Zt/z4aZ1+BiX07h9+lMEP898KzL58Z99JH0u7F+hOB4qs3nW3Xwd+ZiU+7xeV6hHTpwMe3LTvdu3+TaLHepy8VWCfL4Ficf5mI9Ph0nuX9R8Luxaxf/nXo2jM9vG+nTjYt9t5Fv92ui9DKvj48P1+abb77hYp1fxHGxyY71uNiwD/jPvesX0t/gt7r14to0b95c829dZnoaikKhkDV407XaWWFjisTERNy9+7+cTB8fH+zevRuff/45fv75Z3h4eODHH3+U5HKHhoZi48aN+OqrrzB16lT4+vpi06ZNCA7mxxD6YvSDt+bNm+v84RBCCCGk9JJ75k3X8UFhYwrRSaFmzZrhzJkzBW73gw8+4CYSGZLRD94IIYQQUrYYavD2pqDBGyGEEEKMCg3eCkaDN0IIIYQYFRq8FazMDN6uXImT1IAJD4/g2gSulibRu77N12jp8tkELtYihy84OUfJ18zZv+E4Fzt0UVpQdcLV7VybNQf4AqIBAXzSu6gOkKiAqLPiheS+pQNfE0l5g0/43fw+nzD/7Z/8hIiOHaVJy19+yRe+TEjgi9L2Ht+bi3l8O4OLZXzC5xV0fyD9Ktst3My1ee7JTwoYdiWSi33xxRdc7L8GDbhYkyZNJPfnzp3LtUmeP46LPcjkP6fUHP79vvmN9L397bcVXBvRe9tBkPgtSoqec5xfc880RZp8b/3ZfK5N5xQ+4T9wwzouVuGvP7lYjQzpMdVTq4AuAFiGhXGxGTP470HPhvwEl8h3+ecmZEjf79oV+HpiuHqUC71a7DuPaPWWoKAgLvbJJ9JE9Zmfj+LanO4qKMS6h0/I/9Ker8nVWHlOcj9uIP+dvRnHF4MOWbmPi3V4yNcFu3DhAhdTzh4juR/Rji/IW96U/2NbozufVC+qEVe3bl0uduDAAcn9jp0FkwKObeFiXmf5795Pvm24mEe6NDH/6+58sn/16m9xsfAmzbjYrlurudip7/nCwNrf5dvbl3Bt/u40mYtNSX7OxXoEVuRigfP5Arzaf0NE9QqPH//f3yeViv+tfN1o8FawMjN4I4QQQkjpYGpqKhzkayura5bT4I0QQgghRoXOvBWMBm+EEEIIMSpy67zpsoD9m6TMDN60i/Q2EhS+PHz4sOS+7Y5FXJtfugznYvsEBVaHDuLzPNot4BdB1y4WHBbCF8z19vbmYkuXLuVib2fxhUb/6MXnYQ0fJl3ku1/jylybJU/4ytA/r9/OxQKG8DlA2usRTpw4kWvTpwmfs3egLb8A9Kjv+XyT9q78+q/fnV4suR+ZyP9vrK2gkOzjuXwx4kGDBnGxrZs3cbGzH0hz+77z5gsKV7fl8wnHrOT382ZNvrjpiPHjJfcvVecL2rZpw+fxiOoRZSzkP4NlzftxMe1ypEO/4HN7Oo+ew8UmTeLzqz6oWZeL1apVS3J/yxY+X8k3ji+KunIo/9mNTuMX6v7ofT8utiROWmD521l8/0d14j9zBwe+QLGoeGrbSvylnd3tpZ/dsMn88dpNUAx60e3tXGz90YtcLDhCug+Zmb9wbUaP5gsz7/+Dz8v69z2+APKXTvX513SQHtfhu/kc2NOW/PvfezKfv3X7N75guahG1vXr0gXmE8bw39lldTpxMUdz/jP5ZBZfVDjt/ZGS++O1jjkAOFHZk4vJ+fsBiP+G/NZ1hOT+viw+B3bIwDpcrN1PfI7h1Kn8ZyDnb0hhfz+UzzPg2mUE1+Z1knvmzdDLYxmrMjN4I4QQQkjpQIO3gtHgjRBCCCFGhQZvBaPBGyGEEEKMCg3eCkaDN0IIIYQYFRq8FazMDN60i/Ru27adazNqlLSQ5vnz57k25pX44rvvvNOciz0awCel+714zL9muUeS+zXT+WT8M8f4graH2nzExaxbeXOxDw/M42Lv2EsT30UL8X7Wki8gmpyczMWqVKnCxXr16iW537p1a65N9oXdXMzKgS+e6mDGH5j7HvLFKjPaSIvhNh/fkmvz37EoLrbnn8NcbPny5VzM25dPxDY1lR4+wb1DuTb+H3/MxayDArmYw/d8Une/GOnnfvwpXxzXdcMRLnbjNp/43WDGZ1zsk0/4ySbr1kmL7XYXTCh4sLIuF/Pw8OBin33Gv2a/ftKEc5fbfOHqmKV7uViSoLBxjqBCwMWj97iY6wxpUvpX42ZxbUJD+c/u119/5WKifRKp31ia8D9kyBCuTevmjbnYw4V80WXT7//hYk0eSwtt2wuOkzrr+CLM2f52XMxP8Bm/tY4vurxss7Tw9VcDv+LauLi4cLEuXbpwsQEDBnCxyk8vc7HYD6S/Q3sO3eHaPFfxX4SaFvyftvT7j7iYl610YsOYMWO4Nr/99hsXe7WgbZ7mzZtzsdq1a3Oxweu+k9yf2Pl9rs3Tn/kJRpFV+UkeGYLfwg9N+d+J9s7SYsRBKee4Ni8a9/nfv5VK7vHXjQZvBSszgzdCCCGElA6mpqbcf5DJ/9A7QwghhBCjIrfOG62wQAghhBBiBOiyacFo8EYIIYQQo0KDt4Ip2Bu+MJhSqYS9vT3c3NwkH7Iowbp79+6S+9rJ1UA+CdZTf+Zie6P5xOmHWfxKDKGOVpL7LUbwScyVx/MJ1gdP8ZMpRIn20dHRXEx7OZGgoCCuzeDB/ISL9i2bc7GHC/jE2sNaCdZHtJKrAXGCdYcG7lzs7W/4hP/UwA5cTDvRfrNWcjUA3L17l4uJEqzfe4+frPHRR/wEkcopVyT3Y6d8z7URJVg/ECTfB1XgJ2u0GtJQcr/KhG+5Nv/G8dtfsWIFFzt2jF8xIzs7m4tpJ1iLvgedZSZYH/uWn5SiPdmkvCn/PWgfUJGLNZren4tlvJJgnWfjxo1cbMMGaeL+rVu3uDaOjo5crGNHftWLsLAwLubH+Ik8F7+eKbm/Z/dNrs3tFzlcLMCOX5GjbT++2r7vV9KVHk4npHJtRN+DQ4cOcbEXL/jjMyCAXwFl4MCBkvvdunXj2mSu41egOPr1X1xs730+Id68HH/5q2016efS+Bt+NQjWnl8J4I8/+JVTfv/9dy527do1yf0KFSpwbUSrmIh+DwLK88dTXEQEF9u3Tfqa19L5571Vnp8Y1/6DGlyseji//fMp/DGl/V2IjIzk2ihfmaSgVquRlJSE1NRUyUS/1yHvb3bHjh1hZsZPvtCWk5ODXbt2lUhfS1LZHLISQgghxGiZmJjIvulq8eLF8PHxgaWlJRo0aICjR4/m2zYsLAwKhYK7vbrU36pVq4RtMjMzi7TvctDgjRBCCCFGJe+yqZybLjZt2oQxY8ZgypQpOHv2LJo0aYL27dsLr8wAwMKFC5GYmKi5JSQkwNHRET169JC0s7Ozk7RLTEzk1vnWJxq8EUIIIcSoGGrwNm/ePAwePBgff/wxatasiQULFsDT0xNLliwRts9Lu8q7nT59Gs+ePeMunSsUCkk7Nze3Iu+7HDR4I4QQQohR0fWyqVKplNyysrK4bWZnZyMmJobLY2zTpo2w8LLI8uXL0apVK3h5eUni6enp8PLyQuXKldGpUyecPXu2iHsuT5mZsJCS8kySzBgTc4Zre/DgQcl9UXLvjRs3uJgo4bd8+fJc7K233uJiLVq0KPA+ANQNrMXFsiJXcrGbG/7mYhf28MnZZ1Kk1+GVuWqujbM5n0cQ4sKv/lC9qz8Xq9JHusICq8dPMDh16hQX++cfvpJ8VBS/KsKdO3ySvnZugSjxuEYNPuG3ZUt+JQZRpfSaXvxkioy90gkiV9fxScCxh/m+nkvlf1Qy1fxh6GGptYJDZT4Zt2avuvzzevXl++rdkIv9999/XEw7kVn0g5aQwK/4IZr84OzszMUCA6WrS7Rq1Ypr07RpUy7mbcdPjH++i5+gE7eez12JOXlfcv+ykn//Ras1eFvzydLBb/ETG2r04Cf8uPaSJvc/q+DLtRFNJtq/fz8XEx0rDx48kNzXnoQEABUr8hM/6tWrx8Xatm3LxRo35idOuUM6ySBlxyquzeV1/3KxmPP8hI6rafz3RaSaVuL+24LJLP59+dUxHLqEcbFERQUu9u+/0v6KEvljY2O5WFJSEhcTqVy5Mhdr0KCB5L5oQkRISAgXc0rjf0uSN6/mYnEb+eP65LWnkvuiyTJmr8wXyWZqLENCiU5Y6N27N8zN+Ykb2rKzs4UTlcLDwxGhNWHkwYMHqFSpEv7991/JqiozZ87E6tWrcfXq1QJfKzExEZ6enli/fj169vzf5JkTJ07gxo0bCAwMhFKpxMKFC7F7926cO3cOfn786jz6QKVCCCGEEGJUdC0VkpCQIBloWljwM7fzaBf2ZYzJKva7atUqVKhQgVvyrVGjRmjUqJHmfuPGjVG/fn389NNP+PHHHwvdblHQ4I0QQgghRkXuTNK8NnZ2doWeJXR2doaJiQl35jQ5ORmurq4FPpcxhhUrVqB///6FnhEsV64c3n77bVy/fr3Q/hcV5bwRQgghxKgYYsKCubk5GjRowF0ej4yMlFxGFYmKisKNGzeEtS+1McYQGxsLd3c+3UZf6MwbIYQQQoyKoVZYGDt2LPr374+goCCEhIRg2bJluHv3Lj799FMAwOTJk3H//n2sWbNG8rzly5cjODhYWMB62rRpaNSoEfz8/KBUKvHjjz8iNjYWP//MF/DXlzIzYUF7hQXRhAJfX2lS8bvvvsu1EU0oqF+Xr4Ce9c8qLha/YQcXu7hLOgHi1DO+qJ/cCQXBTlZcrIZgQoHXhx9I7iuCu3Jt5E4oOHLkCBfTrmAvKlQomlAgmtAhmlAg+gz8faSJwRn7+Ory19fv42LnDoomFPD9fa4qfELB2+78d8pfMKGgUm9+ZYBMX/5/fdoTCg4cOMC1Ea2cUJwJBa8WngTkTyjwdbLmYi/28p/B5d+lk4DORt/n2lwQvP9yJxS87VuBi9XsKZ1Q4NZ7INcmxZH/7p04cYKLiRLaT548ycXu35fuV24uv6qG6DJNnTr8b0nr1q25WJMmTST3K5nyk6ZSBRMK4tbz35dTZx9yMTkTCrQnEwBAg5pOXKyWYEKBYzf+M0g241c70f5+i46BmJgYLiZ3QoH2mRHRajOiCQWiszTOL/hVdR79KZhQsEH6fTl19QnXJv45P6HARJCSVd2Wz+tqUJ8vUVGz7zuS+3bvD+La3Mv83/GUlpaGgICAEp2wMHjwYNkTFpYvX65TXxcvXoy5c+ciMTERAQEBmD9/vuZ3LSwsDLdv38bhw4c17VNTU+Hu7o6FCxdiyJAh3PY+//xzbN26FUlJSbC3t0e9evUQEREhnHiiL3TmjRBCCCFGRdecN10MHz4cw4cPFz62atUqLmZvby+sKpFn/vz5mD9/vs79KA4avBFCCCHEqCgUClmXROXMEn0T0eCNEEIIIUbFUDlvb4oyk/OmXaQ3Pv421/bKlSuS+6IKyRcuXOBioqKxycl8YUpR7pf2F090zd7Dw4OLiQr/1a1bl4tpF0UFgGrVqknuO5vy+RXZ5w5ysSfH+AKc947yRQ3vnJXmm1xL5/NnkjL5HCBRXpNlOf5/VVUEuU7VK0jXkKvciH/PKjfj3wu7xnxeo6l/My52/9FTLqZd0PHcuXNcG1FMOycQEH9f0tPTuZg20fdFVJxV9H0R5VfVrl1bcr969epcGzc7fr2+nAuHuVjqcT4fMuHIRcn9+/8lcm2uCIro3s/gvy+iwsZmgv+EV7KSfl9q2vJ5NB6CPCHPZny+qGPjJlzMrDafg5mcIc1TvXbtGtdG9Fsi+s0RFQbXzukSfVdEhXutrfncRNH3pWrVqlxM+7sh+r0RFcL29ODfW1UcX0w57Tifz5ZwSHr83P/vAdfm2mP+ctZdQRFaUd6q9p99N0v+fMZbgtw+L0Gx4MpN+GOsYlO+2LF5XWkeb6qJLddGVGLi4sWLXExUQFj77xjwsrjsq1JSUrg2r35f1Go1kpKSSjTnbeTIkQXWasuTlZWFRYsWlUhfS1KJDlmPHDmC9957Dx4eHlAoFNi+fbvkccYYIiIi4OHhASsrKzRv3hyXLl0qmc4SQggh5LUwNTWVfSuLSnTw9vz5c9SpUweLFi0SPj537lzMmzcPixYtwqlTp+Dm5obWrVsjLS3tNfeUEEIIIa+LoRamf1OU6JC1ffv2aN++vfAxxhgWLFiAKVOmoFu3bgCA1atXw9XVFevXr8cnn3zyOrtKCCGEkNfEkLNN3wRGO2SNj49HUlKSpMaOhYUFmjVrJlwsO09WVhaUSqXkRgghhJDSg868FcxoLxbnJeRqF7J0dXUVThDIM2vWLEybNo2L16hRU/Ihi66TOzo6Su5XrlyZa6NdxBQAt0gtAPj788nO2kWAAcDq2W3J/cxT+7k2SUf4grkJ//KJzbc28pMMdgmKPT7NVknu82nNgJ0pf0D42PATBd6qwieI1u4iTVru0KI+18YquB0XU3nyCfSi5P64uDgudlprYsCK8+e5NndXH+ZiT+dv5WKiiSWi74t2cqzo+yJK4BYVGRZV7daeWGKXk8K1yT7LJ3knH+H/c3PvKJ8wH/8X3+6Q1uSSP7JUXBvR98VGUEFUNLGkhqu0kHHVVt5cmyZN+e9B+RC+WLBJDX7ywO27fIFi7YkloiTvTYLvS/z201zs8W97uZio/pOciUiiIr2i70uvXr24mPbkAe3vCgBUtOA/qexzh7jYU1Gh5yj+GLv3g/Q351TaMq7N38WYiFTJij/G/B2khcc93uaXG3q7GX/s2L8jmIhUiy8u/eCJ9D/32t8VADgv+G7sE0xEunmYP8YebuInZqSnf8XFtImKyIu+L6K/KR06dOBi2pNLRBOYPJz+9x1VKpWo4Mr/nr1OVCqkYEY/ZNX+YBhjBX5YkydPRmpqquYmqjZPCCGEEONFZ94KZrRn3tzcXk4vT0pKkixhkpycLPwfSB4LCwtZ04sJIYQQYpwo561gRjtk9fHxgZubm2QtwezsbERFRQnXlSOEEELIm4HOvBWsRM+8paenSwpQxsfHIzY2Fo6OjqhSpQrGjBmDmTNnws/PD35+fpg5cyasra3Rpw+/sDchhBBC3gy0wkLBSnSFhcOHDwuTtwcOHIhVq1aBMYZp06bhl19+wbNnzxAcHIyff/5ZmNydn/xWWEhOfsS1ffBAWr37/v37XJvbt29zsfj4eC529+5dLvbw4UMu9uzZM8l9UaX07Gx+hQIRUVK9qKK6ra20orezszPXplKlSlzM29ubi/n4+HCxKlWqFLot0aVvq6wULqa6xydOZ1/jJ2s8uyhNFn4Sd49r8+TqEy728B5fM/CBIOn6URYfS82RJoSLEvlFPytWguT+ihb8Z+ehVe3drSL/WTr5OXAx5wA+0dipFv85WVSvx8VMvKQTclQOVbg2ou+x9rEDAPfu8Z+B9vEjmpAiOu5Er5mamsrFRJMH5Bw/5uZ8FX3RsWNvb8/FRCsUeHp6Su6Ljh1RTPvYAcQrrGgfPyYp/Humus0n2mff4GOPz/MrODyJ47f3+Ip0lZGkR/x7LTp2tCdIAUB6Ln+0yDl+ygsmUlW04C+beVrxk2VcK/MrGThWkx4/zgH8++8Q8BYXM69Wm4uZVOH/LmVYVOBijx5J//bI/Tsj+psit93jx48l90W1Ul89dtRqNe7du1eiKyzMmDEDlpb8ai7aMjMzMWXKlDK3wkKJnnlr3rw5Cho7KhQKREREICIi4vV1ihBCCCElis68FcxoJywQQgghpGyiwVvBaPBGCCGEEKNCdd4KRoM3QgghhBgVOvNWsBKdsPA65CU/urm5ST5k0QeunRypndgPAE5OTlxMlHwvN0FZO+FfbnJ/BUE1cnZfXnJ/WtwVyf0nl/gJF4+vPuZiT24842IJGXyCcpJW0nJqDp+wLKq6LiKqxO5ozicou2kl91e252v9OfhU4GLO/vx76xTAJ/fb1AzkYma+0qRlhQe/qsbjp/x7lpiYyMVECf/ayfyiiTGiItSi5P6nT59yMdHkGNHqEtpEx44ouV+UPKw9OUaUjC86Try8vGTFRMdPXs3IPDbqDK6NWnDsZAmOndRLfBX9J5duc7HHV6STYx4l8Mv0iY4d0cQYUXK/nONHtOqF6NjRnhgDAB5O/OfpqDU5xrkmv9qB6NixrM6vmGHqwx9PcOWr/mt/l0XHiegYECXyi1bm0U7uT05O5tqkpKRwMX1OLBMdOxUqVOBicieWVa1alYtpH1PaE2q0t5WWloaAgIASnbAwb948WFlZFdo+IyMDY8eOLXMTFsrmkJUQQgghRsuQdd4WL14MHx8fWFpaokGDBjh6lF/GLM/hw4ehUCi425Ur0pMgW7Zsgb+/PywsLODv749t27bp3C9dFOmyqVqtxo0bN5CcnAy1Wvq/wqZN+fXjCCGEEELkMtRl002bNmHMmDFYvHgxGjdujF9++QXt27fH5cuXhaV68ly9elVyZu/VEkHR0dHo1asXvvnmG3Tt2hXbtm1Dz549cezYMQQHB+vUP7l0HrydOHECffr0wZ07d7gyHwqFAioVf4mMEEIIIUQuXQdvSqU0NSG/pTLnzZuHwYMH4+OPPwYALFiwAPv27cOSJUswa9asfF/HxcVFeDk7bxutW7fG5MmTAbxcYz0qKgoLFizAhg0bCt2HotD5fOOnn36KoKAgXLx4EU+fPsWzZ880N1FeDSGEEEKILkxNTWXfgJd5fPb29pqbaCCWnZ2NmJgYtGnTRhJv06YNjh8/XmB/6tWrB3d3d7Rs2RKHDh2SPBYdHc1ts23btoVuszh0PvN2/fp1/Pnnn6hWrZoh+mMwV67ESU55pqbyCcTaFdtFg1FRTJQgLkp8FVWTP3nypOS+diVsUb+Awitk5xEloGsn1mpf+s5POTN+rG8qmDihXa3e2ppPIi1fvjwXE00QcXR05GIKQUV7c61JHaYuLlwbtSAGwQQUteA1cwT/49L+X5h5Jv+ZOGXykxMq5PAJ1n45/GSEhgppuzRTflWQNBP+NZVqQdJ7Fv85pabx/yt9nC1tJ7c6fnpuFhfLYXx/Ae0YP1EgWTDzP82E/+7dFlTbtxd8R521kvQd7fj9Lu9qw8XsBBX5bSvz3w3XBnwF/qrdpInkpu7eXBtTNz6mcOJXx1DZ8Inq2mcZREn1cn+/kpKSuNgFwe+X9u+c6Lfq6XZ+BQel8hgXE/2mZWTwE0m0f6tEkwJk/34JzuJoTx4QVfQXJc2LEv5Fq2+Ifr9ctH6HRBPSRJMTtCfe5Ld9UczBQTrZRJTYb5qRovm3shz/G/K66XrmLSEhQbJforNujx8/hkql4t5zV1dX4XEAAO7u7li2bBkaNGiArKws/P7772jZsiUOHz6sSRNLSkrSaZv6oPPgLTg4GDdu3Ch1gzdCCCGElA661nmzs7OTPdtUuzYcYyzfenHVq1dH9erVNfdDQkKQkJCA77//XpLjr8s29UHnwdtnn32GcePGISkpCYGBgTAzk64hV7s2v+YbIYQQQohchpiw4OzsDBMTE+6MWHJysvAMaH4aNWqEtWvXau67ubkVe5u60nnw1r17dwDAoEGDNDGFQqEZZdKEBUIIIYQUhyEGb+bm5mjQoAEiIyPRtWtXTTwyMhKdO3eWvZ2zZ8/C3f1/dQ5DQkIQGRmJzz//XBPbv38/QkNDZW9TVzoP3kSFQgkhhBBC9MVQpULGjh2L/v37IygoCCEhIVi2bBnu3r2LTz/9FMDLmaL379/HmjVrALycSert7Y1atWohOzsba9euxZYtW7BlyxbNNkePHo2mTZtizpw56Ny5M/766y8cOHAAx47x+Z76ovPgTVTVvDSoUaNmoR+ydvKqduI9IK6GLYrJTb7XjtWtW5drU1GQoK+d9JpfOzmvKUq0tbEQrODwhE+0Vz+6y8VyE6UD/Oz7fGXztLv8JI+0u3yStPIeX1E97fxVLpaeLJ2s8SiLPwMcJ1jpITVHlHzPx7LVfEl7OWnSom+claDyfXlh8r000d5BlIwvqI5v685PBnHSqo4PAN6V+Wr4tp7S0/y2VfjvmZkH/zzTSr5cTOHIV39XOEoT8tOe85NsRMnsT5484WKi5HtREr325KE4QRKx6HmPHvETLpQX+Vjqvze4mHbyvZyJQwCQm1u0RHHt3y5AnHwvitnY8JM1RL8J2knvoqR6US603N8v0fa0f6tEv2ei8g0WyOFi7Mk9LqZKvi25n/vgNtcm8x7/G5eWIO/3K03w25d++pLkvvIxP1HjsWCiUKwgJlq9JkPF/1ZlCn6/CpIt69fNsAw1eOvVqxeePHmC6dOnIzExEQEBAdi9e7dmbJOYmChZeSM7Oxvjx4/H/fv3YWVlhVq1amHXrl3o0KGDpk1oaCg2btyIr776ClOnToWvry82bdpksBpvQBGL9N68eRMLFixAXFwcFAoFatasidGjR8PXl/8BJ4QQQgjRhSHXNh0+fDiGDx8ufGzVqlWS+xMmTMCECRMK3eYHH3yADz74QOe+FJXOe71v3z74+/vjv//+Q+3atREQEICTJ0+iVq1aiIyMNEQfCSGEEFKGGHJ5rDeBzmfeJk2ahM8//xyzZ8/m4hMnTkTr1q311jlCCCGElD2vFuAtrF1ZpPNex8XF4Y8//uDigwYNwoIFC/TRJ4PQLtL74gWfZ/D8+XPJfVHRSO3imACQnp4uKybK5dEurinavqjQn/aiuIC4cK9oe9rFfLX3GxDn6GRl8YVYRXk72jFRHo+osKbcYpsi2gWEy1kUXpATEOc1imKigo/a+UOi3CFRPqSoQHGOIMdIpVWzSC3I7ckR1DXKELTLFsSyBP3I1IplCnI3Rfsk2ndRDC9SJHfLv+Bzzcpn8fltbjl8PpEqVxATFAbOhDQ37kW5Z3wbE/44yTDjj/8XjM+lylTx37WM5yZa9/nvVGqOCRdTCvItRTlM2nmZopzMDBV/vKoYH1OD/10C+FxTbfyRDyQJylo9LccH4wUxK0EhZu1cUFFuqJ0gZivIBbV04I9hKwdLrTZ8TqC1E/89tnTijzvn2nzaUJWWFbiYqaM0B9DEgc//M3HiC/IqbPmC4qIYyvO5g1k50t9g0W/+q7G0tDSsDAjgt/0a6VrnrazR+XxjxYoVERsby8VjY2OFSaiEEEIIIbqgy6YF0/nM25AhQzB06FDcunULoaGhUCgUOHbsGObMmYNx48YZoo+EEEIIKUMMOWHhTaDz4G3q1KmwtbXFDz/8gMmTJwMAPDw8EBERgVGjRum9g4QQQggpWxQKhaxLomX1sqnOgzeFQoHPP/8cn3/+uSbHSlTTjBBCCCGkKOjMW8GKNU2jNA3a5BTp1X5c1N5Ykt5FC/CKPg9PT89CnysqcinavqidKPleux+iNnL3U5j0/pwvzsrStWLpfNK76pkgwf1xIh9L4ZPeM5L5JPeMJ9JE74xHKXybx/zElYz7fN8yzvPJ8Zmp0uTy9HQ+RVxUUDhRELsuMxE+Q1V4IrzgaUUu6Sk6IgU1jGEuM8FdVABZu50owV2UCF/UpHcAsPeSTkCxdpaX9G7pxE9csa5YgYtxSe+CBHdRIrzCji+YqyjPF3AWJb1nZEm/f6IJXaJEeNFEqqJO8tKe4AUASYJJWVdlTA4D+AldoglezxP5fcq4xRcsz8y8LogVXpxZbrFmuRO/iurVv2363G5R0Zm3gskavNWvXx///PMPHBwcUK9evQLfrDNnzuitc4QQQggpe+jMW8FkDd46d+6sOWvUpUsXQ/aHEEIIIWUcnXkrmKzBW3h4uPDfhBBCCCH6RmfeCqZzzltCQgIUCgUqV365wPR///2H9evXw9/fH0OHDtV7BwkhhBBStpiYmMDEhC9kLWpXFuk8eOvTpw+GDh2K/v37IykpCa1atUJAQADWrl2LpKQkfP3114boZ7Fpr7CQnc1XStdeQUDuKgOidqJkXjntRNsXJfJqr5KgS0w7qfjWrVuF9kuXmPZrFud9zMnhPydR4q6cVR2KkwQsJ2boJF/RqhHlrAQxmRNtTE3NCm0nfh4fMzPjtyVngo5oYo+VlZVeY9qTXnIFE2heWPKTDtJlro4hmmij3Q9Rv7Tfi/zaiWLa761oW6L3X/R+mwkmfiCTn2RgmSGNWQraOGTzMVU2P9mH5fDt1IJ2KlWK9L6C/y3MVvC/cVngt59djm+XbSKN5Zjzv0tZmYJVZARTdLKy+Zk82Rn8e5ubKT1+MvmfUGSo+GPsuYwJRkB+q22wQtu8GstiCvzId+u1osumBdP5fOPFixfRsGFDAMAff/yBwMBAHD9+HOvXr8eqVav03T9CCCGElDF5y2MVdiurgzedz7zl5ORo/kd34MABvP/++wCAGjVqIDGRL7tACCGEEKILOvNWMJ3PvNWqVQtLly7F0aNHERkZiXbt2gEAHjx4ACcnwSK5hBBCCCE6oLVNC6bzXs+ZMwe//PILmjdvjg8//BB16tQBAOzYsUNzOZUQQgghpKjyzrzJuZVFOl82bd68OR4/fgylUgkHh/9V5h46dKgwcddYyFlhQZuofXFicpK/RQnFxUkQF7XTToAWvaacJGwAcHbmK7Frt5Ob5K3PWFETv/NrJychXG7SuJmZ4LDL4BOstZPGmYw2AKBOT+FjaYLYC76avHY7VTq//awUQdJ4miAZXMlXptdul63ks7Wzn/MrUGQ9FCSNP+djOc/5CS7ZWrHMDH6SiijxW7QCxWNhMnjhSeOFJYgXFBOtaKHdriTq4ctdHcNE8MdVtGKGnJh4pQ15q29YyniutYXgt9aG/w01s+LbWdjxv6M2rvzKGhZ20t8EcxvBCj12/G+cmS0fM7cTbL8C/3trZiuNlStfgWtTzuZ/K+Mon2fgx04lWz3CkKVCFi9ejO+++w6JiYmoVasWFixYgCZNmgjbbt26FUuWLEFsbCyysrJQq1YtREREoG3btpo2q1atwkcffcQ9NyMjQ/h3QR+KdL7RxMREMnADAG9vb7i48MuxEEIIIYTowlBn3jZt2oQxY8ZgypQpOHv2LJo0aYL27dvj7t27wvZHjhxB69atsXv3bsTExKBFixZ47733cPbsWUk7Ozs7JCYmSm6GGrgBRTjz9vDhQ4wfPx7//PMPkpOTwZj0f4AqlUpvnSOEEEJI2WOoM2/z5s3D4MGD8fHHHwMAFixYgH379mHJkiWYNWsW137BggWS+zNnzsRff/2Fv//+G/Xq1dPEFQoF3Nz4NYYNRefBW1hYGO7evYupU6fC3d29zF5vJoQQQohh6FqkV6mUpoFYWFhwaSvZ2dmIiYnBpEmTJPE2bdrg+PHjsvqlVquRlpYGR0dHSTw9PR1eXl5QqVSoW7cuvvnmG8ngTt90HrwdO3YMR48eRd26dQ3QHUIIIYSUdbqWCvH09JTEw8PDERERIYk9fvwYKpUKrq6ukrirqyuSkpJk9euHH37A8+fP0bNnT02sRo0aWLVqFQIDA6FUKrFw4UI0btwY586dg5+fn6zt6krnwZunpyd3qbSoZs2aha1bt+LKlSuwsrJCaGgo5syZg+rVq2vaMMYwbdo0LFu2DM+ePUNwcDB+/vln1KpVS6fX0l5hISeHT1rWruYvqu4vt+K/qJ32KgCimJzVA/QdE61sIIrJ3b72c0VtUlNTuZjo4BGtxCCKab9vcp8nZ1uAeB+02xVnVQc5MdEKDnJXiCjqShJyV42QuyqFMShnLu8yiz4nLJUrx59BEE1EkjvRSfuPmtyVMOTGRBNt5EyukjuRqqjPFW0rSzDBKEPma2rHRDlKoufps53c90JO/+XGCpsEl5YmmBz1mul62TQhIUHyN170XcmjffwwxmQNFDds2ICIiAj89ddfkhz/Ro0aoVGjRpr7jRs3Rv369fHTTz/hxx8Ns1aFzhMWFixYgEmTJuH27dvFfvGoqCiMGDECJ06cQGRkJHJzc9GmTRvJ8k1z587FvHnzsGjRIpw6dQpubm5o3bq1UXy5CCGEEKJ/uk5YsLOzk9xEgzdnZ2eYmJhwJwqSk5O5s3HaNm3ahMGDB+OPP/5Aq1atCmxbrlw5vP3227h+/bqOey2fzmfeevXqhRcvXsDX1xfW1tbc/xCePn0qe1t79+6V3F+5ciVcXFwQExODpk2bgjGGBQsWYMqUKejWrRsAYPXq1XB1dcX69evxySef6Np9QgghhBi5vOWx5LSTy9zcHA0aNEBkZCS6du2qiUdGRqJz5875Pm/Dhg0YNGgQNmzYgI4dOxb6OowxxMbGIjAwUHbfdKXz4E175oU+5V1Ky0sEjI+PR1JSEtq0aaNpY2FhgWbNmuH48ePCwVtWVpbkUph2EiMhhBBCjJuhlscaO3Ys+vfvj6CgIISEhGDZsmW4e/cuPv30UwDA5MmTcf/+faxZswbAy4HbgAEDsHDhQjRq1Ehz1s7Kygr29vYAgGnTpqFRo0bw8/ODUqnEjz/+iNjYWPz888869U0XOg/eBg4caIh+gDGGsWPH4p133kFAQACA/+VAiZIL79y5I9zOrFmzMG3aNC5elCK9InK3UfTcGP0WARa1057Bo+98Ge1YUXNq8msnpxixra0t10ZUUFhukV45eTuGzpeR+57pMzdGbj5OUWOiNqJ9EhV/RQ5f4BfZfK4my9ZqJ3yeIJbDb0udwRceZpl8TLudqA3LEmxftK0XfAHknBfS56oyBAWLX/Dbz80U5Hg+F7Xjt6fSiuVk8O9ZznM+lzXnsSCfM5OPqbL5MlPa7XIz+DaqHD6WI9i+qACydiFmUZvnghzvZ3ossKwSbF/utkTPldNO1Eby+iVS9lnKUKVCevXqhSdPnmD69OlITExEQEAAdu/eDS8vLwBAYmKipObbL7/8gtzcXIwYMQIjRozQxAcOHIhVq1YBAFJSUjB06FAkJSXB3t4e9erVw5EjRwy66pTOgzcAuHnzJlauXImbN29i4cKFcHFxwd69e+Hp6anzRII8I0eOxPnz53Hs2DHuMV2SCydPnoyxY8dq7iuVSm4WCiGEEEKMlyEXph8+fDiGDx8ufCxvQJbn8OHDhW5v/vz5mD9/vs79KA6dT0VFRUUhMDAQJ0+exNatW5Ge/nKpnPPnzyM8PLxInfjss8+wY8cOHDp0CJUrV9bE8wre6ZJcaGFhwSUuEkIIIaT0oLVNC6bz4G3SpEn49ttvERkZKbmk0qJFC0RHR+u0LcYYRo4cia1bt+LgwYPw8fGRPO7j4wM3NzdERkZqYtnZ2YiKikJoaKiuXSeEEEJIKZBXpFfOrSzS+bLphQsXsH79ei5esWJFPHnCLyhdkBEjRmD9+vX466+/YGtrqznDZm9vDysrKygUCowZMwYzZ86En58f/Pz8MHPmTFhbW6NPnz66dp0QQgghpYAhL5u+CXQevFWoUAGJiYncWbKzZ8+iUqVKOm1ryZIlAIDmzZtL4itXrkRYWBgAYMKECcjIyMDw4cM1RXr3798vTEgviHaRXpWKT8jUXpdVVNhUtHarKFbU58rdlqgIcFGfW5z+y+mH3MLGcgrhym0nd1tyY6Jivq/WIwSKV8BZznOLs325n6ecwsNyvy+imHaBb7nbFxUGL+pz5RYU1mfM0MWOi1MQ2ViLKRdHObOiTU6TOzmseM9VaLXhzxzps0C0KCYa8Lw6UUilUgFnErg2rxMN3gqm8+CtT58+mDhxIjZv3gyFQgG1Wo1///0X48ePx4ABA3TalpyVGhQKBSIiIrhlLgghhBDyZqLBW8F0/u/JjBkzUKVKFVSqVAnp6enw9/dH06ZNERoaiq+++soQfSSEEEJIGZJXKkTOrSzS+cybmZkZ1q1bh+nTp+Ps2bNQq9WoV6+ewRZfJYQQQkjZQmfeClakOm8A4OvrC19fX332hRBCCCGEBm+F0HnwxhjDn3/+iUOHDiE5OZlLdt26daveOqdP+lphQa6ivpb+k2MLf66hk2NFbQpLmM0jmgYu57mi54m2X5x+yFlJQrSCgOj9kLPSgKiNaDUIuasdyOmv3FUd5Ma0V2wo6vOK0070Xog+X7nt5LyPRX2e3OfK3b7clVnkPFfucWJiIvhdyuUnFEHFT7Th2uXwE4dYUbcFALla21MJJtkIV+0QxHL512RZ/CoU2s8VP0/ua/L7JFy5I0drIpJgBQ119v/apGVmoeqZM1yb14kGbwXTefA2evRoLFu2DC1atICrq2uZfeMIIYQQYjg0vsifzoO3tWvXYuvWrejQoYMh+kMIIYSQMk5uAV4q0iuTvb09qlataoi+EEIIIYTQZdNC6JyYFRERgWnTpiEjg7+WTwghhBBSXLS2acF0PvPWo0cPbNiwAS4uLvD29uYSbM+UcJJjfrRXWBDVB9auzi6qPC6q4C63Krqc6u/63r6cdqJiyXJXkijqqhFy+y83Jud9lBsT9U3OCgJFfZ7c5xb1ecXpm+h5otUmRP+ZE630IGflATnP06WddqyozyvOc4vzPdPnMazPdq/jNYv6PH2uVFGc1THktNP39ov63FfbGMOqG3TmrWA6D97CwsIQExODfv360YQFQgghhOid3AK8VKRXpl27dmHfvn145513DNEfQgghhJRxdOatYDoP3jw9PSWXHwkhhBBC9IkGbwXT+XzjDz/8gAkTJuD27dsG6A4hhBBCyjqasFAwnc+89evXDy9evICvry+sra25CQtPnz7VW+f06XWvsFBU+u6joXMGirr94qwkIaedvleqEP1AyKkvJPd5ctrpc1v6fk25lfu128ndltxVKeQ8t6jP0+W52u1KYvtFXTlBbjt9b9/Qryle/cGk0DZyjwG5MTnHgL5j2vtQWL/S0tLg7+/PtXmdDJnztnjxYnz33XdITExErVq1sGDBAjRp0iTf9lFRURg7diwuXboEDw8PTJgwAZ9++qmkzZYtWzB16lTcvHkTvr6+mDFjBrp27apz3+TSefC2YMECA3SDEEIIIeQlQ1023bRpE8aMGYPFixejcePG+OWXX9C+fXtcvnwZVapU4drHx8ejQ4cOGDJkCNauXYt///0Xw4cPR8WKFdG9e3cAQHR0NHr16oVvvvkGXbt2xbZt29CzZ08cO3YMwcHBOvVPLgWTMz+7FFMqlbC3t4ebmxudeTPAa9KZt4LRmbeC29GZN8Ntn868SdGZN/n9yjvzlpqa+tpz3PP+Zj979kzWayuVSjg4OMjua3BwMOrXr48lS5ZoYjVr1kSXLl0wa9Ysrv3EiROxY8cOxMXFaWKffvopzp07h+joaABAr169oFQqsWfPHk2bdu3awcHBARs2bCi0T0Uh6y+cUqmU/LugGyGEEEJIceia86Y9FhHVo8zOzkZMTAzatGkjibdp0wbHjx8X9iM6Oppr37ZtW5w+fVpT9zG/NvltUx9kXTZ1cHBAYmIiXFxcUKFCBeH/RBhjUCgUwiKKxkBOkV7twoRyiz/qs11xCk4WtR/Gsk9yi3kWdZ+KU+BTTt/0/T6+7u2Lniv3u1fUmNyitPqMFWdbRe2vofdJVDxYVDhZn4XHX0cR86K+jyL63H5x9lOf2ypqgd/CfgtF3/PXTfH/NzntgJeVMF4VHh6OiIgISezx48dQqVRwdXWVxF1dXZGUlCTcflJSkrB9bm4uHj9+DHd393zb5LdNfZA1eDt48CAcHR0BAIcOHTJYZwghhBBCwNQvb3LaAUhISJCcoLGwsMj3KdonoPJOPunSXjuu6zaLS9bgrVmzZsJ/E0IIIYTonVr98ianHQA7O7tCc96cnZ1hYmLCnRFLTk7mzpzlcXNzE7Y3NTWFk5NTgW3y26Y+yBq8nT9/XvYGa9euXeTOEEIIIYSAqV7e5LSTydzcHA0aNEBkZKSkjEdkZCQ6d+4sfE5ISAj+/vtvSWz//v0ICgrSTBwKCQlBZGQkPv/8c0mb0NBQ2X3TlazBW926daFQKGSdBjTWnDdCCCGElBI6XjaVa+zYsejfvz+CgoIQEhKCZcuW4e7du5q6bZMnT8b9+/exZs0aAC9nli5atAhjx47FkCFDEB0djeXLl0tmkY4ePRpNmzbFnDlz0LlzZ/z11184cOAAjh07plPfdCFr8BYfH6/599mzZzF+/Hh88cUXCAkJAfBypsUPP/yAuXPnGqaXelBaivTqm6H3WZ/bN5Zt6buMiSG3b+iSKyJy8zj0WUpFn/0oTv/1+ZrF2U/t5+q7NI7cUhv63Jac5xa1X4C8Miz67Kvc5xr6PRM9t7DnZWZm4sCBA1yb10rHy6Zy9erVC0+ePMH06dORmJiIgIAA7N69G15eXgCAxMRE3L17V9Pex8cHu3fvxueff46ff/4ZHh4e+PHHHzU13gAgNDQUGzduxFdffYWpU6fC19cXmzZtMliNN6AIdd4aNmyIiIgIdOjQQRLfvXs3pk6dipiYGL12sLhKW503faPBm+GeS4O3gtHgreDn0eBN9+cWZ/Cmz+2XhcHb1KlTS7TOW0pSguw6bxXcPEukryVJ5xUWLly4AB8fHy7u4+ODy5cv66VThBBCCCnDVLmAii+BI2xXBul86qBmzZr49ttvkZmZqYllZWXh22+/Rc2aNfXaOUIIIYSUQXk5b3JuZZDOZ96WLl2K9957D56enqhTpw4A4Ny5c1AoFNi5c6feO0gIIYSQMsZAOW9vCp0Hbw0bNkR8fDzWrl2LK1eugDGGXr16oU+fPrCxsTFEH/VCe4UFUe3moq48UJyYttfxmnJWQBDR5/aL+v68jteU24+SWJHD0NuXU7Hd0KtviBh6RRERY3kfi1pZ39BV+g29fbmvWdRtyd1eSbyPhu6HqM2rq3SIlpZ67Qw02/RNofPgDQCsra0xdOhQffeFEEIIIYTOvBWiSIM3QgghhBDDkVmkF2WztiwN3gghhBBiXOiyaYFo8EYIIYQQ40KXTQtUZgZvZXWFhZJgDO9zSfShNBVEfh2vYegixsbcj9f9mvosnFyc1zCW7Rv6/TB0oeqibkvu9grb1quTF0oMnXkrUJEHb9nZ2UhOTuZmrVSpUqXYnSKEEEJIGUZFeguk8+Dt+vXrGDRoEI4fPy6J5y1aTwvTE0IIIaQ4mFoNJqfkDl02lScsLAympqbYuXMn3N3dZZ/yJYQQQgiRhS6bFkjnwVtsbCxiYmJQo0YNQ/SHEEIIIWUdDd4KpPPgzd/fH48fP9bLiy9ZsgRLlizB7du3AQC1atXC119/jfbt2wN4eSl22rRpWLZsGZ49e4bg4GD8/PPPqFWrls6vVZQVFkSKU51dzvb1ucpAcV5D3ysgaCvqigJy2+m7cr8++1Gcvulz+8bwPup7tQO5z33d/dDnZy7anj7fC7nbex3H2Ot+bw39PStOP+Ruq6grLLzqxYsX2Ldvn6w+GAzNNi2QzlOb5syZgwkTJuDw4cN48uQJlEql5KaLypUrY/bs2Th9+jROnz6Nd999F507d8alS5cAAHPnzsW8efOwaNEinDp1Cm5ubmjdujXS0tJ07TYhhBBCSgumkn8rg3Q+89aqVSsAQMuWLSXxokxYeO+99yT3Z8yYgSVLluDEiRPw9/fHggULMGXKFHTr1g0AsHr1ari6umL9+vX45JNPdO06IYQQQkoDOvNWIJ0Hb4cOHTJEP6BSqbB582Y8f/4cISEhiI+PR1JSEtq0aaNpY2FhgWbNmuH48eP5Dt6ysrIki+rqejaQEEIIISWMMZk5b/IuMb9pdB68NWvWTK8duHDhAkJCQpCZmYny5ctj27Zt8Pf315QicXV1lbR3dXXFnTt38t3erFmzMG3aNL32kRBCCCGvEVMBahlX8uiyqXxHjx7FL7/8glu3bmHz5s2oVKkSfv/9d/j4+OCdd97RaVvVq1dHbGwsUlJSsGXLFgwcOBBRUVGax7VLkeRdns3P5MmTMXbsWM19pVIJT09PWmGBlJnP35j301j79iauyFESr0nbN9z2DL3vr/5dNYZ6rSw3Byy38CK9ctoU1bNnzzBq1Cjs2LEDAPD+++/jp59+QoUKFYTtc3Jy8NVXX2H37t24desW7O3t0apVK8yePRseHh6ads2bN5eMcwCgV69e2Lhxo+y+6fxt2LJlC9q2bQsrKyucOXNGc4kyLS0NM2fO1HVzMDc3R7Vq1RAUFIRZs2ahTp06WLhwIdzc3AAASUlJkvbJycnc2bhXWVhYwM7OTnIjhBBCSCmiVsm/GUifPn0QGxuLvXv3Yu/evYiNjUX//v3zbf/ixQucOXMGU6dOxZkzZ7B161Zcu3YN77//Ptd2yJAhSExM1Nx++eUXnfqm85m3b7/9FkuXLsWAAQMko8TQ0FBMnz5d181xGGPIysqCj48P3NzcEBkZiXr16gF4uSRXVFQU5syZU+zXIYQQQohxYioVmIwzgHLaFEVcXBz27t2LEydOIDg4GADw66+/IiQkBFevXkX16tW559jb2yMyMlIS++mnn9CwYUPcvXtXsnyotbW15iRVUeh85u3q1ato2rQpF7ezs0NKSopO2/ryyy9x9OhR3L59GxcuXMCUKVNw+PBh9O3bFwqFAmPGjMHMmTOxbds2XLx4EWFhYbC2tkafPn107TYhhBBCSou82aZybgBXtuzViYtFER0dDXt7e83ADQAaNWoEe3t7bnnQgqSmpkKhUHCXWtetWwdnZ2fUqlUL48eP17kEms5n3tzd3XHjxg14e3tL4seOHUPVqlV12tbDhw/Rv39/JCYmwt7eHrVr18bevXvRunVrAMCECROQkZGB4cOHa4r07t+/H7a2trp2W1aRXm36LqxZ1NeQ24+S2L4+C04W57lFfQ1Df8bU/5LdPmAc31FDb9/Qv0HFeY2ysv2ivqaht1+U10hPTxeepHmt1Gp5l0T//z3w9PSUhMPDwxEREVHkl09KSoKLiwsXd3Fx4dK58pOZmYlJkyahT58+kvFH3759NVcXL168iMmTJ+PcuXPcWbuC6Dx4++STTzB69GisWLECCoUCDx48QHR0NMaPH4+vv/5ap20tX768wMcVCgUiIiKK9QEQQgghpHRhahWYjMFbXpuEhATJAMnCwkLYPiIiotCKFKdOnQLAT5gECp80mScnJwe9e/eGWq3G4sWLJY8NGTJE8++AgAD4+fkhKCgIZ86cQf369QvdNlCEwduECROQmpqKFi1aIDMzE02bNoWFhQXGjx+PkSNH6ro5QgghhBApJrNI7//XgpM7QXHkyJHo3bt3gW28vb1x/vx5PHz4kHvs0aNHBU6aBF4O3Hr27In4+HgcPHiw0H7Vr18fZmZmuH79uuEGb8DLlRCmTJmCy5cvQ61Ww9/fH+XLly/KpgghhBBCJHQ98yaXs7MznJ2dC20XEhKC1NRU/Pfff2jYsCEA4OTJk0hNTUVoaGi+z8sbuF2/fh2HDh2Ck5NToa916dIl5OTkwN3dXfZ+FGnwBrycKREUFFTUpxNCCCGEiMktA2KgUiE1a9ZEu3btMGTIEE0Zj6FDh6JTp06SmaY1atTArFmz0LVrV+Tm5uKDDz7AmTNnsHPnTqhUKk1+nKOjI8zNzXHz5k2sW7cOHTp0gLOzMy5fvoxx48ahXr16aNy4sez+yRq85a0tKsfWrVtlt32dqEgvKcvou6+7svyevYn7Xpr2qaT7WpwJMHrsRImvbbpu3TqMGjVKs0zn+++/j0WLFknaXL16FampqQCAe/fuaQr61q1bV9Lu0KFDaN68OczNzfHPP/9g4cKFSE9Ph6enJzp27Ijw8HCYmJjI7puswZu9vb3m34wxbNu2Dfb29pozbzExMUhJSdFpkEcIIYQQIsJycsBysmW1MxRHR0esXbu24Nd/ZSavt7d3oTN7PT09udUVikLW4G3lypWaf0+cOBE9e/bE0qVLNaNElUqF4cOH02oGhBBCCCm+Er5saux0Pje7YsUKjB8/XnJ6z8TEBGPHjsWKFSv02jlCCCGElD1MrZZ9K4t0Hrzl5uYiLi6Oi8fFxRnHdXJCCCGElG5GsLapMdN5tulHH32EQYMG4caNG2jUqBEA4MSJE5g9ezY++ugjvXdQX4qywoJIcaphi5RE1W9Dbqs429NnP/T9Hwlj2KfXsVKFHPp8b41ln+i4Ntz2jPW7BxjHPhnL5/Tqe5uWloY6deroq0tFw2QOzBgN3mT5/vvv4ebmhvnz5yMxMRHAyyWzJkyYgHHjxum9g4QQQggpW+ReEi2rl011HryVK1cOEyZMwIQJE6BUKgGAJioQQgghRH90XNu0rClykV6ABm2EEEIIMQCabVogWYO3evXqyVqIFQDOnDlTrA4RQgghpGxjKhWYSsbyWDLavIlkDd66dOmi+XdmZiYWL14Mf39/hISEAHg5YeHSpUsYPny4QTqpD7TCAiHkdaHfmjdHWfwsjaJyRG4OkCtjxYFcwxXpNWayBm/h4eGaf3/88ccYNWoUvvnmG65NQkKCfntHCCGEkDLHUAvTvyl0/i/F5s2bMWDAAC7er18/bNmyRS+dIoQQQkjZRUV6C6bz4M3KygrHjh3j4seOHYOlpaVeOkUIIYSQsoupGZhKXfhNrd86eaWFzrNNx4wZg2HDhiEmJkZSpHfFihX4+uuv9d5BQgghhJQteYMzOe3KIp0Hb5MmTULVqlWxcOFCrF+/HgBQs2ZNrFq1Cj179tR7B/VFXyssyKXvqtnaDJ1Qauj+l8Rrlvbti5T278Gb+D17Ha9REgnl9F3QXWk9PtPS0lCtWjWDbFsuKtJbsCLVeevZs6dRD9QIIYQQUnrRmbeCFatILyGEEEKIvtHgrWCyBm+Ojo64du0anJ2d4eDgUGDB3qdPn+qtc4QQQggpe5hKBTUV6c2XrMHb/PnzYWtrCwBYsGCBIftjMFSklxBCyOtUWv/mGEORXnVuLtQ5hRfpVefmvobeGB9Zg7eBAwcK/00IIYQQom/GcNn02bNnGDVqFHbs2AEAeP/99/HTTz+hQoUK+T4nLCwMq1evlsSCg4Nx4sQJzf2srCyMHz8eGzZsQEZGBlq2bInFixejcuXKsvsmO+dNqVTKakeL1RNCCCGkOJiayZxtargZw3369MG9e/ewd+9eAMDQoUPRv39//P333wU+r127dli5cqXmvrm5ueTxMWPG4O+//8bGjRvh5OSEcePGoVOnToiJiYGJiYwlwaDD4K1ChQoF5roxxqBQKKAqo9efCSGEEKIfapUaahln1eS0KYq4uDjs3bsXJ06cQHBwMADg119/RUhICK5evYrq1avn+1wLCwu4ubkJH0tNTcXy5cvx+++/o1WrVgCAtWvXwtPTEwcOHEDbtm1l9U/24O3QoUOafzPG0KFDB/z222+oVKmS3E0QQgghhBRK18um2lcHLSwsYGFhUeTXj46Ohr29vWbgBgCNGjWCvb09jh8/XuDg7fDhw3BxcUGFChXQrFkzzJgxAy4uLgCAmJgY5OTkoE2bNpr2Hh4eCAgIwPHjx/U/eGvWrJnkvomJCRo1aoSqVavK3USJet1FeuUqicKUchhDwmp+jPU9M9Z+AcbTN2Pphzb6vuvOWPsFUN8KU9j3XalUwsvL6zX1RkzXwZunp6ckHh4ejoiIiCK/flJSkmbA9SoXFxckJSXl+7z27dujR48e8PLyQnx8PKZOnYp3330XMTExsLCwQFJSEszNzeHg4CB5nqura4Hb1UZ13gghhBBiVBiTucICe9kmISFBcoImv7NuERERmDZtWoHbPHXqFAAIU8XyUsTy06tXL82/AwICEBQUBC8vL+zatQvdunUrYD8K3q42GrwRQgghxKjoeubNzs5O1oTJkSNHonfv3gW28fb2xvnz5/Hw4UPusUePHsHV1bXQ18nj7u4OLy8vXL9+HQDg5uaG7OxsPHv2THL2LTk5GaGhobK3W6zBmy6jREIIIYQQOQxVKsTZ2RnOzs6FtgsJCUFqair+++8/NGzYEABw8uRJpKam6jTIevLkCRISEuDu7g4AaNCgAczMzBAZGalZZjQxMREXL17E3LlzZW9X9uBN+3RfZmYmPv30U9jY2EjiW7dulf3ihBBCCCHaVDm5UMkocqzKMUyR3po1a6Jdu3YYMmQIfvnlFwAvS4V06tRJMlmhRo0amDVrFrp27Yr09HRERESge/fucHd3x+3bt/Hll1/C2dkZXbt2BQDY29tj8ODBGDduHJycnODo6Ijx48cjMDBQM/tUDtmDN3t7e8n9fv36yX4RY0ArLBBCCCGFM4YJPC/PvMlZHstwfV23bh1GjRqlmRn6/vvvY9GiRZI2V69eRWpqKoCXEzkvXLiANWvWICUlBe7u7mjRogU2bdqkWaUKeLlqlampKXr27Kkp0rtq1SrZNd4AQMGMYeqLASmVStjb28PNzY0Gb4QQQkgh1Go1kpKSkJqa+toL7+f9zT77STfYWpgV2j4tKwf1ftlaIn0tSTRhgRBCCCFGhall5rwZwVnCkkCDN0IIIYQYF5kTFmDAy6bGjAZvhBBCCDEqJb08lrErM4M3Y11hQZ/exPRFY0icfR3exM/uTdwnkTdxP9/EfRJ5E39f9PHZKZVKeHh46KE3RcfUMov0voGfoRxGk8E/a9YsKBQKjBkzRhNjjCEiIgIeHh6wsrJC8+bNcenSpZLrJCGEEEIMLq/Om5xbWWQUg7dTp05h2bJlqF27tiQ+d+5czJs3D4sWLcKpU6fg5uaG1q1bIy0trYR6SgghhBBDYyom+1YWlfjgLT09HX379sWvv/4qWSqCMYYFCxZgypQp6NatGwICArB69Wq8ePEC69evz3d7WVlZUCqVkhshhBBCSg+1Wq3JeyvwRpdNS8aIESPQsWNHrrJwfHw8kpKSNMXxgJcLzTZr1gzHjx/Pd3uzZs2Cvb295ubp6WmwvhNCCCFE/1Q5DKoctYxb2TzzVqITFjZu3IgzZ87g1KlT3GNJSUkAwC0A6+rqijt37uS7zcmTJ2Ps2LGa+0qlEp6enrTCAiGEECKDMZzNYio1WDn9r236piixwVtCQgJGjx6N/fv3w9LSMt92CoV0VihjjIu9ysLCAhYWFnrrJyGEEEJeL6ZiYOUKP6tGOW+vWUxMDJKTk9GgQQOYmprC1NQUUVFR+PHHH2Fqaqo545Z3Bi5PcnIydzaOEEIIIW8OtYrJvpVFJTZ4a9myJS5cuIDY2FjNLSgoCH379kVsbCyqVq0KNzc3REZGap6TnZ2NqKgohIaGllS3CSGEEGJgVCqkYCV22dTW1hYBAQGSmI2NDZycnDTxMWPGYObMmfDz84Ofnx9mzpwJa2tr9OnTR+fXKwtFeg2trBTuNDRjyCd5E9D3UT/ofdSPN+l9VCqVcHFxKdE+qBmDWl34e6p+g953XRj1CgsTJkxARkYGhg8fjmfPniE4OBj79++Hra1tSXeNEEIIIYaiYmAKGQOzMnrZ1KgGb4cPH5bcVygUiIiIQERERIn0hxBCCCGvn1qlhlpBa5vmx6gGb4QQQgghTOaZt7I625QGb4QQQggxKqpsFVTqwnPTVbmq19Ab41NmBm9UpJcQQggpnDFMqmKMgcmYsPAmTRTRBY1mCCGEEGJUjKHO27Nnz9C/f3/Ncpv9+/dHSkpKgc9RKBTC23fffadp07x5c+7x3r1769S3MnPmjRBCCCGlA1MxMMhZHstwg7c+ffrg3r172Lt3LwBg6NCh6N+/P/7+++98n5OYmCi5v2fPHgwePBjdu3eXxIcMGYLp06dr7ltZWenUNxq8EUIIIcSovBy8ldyEhbi4OOzduxcnTpxAcHAwAODXX39FSEgIrl69iurVqwuf5+bmJrn/119/oUWLFqhataokbm1tzbXVBV02JYQQQohR0fWyqVKplNyysrKK9frR0dGwt7fXDNwAoFGjRrC3t8fx48dlbePhw4fYtWsXBg8ezD22bt06ODs7o1atWhg/fjzS0tJ06l+ZOfNGKyy82cpq0mpZYQwJ1MSw6Bg2HkqlEs7OziXaB6ZWgykK/zvN/v+3wdPTUxIPDw8vVo3YpKQk4SoTLi4u3Jrr+Vm9ejVsbW3RrVs3Sbxv377w8fGBm5sbLl68iMmTJ+PcuXOS5UALU2YGb4QQQggpHdQqBrWMy6Z5Z94SEhIkJ2gsLCyE7SMiIjBt2rQCt3nq1CkALycfaGOMCeMiK1asQN++fWFpaSmJDxkyRPPvgIAA+Pn5ISgoCGfOnEH9+vVlbZsGb4QQQggxKkwtM+ft/8uJ2NnZaV1dExs5cmShMzu9vb1x/vx5PHz4kHvs0aNHcHV1LfR1jh49iqtXr2LTpk2Ftq1fvz7MzMxw/fp1GrwRQgghpHRSZ6ugkpGVr1brVqTX2dlZ1iXhkJAQpKam4r///kPDhg0BACdPnkRqaipCQ0MLff7y5cvRoEED1KlTp9C2ly5dQk5ODtzd3Qvfgf9HExYIIYQQYlSYism+GULNmjXRrl07DBkyBCdOnMCJEycwZMgQdOrUSTLTtEaNGti2bZvkuUqlEps3b8bHH3/MbffmzZuYPn06Tp8+jdu3b2P37t3o0aMH6tWrh8aNG8vuX5k580YrLBBCCCGFM4YJQmrGoJYxiUVOm6Jat24dRo0ahTZt2gAA3n//fSxatEjS5urVq0hNTZXENm7cCMYYPvzwQ26b5ubm+Oeff7Bw4UKkp6fD09MTHTt2RHh4OExMTGT3TcHe8Ck+SqUS9vb2cHNzo8EbIYQQUgi1Wo2kpCSkpqbKyiPTp7y/2WsrVod1ucIHMy/UKvR7dLVE+lqSysyZN0IIIYSUDir28ianXVlEgzdCCCGEGBUVY1DJuDAop82biAZvhBBCCDEqdOatYGVm8EYrLJDS7g1PTyVvKGNIfie6USqVcHJyKtE+qGWeeTPkhAVjVmYGb4QQQggpHVSQeebN4D0xTjR4I4QQQohRyVYzmCgKH71l05k3QgghhJCSp2LyzqpRzhshhBBCiBFQMQaVjLVNabbpG45WWCCEEEIKZwyTTNQyz7ypy+bYrewM3gghhBBSOtCZt4LR4I0QQgghRoVy3gpGgzdCCCGEGJWXgzc5Z95eQ2eMUJkZvFGRXkIIQMWOCSmMUqmEg4NDifaBzrwVrMwM3gghhBBSOlDOW8Fo8EYIIYQQo8IAyJnzWjaHbjR4I4QQQoiRyVYzKGiFhXzR4I0QQgghRoUumxaszAzeqEgvIYQQUjhjKNJLExYKVmYGb4QQQggpHejMW8Fo8EYIIYQQo0LLYxWMBm+EEEIIMSp05q1gb/zgLa8gpzFcwyeEEEKMXd7fy5IsaJ0Btax8tmxZBUXePG/84O3JkycAgOTk5BLuCSGEEFJ6pKWlwd7e/rW+prm5Odzc3LAu6b7s57i5ucHc3NyAvTI+CvaGrxWTkpICBwcH3L1797V/CXWlVCrh6emJhIQEraW8SofS3n+g9O8D9b9kUf9LFvVfPxhjSEtLg4eHR4lUacjMzER2drbs9ubm5rC0tDRgj4zPG3/mLe+LZ29vX2oOZjs7u1LTV5HS3n+g9O8D9b9kUf9LFvW/+EryZIelpWWZG4zpigqfEUIIIYSUIjR4I4QQQggpRd74wZuFhQXCw8NhYWFR0l0pVGnqq0hp7z9Q+veB+l+yqP8li/pPyoo3fsICIYQQQsib5I0/80YIIYQQ8iahwRshhBBCSClCgzdCCCGEkFKEBm+EEEIIIaXIGzt4O3LkCN577z14eHhAoVBg+/btJd0ljcL6xhhDREQEPDw8YGVlhebNm+PSpUsl01mBWbNm4e2334atrS1cXFzQpUsXXL16VdLGmPdhyZIlqF27tqYQZkhICPbs2aN53Jj7LjJr1iwoFAqMGTNGEzPmfYiIiIBCoZDc3NzcNI8bc9/z3L9/H/369YOTkxOsra1Rt25dxMTEaB435n3w9vbm3n+FQoERI0YAMO6+A0Bubi6++uor+Pj4wMrKClWrVsX06dMl61cb+z6kpaVhzJgx8PLygpWVFUJDQ3Hq1CnN48bef2IE2Btq9+7dbMqUKWzLli0MANu2bVtJd0mjsL7Nnj2b2drasi1btrALFy6wXr16MXd3d6ZUKkumw1ratm3LVq5cyS5evMhiY2NZx44dWZUqVVh6erqmjTHvw44dO9iuXbvY1atX2dWrV9mXX37JzMzM2MWLFxljxt13bf/99x/z9vZmtWvXZqNHj9bEjXkfwsPDWa1atVhiYqLmlpycrHncmPvOGGNPnz5lXl5eLCwsjJ08eZLFx8ezAwcOsBs3bmjaGPM+JCcnS977yMhIBoAdOnSIMWbcfWeMsW+//ZY5OTmxnTt3svj4eLZ582ZWvnx5tmDBAk0bY9+Hnj17Mn9/fxYVFcWuX7/OwsPDmZ2dHbt37x5jzPj7T0reGzt4e5WxDd5epd03tVrN3Nzc2OzZszWxzMxMZm9vz5YuXVoCPSxccnIyA8CioqIYY6VzHxwcHNhvv/1WqvqelpbG/Pz8WGRkJGvWrJlm8Gbs+xAeHs7q1KkjfMzY+84YYxMnTmTvvPNOvo+Xhn141ejRo5mvry9Tq9Wlou8dO3ZkgwYNksS6devG+vXrxxgz/vf/xYsXzMTEhO3cuVMSr1OnDpsyZYrR958Yhzf2smlpFR8fj6SkJLRp00YTs7CwQLNmzXD8+PES7Fn+UlNTAQCOjo4AStc+qFQqbNy4Ec+fP0dISEip6vuIESPQsWNHtGrVShIvDftw/fp1eHh4wMfHB71798atW7cAlI6+79ixA0FBQejRowdcXFxQr149/Prrr5rHS8M+5MnOzsbatWsxaNAgKBSKUtH3d955B//88w+uXbsGADh37hyOHTuGDh06ADD+9z83NxcqlYpbu9PKygrHjh0z+v4T40CDNyOTlJQEAHB1dZXEXV1dNY8ZE8YYxo4di3feeQcBAQEASsc+XLhwAeXLl4eFhQU+/fRTbNu2Df7+/qWi7wCwceNGnDlzBrNmzeIeM/Z9CA4Oxpo1a7Bv3z78+uuvSEpKQmhoKJ48eWL0fQeAW7duYcmSJfDz88O+ffvw6aefYtSoUVizZg0A43//X7V9+3akpKQgLCwMQOno+8SJE/Hhhx+iRo0aMDMzQ7169TBmzBh8+OGHAIx/H2xtbRESEoJvvvkGDx48gEqlwtq1a3Hy5EkkJiYaff+JcTAt6Q4QMYVCIbnPGONixmDkyJE4f/48jh07xj1mzPtQvXp1xMbGIiUlBVu2bMHAgQMRFRWledyY+56QkIDRo0dj//793P/eX2Ws+9C+fXvNvwMDAxESEgJfX1+sXr0ajRo1AmC8fQcAtVqNoKAgzJw5EwBQr149XLp0CUuWLMGAAQM07Yx5H/IsX74c7du3h4eHhyRuzH3ftGkT1q5di/Xr16NWrVqIjY3FmDFj4OHhgYEDB2raGfM+/P777xg0aBAqVaoEExMT1K9fH3369MGZM2c0bYy5/6Tk0Zk3I5M36077f1jJycnc/8RK2meffYYdO3bg0KFDqFy5siZeGvbB3Nwc1apVQ1BQEGbNmoU6depg4cKFpaLvMTExSE5ORoMGDWBqagpTU1NERUXhxx9/hKmpqaafxrwPr7KxsUFgYCCuX79eKt5/d3d3+Pv7S2I1a9bE3bt3AZSO7z8A3LlzBwcOHMDHH3+siZWGvn/xxReYNGkSevfujcDAQPTv3x+ff/655ix0adgHX19fREVFIT09HQkJCfjvv/+Qk5MDHx+fUtF/UvJo8GZk8g7eyMhITSw7OxtRUVEIDQ0twZ79D2MMI0eOxNatW3Hw4EH4+PhIHi8N+6CNMYasrKxS0feWLVviwoULiI2N1dyCgoLQt29fxMbGomrVqka/D6/KyspCXFwc3N3dS8X737hxY640zrVr1+Dl5QWg9Hz/V65cCRcXF3Ts2FETKw19f/HiBcqVk/7pMjEx0ZQKKQ37kMfGxgbu7u549uwZ9u3bh86dO5eq/pMSVDLzJAwvLS2NnT17lp09e5YBYPPmzWNnz55ld+7cKemuFdq32bNnM3t7e7Z161Z24cIF9uGHHxrVNPFhw4Yxe3t7dvjwYUnJgRcvXmjaGPM+TJ48mR05coTFx8ez8+fPsy+//JKVK1eO7d+/nzFm3H3Pz6uzTRkz7n0YN24cO3z4MLt16xY7ceIE69SpE7O1tWW3b99mjBl33xl7WZ7F1NSUzZgxg12/fp2tW7eOWVtbs7Vr12raGPs+qFQqVqVKFTZx4kTuMWPv+8CBA1mlSpU0pUK2bt3KnJ2d2YQJEzRtjH0f9u7dy/bs2cNu3brF9u/fz+rUqcMaNmzIsrOzGWPG339S8t7YwduhQ4cYAO42cODAku5aoX1Tq9UsPDycubm5MQsLC9a0aVN24cKFku30K0R9B8BWrlypaWPM+zBo0CDm5eXFzM3NWcWKFVnLli01AzfGjLvv+dEevBnzPuTVrDIzM2MeHh6sW7du7NKlS5rHjbnvef7++28WEBDALCwsWI0aNdiyZcskjxv7Puzbt48BYFevXuUeM/a+K5VKNnr0aFalShVmaWnJqlatyqZMmcKysrI0bYx9HzZt2sSqVq3KzM3NmZubGxsxYgRLSUnRPG7s/SclT8EYY6/5ZB8hhBBCCCkiynkjhBBCCClFaPBGCCGEEFKK0OCNEEIIIaQUocEbIYQQQkgpQoM3QgghhJBShAZvhBBCCCGlCA3eCCGEEEJKERq8EUIIIYSUIjR4I6SUu337NhQKBWJjYwts17x5c4wZM8agfTl8+DAUCgVSUlIM+jqEEFKW0QoLhLwGYWFhWL16NQDA1NQUnp6e6NatG6ZNmwYbG5tibVulUuHRo0dwdnaGqakpDh8+jBYtWuDZs2eoUKGCpt3Tp09hZmYGW1vbYr1eQbKzs/H06VO4urpCoVAY7HUIIaQsMy3pDhBSVrRr1w4rV65ETk4Ojh49io8//hjPnz/HkiVLirVdExMTuLm5FdrO0dGxWK8jh7m5uay+EEIIKTq6bErIa2JhYQE3Nzd4enqiT58+6Nu3L7Zv3w4AyMrKwqhRo+Di4gJLS0u88847OHXqlOa5z549Q9++fVGxYkVYWVnBz88PK1euBCC9bHr79m20aNECAODg4ACFQoGwsDAA/GXTZ8+eYcCAAXBwcIC1tTXat2+P69evax5ftWoVKlSogH379qFmzZooX7482rVrh8TExHz3UfuyaVG2UdC+AsD9+/fRq1cvODg4wMnJCZ07d8bt27c1j6tUKowdOxYVKlSAk5MTJkyYgIEDB6JLly6aNt7e3liwYIHkdevWrYuIiAjN/dTUVAwdOhQuLi6ws7PDu+++i3Pnzmkej4iIQN26dfH777/D29sb9vb26N27N9LS0jRt1Go15syZg2rVqsHCwgJVqlTBjBkzZO8LIYSI0OCNkBJiZWWFnJwcAMCECROwZcsWrF69GmfOnEG1atXQtm1bPH36FAAwdepUXL58GXv27EFcXByWLFkCZ2dnbpuenp7YsmULAODq1atITEzEwoULha8fFhaG06dPY8eOHYiOjgZjDB06dND0CQBevHiB77//Hr///juOHDmCu3fvYvz48Trtp67bKGhfX7x4gRYtWqB8+fI4cuQIjh07phkQZmdnAwB++OEHrFixAsuXL8exY8fw9OlTbNu2Tac+M8bQsWNHJCUlYffu3YiJiUH9+vXRsmVLzWcCADdv3sT27duxc+dO7Ny5E1FRUZg9e7bm8cmTJ2POnDmafVq/fj1cXV1l7wshhAgxQojBDRw4kHXu3Flz/+TJk8zJyYn17NmTpaenMzMzM7Zu3TrN49nZ2czDw4PNnTuXMcbYe++9xz766CPhtuPj4xkAdvbsWcYYY4cOHWIA2LNnzyTtmjVrxkaPHs0YY+zatWsMAPv33381jz9+/JhZWVmxP/74gzHG2MqVKxkAduPGDU2bn3/+mbm6uua7n9qvXZRtFLSvy5cvZ9WrV2dqtVoTy8rKYlZWVmzfvn2MMcbc3d3Z7NmzNY/n5OSwypUrS95/Ly8vNn/+fMm269Spw8LDwxljjP3zzz/Mzs6OZWZmStr4+vqyX375hTHGWHh4OLO2tmZKpVLz+BdffMGCg4MZY4wplUpmYWHBfv311yLvCyGEiFDOGyGvyc6dO1G+fHnk5uYiJycHnTt3xk8//YSbN28iJycHjRs31rQ1MzNDw4YNERcXBwAYNmwYunfvjjNnzqBNmzbo0qULQkNDi9yXuLg4mJqaIjg4WBNzcnJC9erVNa8JANbW1vD19dXcd3d3R3Jysk6vpes2CtrXmJgY3Lhxg5t0kZmZiZs3byI1NRWJiYkICQnRPGZqaoqgoCAwHeZmxcTEID09HU5OTpJ4RkYGbt68qbnv7e0t6cur+xYXF4esrCy0bNky39coaF8IISQ/NHgj5DVp0aIFlixZAjMzM3h4eMDMzAwANPlf2rMzGWOaWPv27XHnzh3s2rULBw4cQMuWLTFixAh8//33RepLfgOZV18TgKaPeRQKhU6DoKJso6B9VavVaNCgAdatW8c9r2LFirL7VK5cOa4Pr14uVqvVcHd3x+HDh7nnvjqDV7RvarUawMvL4gXR174QQsoeynkj5DWxsbFBtWrV4OXlJfmjX61aNZibm+PYsWOaWE5ODk6fPo2aNWtqYhUrVkRYWBjWrl2LBQsWYNmyZcLXMTc3B/AycT8//v7+yM3NxcmTJzWxJ0+e4Nq1a5LXLCn57Wv9+vVx/fp1uLi4oFq1apKbvb097O3t4e7ujhMnTmi2lZubi5iYGG77r06aUCqViI+P19yvX78+kpKSYGpqyr2OKNdQxM/PD1ZWVvjnn3+Ejxe2L4QQkh8avBFSwmxsbDBs2DB88cUX2Lt3Ly5fvowhQ4bgxYsXGDx4MADg66+/xl9//YUbN27g0qVL2LlzZ76DLC8vLygUCuzcuROPHj1Ceno618bPzw+dO3fGkCFDcOzYMZw7dw79+vVDpUqV0LlzZ4Pub2EK2te+ffvC2dkZnTt3xtGjRxEfH4+oqCiMHj0a9+7dAwCMHj0as2fPxrZt23DlyhUMHz6cKxr87rvv4vfff8fRo0dx8eJFDBw4ECYmJprHW7VqhZCQEHTp0gX79u3D7du3cfz4cXz11Vc4ffq0rP2wtLTExIkTMWHCBKxZswY3b97EiRMnsHz5ctn7QgghIjR4I8QIzJ49G927d0f//v1Rv3593LhxA/v27YODgwOAl2fTJk+ejNq1a6Np06YwMTHBxo0bhduqVKkSpk2bhkmTJsHV1RUjR44Utlu5ciUaNGiATp06ISQkBIwx7N69m7sU+LoVtK/W1tY4cuQIqlSpgm7duqFmzZoYNGgQMjIyYGdnBwAYN24cBgwYgLCwMISEhMDW1hZdu3aVvMbkyZPRtGlTdOrUCR06dECXLl0keXkKhQK7d+9G06ZNMWjQILz11lvo3bs3bt++rZktKsfUqVMxbtw4fP3116hZsyZ69eqlyYmTsy+EECJCKywQQt54YWFhSElJ0dTVI4SQ0ozOvBFCCCGElCI0eCOEEEIIKUXosikhhBBCSClCZ94IIYQQQkoRGrwRQgghhJQiNHgjhBBCCClFaPBGCCGEEFKK0OCNEEIIIaQUocEbIYQQQkgpQoM3QgghhJBShAZvhBBCCCGlyP8Bsgv6oJiRAiAAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 800x300 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## PyTorch\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data\n",
    "import torch.optim as optim\n",
    "\n",
    "def scaled_dot_product(q, k, v, mask=None):\n",
    "    d_k = q.size()[-1]\n",
    "    attn_logits = torch.matmul(q, k.transpose(-2, -1))\n",
    "    attn_logits = attn_logits / math.sqrt(d_k)\n",
    "    if mask is not None:\n",
    "        attn_logits = attn_logits.masked_fill(mask == 0, -9e15)\n",
    "    attention = F.softmax(attn_logits, dim=-1)\n",
    "    values = torch.matmul(attention, v)\n",
    "    return values, attention\n",
    "\n",
    "seq_len, d_k = 3, 2\n",
    "# pl.seed_everything(42)\n",
    "q = torch.randn(seq_len, d_k)\n",
    "k = torch.randn(seq_len, d_k)\n",
    "v = torch.randn(seq_len, d_k)\n",
    "values, attention = scaled_dot_product(q, k, v)\n",
    "print(\"Q\\n\", q)\n",
    "print(\"K\\n\", k)\n",
    "print(\"V\\n\", v)\n",
    "print(\"Values\\n\", values)\n",
    "print(\"Attention\\n\", attention)\n",
    "\n",
    "class MultiheadAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim, embed_dim, num_heads):\n",
    "        super().__init__()\n",
    "        assert embed_dim % num_heads == 0, \"Embedding dimension must be 0 modulo number of heads.\"\n",
    "\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "\n",
    "        # Stack all weight matrices 1...h together for efficiency\n",
    "        # Note that in many implementations you see \"bias=False\" which is optional\n",
    "        self.qkv_proj = nn.Linear(input_dim, 3*embed_dim)\n",
    "        self.o_proj = nn.Linear(embed_dim, embed_dim)\n",
    "\n",
    "        self._reset_parameters()\n",
    "\n",
    "    def _reset_parameters(self):\n",
    "        # Original Transformer initialization, see PyTorch documentation\n",
    "        nn.init.xavier_uniform_(self.qkv_proj.weight)\n",
    "        self.qkv_proj.bias.data.fill_(0)\n",
    "        nn.init.xavier_uniform_(self.o_proj.weight)\n",
    "        self.o_proj.bias.data.fill_(0)\n",
    "\n",
    "    def forward(self, x, mask=None, return_attention=False):\n",
    "        batch_size, seq_length, _ = x.size()\n",
    "        qkv = self.qkv_proj(x)\n",
    "\n",
    "        # Separate Q, K, V from linear output\n",
    "        qkv = qkv.reshape(batch_size, seq_length, self.num_heads, 3*self.head_dim)\n",
    "        qkv = qkv.permute(0, 2, 1, 3) # [Batch, Head, SeqLen, Dims]\n",
    "        q, k, v = qkv.chunk(3, dim=-1)\n",
    "\n",
    "        # Determine value outputs\n",
    "        values, attention = scaled_dot_product(q, k, v, mask=mask)\n",
    "        values = values.permute(0, 2, 1, 3) # [Batch, SeqLen, Head, Dims]\n",
    "        values = values.reshape(batch_size, seq_length, self.embed_dim)\n",
    "        o = self.o_proj(values)\n",
    "\n",
    "        if return_attention:\n",
    "            return o, attention\n",
    "        else:\n",
    "            return o\n",
    "        \n",
    "class EncoderBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim, num_heads, dim_feedforward, dropout=0.0):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "            input_dim - Dimensionality of the input\n",
    "            num_heads - Number of heads to use in the attention block\n",
    "            dim_feedforward - Dimensionality of the hidden layer in the MLP\n",
    "            dropout - Dropout probability to use in the dropout layers\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # Attention layer\n",
    "        self.self_attn = MultiheadAttention(input_dim, input_dim, num_heads)\n",
    "\n",
    "        # Two-layer MLP\n",
    "        self.linear_net = nn.Sequential(\n",
    "            nn.Linear(input_dim, dim_feedforward),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(dim_feedforward, input_dim)\n",
    "        )\n",
    "\n",
    "        # Layers to apply in between the main layers\n",
    "        self.norm1 = nn.LayerNorm(input_dim)\n",
    "        self.norm2 = nn.LayerNorm(input_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        # Attention part\n",
    "        attn_out = self.self_attn(x, mask=mask)\n",
    "        x = x + self.dropout(attn_out)\n",
    "        x = self.norm1(x)\n",
    "\n",
    "        # MLP part\n",
    "        linear_out = self.linear_net(x)\n",
    "        x = x + self.dropout(linear_out)\n",
    "        x = self.norm2(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "class TransformerEncoder(nn.Module):\n",
    "\n",
    "    def __init__(self, num_layers, **block_args):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([EncoderBlock(**block_args) for _ in range(num_layers)])\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        for l in self.layers:\n",
    "            x = l(x, mask=mask)\n",
    "        return x\n",
    "\n",
    "    def get_attention_maps(self, x, mask=None):\n",
    "        attention_maps = []\n",
    "        for l in self.layers:\n",
    "            _, attn_map = l.self_attn(x, mask=mask, return_attention=True)\n",
    "            attention_maps.append(attn_map)\n",
    "            x = l(x)\n",
    "        return attention_maps\n",
    "    \n",
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        \"\"\"\n",
    "        Inputs\n",
    "            d_model - Hidden dimensionality of the input.\n",
    "            max_len - Maximum length of a sequence to expect.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # Create matrix of [SeqLen, HiddenDim] representing the positional encoding for max_len inputs\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "\n",
    "        # register_buffer => Tensor which is not a parameter, but should be part of the modules state.\n",
    "        # Used for tensors that need to be on the same device as the module.\n",
    "        # persistent=False tells PyTorch to not add the buffer to the state dict (e.g. when we save the model)\n",
    "        self.register_buffer('pe', pe, persistent=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:, :x.size(1)]\n",
    "        return x\n",
    "    \n",
    "import matplotlib.pyplot as plt\n",
    "plt.set_cmap('cividis')\n",
    "%matplotlib inline    \n",
    "    \n",
    "encod_block = PositionalEncoding(d_model=48, max_len=96)\n",
    "pe = encod_block.pe.squeeze().T.cpu().numpy()\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(8,3))\n",
    "pos = ax.imshow(pe, cmap=\"RdGy\", extent=(1,pe.shape[1]+1,pe.shape[0]+1,1))\n",
    "fig.colorbar(pos, ax=ax)\n",
    "ax.set_xlabel(\"Position in sequence\")\n",
    "ax.set_ylabel(\"Hidden dimension\")\n",
    "ax.set_title(\"Positional encoding over hidden dimensions\")\n",
    "ax.set_xticks([1]+[i*10 for i in range(1,1+pe.shape[1]//10)])\n",
    "ax.set_yticks([1]+[i*10 for i in range(1,1+pe.shape[0]//10)])\n",
    "plt.show()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mzhao\\AppData\\Local\\Temp\\ipykernel_249972\\1646494204.py:2: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\c10/core/TensorImpl.h:1761.)\n",
      "  _ = torch.tensor([0.2126, 0.7152, 0.0722], names=['c'])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "_ = torch.tensor([0.2126, 0.7152, 0.0722], names=['c'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 3, 5, 5]), torch.Size([2, 3, 5, 5]), torch.Size([3, 1, 1]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_t = torch.randn(3, 5, 5) # shape [channels, rows, columns]\n",
    "weights = torch.tensor([0.2126, 0.7152, 0.0722])\n",
    "batch_t = torch.randn(2, 3, 5, 5) # shape [batch, channels, rows, columns]\n",
    "img_gray_naive = img_t.mean(-3)\n",
    "batch_gray_naive = batch_t.mean(-3)\n",
    "img_gray_naive.shape, batch_gray_naive.shape\n",
    "\n",
    "\n",
    "unsqueezed_weights = weights.unsqueeze(-1).unsqueeze_(-1)\n",
    "img_weights = (img_t * unsqueezed_weights)\n",
    "batch_weights = (batch_t * unsqueezed_weights)\n",
    "img_gray_weighted = img_weights.sum(-3)\n",
    "batch_gray_weighted = batch_weights.sum(-3)\n",
    "batch_weights.shape, batch_t.shape, unsqueezed_weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_t.size(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[0.2126]],\n",
       " \n",
       "         [[0.7152]],\n",
       " \n",
       "         [[0.0722]]]),\n",
       " tensor([[[ 0.0807,  0.3880, -0.0241, -0.0897,  0.0553],\n",
       "          [ 0.1387, -0.3558,  0.0803,  0.3073,  0.0417],\n",
       "          [ 0.1513,  0.3157,  0.1681, -0.1146,  0.5032],\n",
       "          [ 0.1700, -0.1654, -0.1617,  0.0277, -0.1240],\n",
       "          [ 0.0384,  0.2880,  0.3413, -0.0979,  0.0711]],\n",
       " \n",
       "         [[ 0.0989,  0.1896,  0.2428, -1.1069,  0.8830],\n",
       "          [ 0.9399,  1.1653, -0.0043, -0.0417,  1.0848],\n",
       "          [-0.1445, -0.9302, -0.3292, -0.2906,  0.9009],\n",
       "          [ 0.7256, -0.5880, -0.4107,  0.2461, -0.1811],\n",
       "          [ 1.1483, -0.7916, -1.0151,  0.2985,  0.2352]],\n",
       " \n",
       "         [[-0.0019, -0.0944,  0.0474,  0.0087, -0.0428],\n",
       "          [ 0.0460,  0.0536, -0.0283,  0.0961, -0.0517],\n",
       "          [ 0.0257,  0.0126,  0.0092,  0.0404, -0.1280],\n",
       "          [-0.1045, -0.0228, -0.0848,  0.0314, -0.0468],\n",
       "          [-0.0302,  0.0103,  0.1589, -0.1518, -0.0530]]]),\n",
       " tensor([[ 0.0807,  0.3880, -0.0241, -0.0897,  0.0553],\n",
       "         [ 0.1387, -0.3558,  0.0803,  0.3073,  0.0417],\n",
       "         [ 0.1513,  0.3157,  0.1681, -0.1146,  0.5032],\n",
       "         [ 0.1700, -0.1654, -0.1617,  0.0277, -0.1240],\n",
       "         [ 0.0384,  0.2880,  0.3413, -0.0979,  0.0711]]),\n",
       " tensor([[ 0.0989,  0.1896,  0.2428, -1.1069,  0.8830],\n",
       "         [ 0.9399,  1.1653, -0.0043, -0.0417,  1.0848],\n",
       "         [-0.1445, -0.9302, -0.3292, -0.2906,  0.9009],\n",
       "         [ 0.7256, -0.5880, -0.4107,  0.2461, -0.1811],\n",
       "         [ 1.1483, -0.7916, -1.0151,  0.2985,  0.2352]]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unsqueezed_weights, batch_weights[1], batch_t[1][0]*0.2126, batch_t[1][1]*0.7152"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "240px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

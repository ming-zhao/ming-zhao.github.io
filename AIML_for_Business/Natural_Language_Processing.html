<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.450">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Ming Zhao">
<meta name="dcterms.date" content="2024-09-14">

<title>Natural Language Processing</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="Natural_Language_Processing_files/libs/clipboard/clipboard.min.js"></script>
<script src="Natural_Language_Processing_files/libs/quarto-html/quarto.js"></script>
<script src="Natural_Language_Processing_files/libs/quarto-html/popper.min.js"></script>
<script src="Natural_Language_Processing_files/libs/quarto-html/tippy.umd.min.js"></script>
<script src="Natural_Language_Processing_files/libs/quarto-html/anchor.min.js"></script>
<link href="Natural_Language_Processing_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="Natural_Language_Processing_files/libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="Natural_Language_Processing_files/libs/bootstrap/bootstrap.min.js"></script>
<link href="Natural_Language_Processing_files/libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="Natural_Language_Processing_files/libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" integrity="sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script>
<script type="application/javascript">define('jquery', [],function() {return window.jQuery;})</script>

  <script>window.backupDefine = window.define; window.define = undefined;</script><script src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js"></script>
  <script>document.addEventListener("DOMContentLoaded", function () {
 var mathElements = document.getElementsByClassName("math");
 var macros = [];
 for (var i = 0; i < mathElements.length; i++) {
  var texText = mathElements[i].firstChild;
  if (mathElements[i].tagName == "SPAN") {
   katex.render(texText.data, mathElements[i], {
    displayMode: mathElements[i].classList.contains('display'),
    throwOnError: false,
    macros: macros,
    fleqn: false
   });
}}});
  </script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css">

</head>

<body>

<div id="quarto-content" class="page-columns page-rows-contents page-layout-article toc-left">
<div id="quarto-sidebar-toc-left" class="sidebar toc-left">
  <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#word-embedding" id="toc-word-embedding" class="nav-link active" data-scroll-target="#word-embedding">Word Embedding</a>
  <ul class="collapse">
  <li><a href="#tokenization" id="toc-tokenization" class="nav-link" data-scroll-target="#tokenization">Tokenization</a></li>
  <li><a href="#one-hot-encoding" id="toc-one-hot-encoding" class="nav-link" data-scroll-target="#one-hot-encoding">One-hot encoding</a></li>
  <li><a href="#frequency-encoding" id="toc-frequency-encoding" class="nav-link" data-scroll-target="#frequency-encoding">Frequency Encoding</a></li>
  <li><a href="#embedding" id="toc-embedding" class="nav-link" data-scroll-target="#embedding">Embedding</a>
  <ul class="collapse">
  <li><a href="#exploring-word-similarity" id="toc-exploring-word-similarity" class="nav-link" data-scroll-target="#exploring-word-similarity">Exploring Word Similarity</a></li>
  <li><a href="#finding-the-odd-word-out" id="toc-finding-the-odd-word-out" class="nav-link" data-scroll-target="#finding-the-odd-word-out">Finding the Odd Word Out</a></li>
  <li><a href="#performing-mathematical-operations" id="toc-performing-mathematical-operations" class="nav-link" data-scroll-target="#performing-mathematical-operations">Performing Mathematical Operations</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#applications" id="toc-applications" class="nav-link" data-scroll-target="#applications">Applications</a>
  <ul class="collapse">
  <li><a href="#sentiment-analysis-of-imdb-movie-reviews" id="toc-sentiment-analysis-of-imdb-movie-reviews" class="nav-link" data-scroll-target="#sentiment-analysis-of-imdb-movie-reviews">Sentiment Analysis of IMDB Movie Reviews</a></li>
  <li><a href="#recommendation-system" id="toc-recommendation-system" class="nav-link" data-scroll-target="#recommendation-system">Recommendation System</a></li>
  </ul></li>
  </ul>
</nav>
</div>
<div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
</div>
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Natural Language Processing</h1>
</div>



<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Ming Zhao </p>
          </div>
  </div>
    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">September 14, 2024</p>
    </div>
  </div>
  
    
  </div>
  

</header>

<p>
<a href="https://colab.research.google.com/drive/163GItZQt4HYOIAeivnhrgnmpS7c9Bkq_" target="_blank"><img alt="Colab" src="https://colab.research.google.com/assets/colab-badge.svg" style="max-width: 100%;"></a>
</p>
<section id="word-embedding" class="level1">
<h1>Word Embedding</h1>
<p>Machine learning models only work with a numeric representation of input data. Suppose we have a training corpus with three sentences:</p>
<ul>
<li>“the dog saw a cat”,</li>
<li>“the dog chased the cat”,</li>
<li>“the cat climbed a tree”.</li>
</ul>
<p>The corpus vocabulary has eight words, which are listed alphabetically as follows</p>
<table class="table">
<thead>
<tr class="header">
<th>index</th>
<th>vocabulary</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1</td>
<td>a</td>
</tr>
<tr class="even">
<td>2</td>
<td>cat</td>
</tr>
<tr class="odd">
<td>3</td>
<td>chased</td>
</tr>
<tr class="even">
<td>4</td>
<td>climbed</td>
</tr>
<tr class="odd">
<td>5</td>
<td>dog</td>
</tr>
<tr class="even">
<td>6</td>
<td>saw</td>
</tr>
<tr class="odd">
<td>7</td>
<td>the</td>
</tr>
<tr class="even">
<td>8</td>
<td>tree</td>
</tr>
</tbody>
</table>
<section id="tokenization" class="level2">
<h2 class="anchored" data-anchor-id="tokenization">Tokenization</h2>
<p>The different units into which you can break down text are called tokens. The token can be a word (e.g.&nbsp;“cat”), or a phrase (e.g., “the cat”, or “a cat”). Eventually, we will transform each token to a numeric vector.</p>
<div class="cell" data-execution_count="41">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> tensorflow.keras.preprocessing.text <span class="im">import</span> Tokenizer</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>texts <span class="op">=</span> [<span class="st">"the dog saw a cat"</span>,</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>         <span class="st">"the dog chased the cat"</span>,</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>         <span class="st">"the cat climbed a tree"</span>]</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>tokenizer <span class="op">=</span> Tokenizer()</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>tokenizer.fit_on_texts(texts)</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>sequences <span class="op">=</span> tokenizer.texts_to_sequences(texts)</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="co"># print("The count of words", tokenizer.word_counts)</span></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"The sequences generated from text are:"</span>, sequences)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>The sequences generated from text are: [[1, 3, 5, 4, 2], [1, 3, 6, 1, 2], [1, 2, 7, 4, 8]]</code></pre>
</div>
</div>
<p>The code provides an example of tokenization. It creates tokens for each of the 8 words in the vocabulary and assigns an integer to each token (e.g., “then” = 1 and “dog” = 3). As a result, each sentence is transformed into a sequence of numbers.</p>
</section>
<section id="one-hot-encoding" class="level2">
<h2 class="anchored" data-anchor-id="one-hot-encoding">One-hot encoding</h2>
<p>One-hot encoding is the most common, most basic way to turn a token into a vector. It is a binary vector of the size of the vocabulary where the vector has 1 for the index of the word, and 0 elsewhere. For example, “cat” has index 2 in the alphabetical order of the vocabulary. Then, the one-hot encoding of the word “cat” can be <span class="math display">\underbrace{[0,1,0,0,0,0,0,0]}_{8}</span> and, the sentence “the dog saw a cat” can be represented as a <span class="math inline">5 \times 8</span> matrix</p>
<p><span class="math display">
\left[
\begin{matrix}
0,0,0,0,0,0,1,0 \\
0,0,0,0,1,0,0,0\\
0,0,0,0,0,1,0,0\\
1,0,0,0,0,0,0,0\\
0,1,0,0,0,0,0,0
\end{matrix} \right]
</span></p>
<p>since the sentence has 5 words and the corpus vocabulary has 8 words. recall that this is exactly how we reshaped labels when using ConvNet for MNIST data.</p>
<p>One-hot encoding has two drawbacks for natural language processing (NLP) tasks:</p>
<ul>
<li><p>The dimensionality of the vector space becomes very high and sparse because each vector has the size of the vocabulary.</p></li>
<li><p>There is no connection between words with similar meanings. For example, it is impossible to know that “dog” and “cat” are both pets.</p></li>
</ul>
</section>
<section id="frequency-encoding" class="level2">
<h2 class="anchored" data-anchor-id="frequency-encoding">Frequency Encoding</h2>
<p>One-hot encoding is a binary vector of the size of the vocabulary where the vector is all zeros, but has 1 for the index of the word. We can also encode the sentence based on the frequency of each word’s occurrence. Then, the sentence “the dog saw a cat” becomes an array</p>
<p><span class="math display">[4, 2, 1, 2, 3]</span></p>
<p>However, similar to one-hot encoding, this frequency-based encoding does not capture any relationships between words with similar meanings. Even worse, in our example, “dog” and “a” have the same frequency, making them indistinguishable from each other.</p>
</section>
<section id="embedding" class="level2">
<h2 class="anchored" data-anchor-id="embedding">Embedding</h2>
<p>Word embeddings are a popular and powerful way to represent words as vectors. These word vectors capture the semantic meaning of words, allowing for more effective processing in natural language processing (NLP) tasks.</p>
<p><img src="https://raw.githubusercontent.com/ming-zhao/ming-zhao.github.io/master/AIML_for_Business/figure/word_embedding.jpg" width="800"></p>
<p>Given input texts, a word embedding model constructs a numeric vector for each word, transforming the text into a numerical representation. Initially, words are often represented using basic methods like one-hot encoding. These vectors are then passed through an embedding model. This model, built from large corpora of text, functions as a complex mathematical system that performs extensive calculations to generate more informative word vectors. These vectors position words with similar meanings close to each other in the vector space, effectively capturing their semantic relationships.</p>
<p>Word embeddings are foundational in NLP, enabling advanced applications such as sentiment analysis, machine translation, and more. By capturing the nuances of language, word embeddings significantly enhance the performance of machine learning models in understanding and generating human language.</p>
<p>Next, we will use a popular pre-trained embedding model, <strong>word2vec</strong>, to see how we can perform various operations with these vectors.</p>
<p>In word2vec, each word is converted into a vector of numbers. For example, the word “university” might be represented as a 300-dimensional vector, meaning it has 300 numbers associated with it. These numbers capture various aspects of the word’s meaning and context. The example below shows the first 4 numbers in the vector.</p>
<div class="cell" data-execution_count="53">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a>model[<span class="st">'university'</span>][:<span class="dv">4</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="53">
<pre><code>array([-0.0604623, -0.0463157,  0.0678263,  0.114723 ], dtype=float32)</code></pre>
</div>
</div>
<section id="exploring-word-similarity" class="level3">
<h3 class="anchored" data-anchor-id="exploring-word-similarity">Exploring Word Similarity</h3>
<p>One of the cool things about word embeddings is that we can measure how similar two words are by looking at the distance between their vectors. Words with similar meanings will have vectors that are close to each other.</p>
<p>For instance, using the word2vec model, we can find the top 3 words most similar to “university” are: “universities”, “faculty”, “undergraduate”. The number after each word quantifies the probability of the similarity.</p>
<div class="cell" data-execution_count="48">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a>model.most_similar(positive<span class="op">=</span>[<span class="st">'university'</span>], topn <span class="op">=</span> <span class="dv">3</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="48">
<pre><code>[('universities', 0.7003918290138245),
 ('faculty', 0.6780907511711121),
 ('undergraduate', 0.6587096452713013)]</code></pre>
</div>
</div>
</section>
<section id="finding-the-odd-word-out" class="level3">
<h3 class="anchored" data-anchor-id="finding-the-odd-word-out">Finding the Odd Word Out</h3>
<p>Word embeddings can also help us find a word that doesn’t fit in a list of words. For example, given the list [“apple,” “banana,” “car,” “orange”], the word “car” would be identified as the odd one out because it is not a fruit.</p>
<div class="cell" data-execution_count="50">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a>model.doesnt_match([<span class="st">"apple"</span>, <span class="st">"banana"</span>, <span class="st">"car"</span>, <span class="st">"orange"</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="50">
<pre><code>'car'</code></pre>
</div>
</div>
<div class="cell" data-execution_count="34">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a>model.doesnt_match([<span class="st">'breakfast'</span>, <span class="st">'cereal'</span>, <span class="st">'dinner'</span>, <span class="st">'lunch'</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="34">
<pre><code>'cereal'</code></pre>
</div>
</div>
</section>
<section id="performing-mathematical-operations" class="level3">
<h3 class="anchored" data-anchor-id="performing-mathematical-operations">Performing Mathematical Operations</h3>
<p>Another interesting feature of word embeddings is that we can perform mathematical operations with the vectors to find relationships between words. Here are two examples:</p>
<blockquote class="blockquote">
<p>Word Analogy: “woman” + “king” - “man” = “queen”</p>
</blockquote>
<p>This operation finds that the relationship between “woman” and “man” is similar to the relationship between “queen” and “king.”</p>
<blockquote class="blockquote">
<p>Geographic Analogy: “Paris” + “Germany” - “Berlin” = “France”</p>
</blockquote>
<p>This operation shows that the relationship between “Paris” and “Berlin” is similar to the relationship between “France” and “Germany.”</p>
<div class="cell" data-execution_count="56">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a>model.most_similar(positive<span class="op">=</span>[<span class="st">'woman'</span>,<span class="st">'king'</span>], negative<span class="op">=</span>[<span class="st">'man'</span>], topn <span class="op">=</span> <span class="dv">1</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="56">
<pre><code>[('queen', 0.7118193507194519)]</code></pre>
</div>
</div>
<div class="cell" data-execution_count="57">
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a>model.most_similar(positive<span class="op">=</span>[<span class="st">'Paris'</span>,<span class="st">'Germany'</span>], negative<span class="op">=</span>[<span class="st">'Berlin'</span>], topn <span class="op">=</span> <span class="dv">1</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="57">
<pre><code>[('France', 0.7884091138839722)]</code></pre>
</div>
</div>
<p>Word embeddings like word2vec provide a powerful way to represent words and their meanings in a numerical form. This allows us to perform various operations to understand word relationships, find similarities, and even solve word analogies. These capabilities make word embeddings an essential tool in natural language processing and machine learning.</p>
</section>
</section>
</section>
<section id="applications" class="level1">
<h1>Applications</h1>
<section id="sentiment-analysis-of-imdb-movie-reviews" class="level2">
<h2 class="anchored" data-anchor-id="sentiment-analysis-of-imdb-movie-reviews">Sentiment Analysis of IMDB Movie Reviews</h2>
<p>The IMDB dataset is a commonly used dataset for machine learning tutorials related to text and language. It contains 50,000 movie reviews, with 25,000 in the training set and 25,000 in the testing set, collected from IMDB. Each review in the dataset has been labeled with a binary sentiment: positive (1) or negative (0). The following code loads the IMDB data:</p>
<div class="sourceCode" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a>(x_train, y_train), (x_test, y_test) <span class="op">=</span> imdb.load_data(num_words<span class="op">=</span>max_features)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>The following code snippet shows the 11th to 15th words in the dictionary of this dataset:</p>
<div class="cell" data-execution_count="64">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> keras.datasets <span class="im">import</span> imdb</span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a>max_features <span class="op">=</span> <span class="dv">10000</span></span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a>(x_train, y_train), (x_test, y_test) <span class="op">=</span> imdb.load_data(num_words<span class="op">=</span>max_features)</span>
<span id="cb16-5"><a href="#cb16-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-6"><a href="#cb16-6" aria-hidden="true" tabindex="-1"></a>word_index <span class="op">=</span> imdb.get_word_index()</span>
<span id="cb16-7"><a href="#cb16-7" aria-hidden="true" tabindex="-1"></a>reverse_word_index <span class="op">=</span> <span class="bu">dict</span>([(value, key) <span class="cf">for</span> (key, value) <span class="kw">in</span> word_index.items()])</span>
<span id="cb16-8"><a href="#cb16-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-9"><a href="#cb16-9" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Example of dictionary:</span><span class="ch">\n</span><span class="st">"</span>, <span class="bu">dict</span>(<span class="bu">sorted</span>(reverse_word_index.items())[<span class="dv">10</span>:<span class="dv">15</span>]))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Example of dictionary:
 {11: 'this', 12: 'that', 13: 'was', 14: 'as', 15: 'for'}</code></pre>
</div>
</div>
<p>The raw data contains the text of each movie review. The following code snippet shows the first review in the training data, which contains 218 words and has a positive sentiment:</p>
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> show_review(index):</span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a>    decoded_review <span class="op">=</span> <span class="st">' '</span>.join([reverse_word_index.get(i <span class="op">-</span> <span class="dv">3</span>, <span class="st">'?'</span>) <span class="cf">for</span> i <span class="kw">in</span> x_train[index]])</span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">'Review:'</span>, decoded_review)</span>
<span id="cb18-4"><a href="#cb18-4" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">'</span><span class="ch">\n</span><span class="st">'</span>)</span>
<span id="cb18-5"><a href="#cb18-5" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">'Number of words:</span><span class="sc">{}</span><span class="st">'</span>.<span class="bu">format</span>(<span class="bu">len</span>(decoded_review.split())))</span>
<span id="cb18-6"><a href="#cb18-6" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">'</span><span class="ch">\n</span><span class="st">sentiment:'</span>, y_train[<span class="dv">0</span>])</span>
<span id="cb18-7"><a href="#cb18-7" aria-hidden="true" tabindex="-1"></a>    <span class="co"># print('Indices for all ?s:', [x_train[index][i] for i, w in enumerate(decoded_review.split()) if w == '?'])</span></span>
<span id="cb18-8"><a href="#cb18-8" aria-hidden="true" tabindex="-1"></a>show_review(<span class="dv">0</span>)</span>
<span id="cb18-9"><a href="#cb18-9" aria-hidden="true" tabindex="-1"></a><span class="co"># print("Decode review x_train[0][:10]]:\n",</span></span>
<span id="cb18-10"><a href="#cb18-10" aria-hidden="true" tabindex="-1"></a><span class="co">#       ' '.join([reverse_word_index.get(i - 3, '?') for i in x_train[0][:10]]))</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<p>Review: ? this film was just brilliant casting location scenery story direction everyone’s really suited the part they played and you could just imagine being there robert ? is an amazing actor and now the same being director ? father came from the same scottish island as myself so i loved the fact there was a real connection with this film the witty remarks throughout the film were great it was just brilliant so much that i bought the film as soon as it was released for ? and would recommend it to everyone to watch and the fly fishing was amazing really cried at the end it was so sad and you know what they say if you cry at a film it must have been good and this definitely was also ? to the two little boy’s that played the ? of norman and paul they were just brilliant children are often left out of the ? list i think because the stars that play them all grown up are such a big profile for the whole film but these children are amazing and should be praised for what they have done don’t you think the whole story was so lovely because it was true and was someone’s life after all that was shared with us all</p>
<p>Number of words:218</p>
<p>sentiment: 1</p>
<p>Next, we ask AI to help perform some basic exploratory data analysis.</p>
<div class="callout callout-style-default callout-tip callout-titled" title="Prompt">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-1-contents" aria-controls="callout-1" aria-expanded="true" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Prompt
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-1" class="callout-1-contents callout-collapse collapse show">
<div class="callout-body-container callout-body">
<p>Using the IMDB dataset in Keras, we generate key statistics for both the training and test datasets. These statistics include:</p>
<ul>
<li><p>Number of reviews</p></li>
<li><p>Proportion of positive reviews</p></li>
<li><p>Proportion of negative reviews</p></li>
<li><p>Number of words in positive reviews (show min/max/median)</p></li>
<li><p>Number of words in negative reviews (show min/max/median)</p></li>
</ul>
<p>Creates a table with one column for the training data and another column for the test data.</p>
</div>
</div>
</div>
<div class="cell" data-execution_count="60">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> tensorflow.keras.datasets <span class="im">import</span> imdb</span>
<span id="cb19-4"><a href="#cb19-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-5"><a href="#cb19-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Load the IMDB dataset</span></span>
<span id="cb19-6"><a href="#cb19-6" aria-hidden="true" tabindex="-1"></a>(train_data, train_labels), (test_data, test_labels) <span class="op">=</span> imdb.load_data()</span>
<span id="cb19-7"><a href="#cb19-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-8"><a href="#cb19-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate key statistics for the training data</span></span>
<span id="cb19-9"><a href="#cb19-9" aria-hidden="true" tabindex="-1"></a>num_train_reviews <span class="op">=</span> <span class="bu">len</span>(train_data)</span>
<span id="cb19-10"><a href="#cb19-10" aria-hidden="true" tabindex="-1"></a>num_test_reviews <span class="op">=</span> <span class="bu">len</span>(test_data)</span>
<span id="cb19-11"><a href="#cb19-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-12"><a href="#cb19-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Proportions of positive and negative reviews</span></span>
<span id="cb19-13"><a href="#cb19-13" aria-hidden="true" tabindex="-1"></a>proportion_positive_train <span class="op">=</span> np.<span class="bu">sum</span>(train_labels) <span class="op">/</span> num_train_reviews</span>
<span id="cb19-14"><a href="#cb19-14" aria-hidden="true" tabindex="-1"></a>proportion_negative_train <span class="op">=</span> <span class="dv">1</span> <span class="op">-</span> proportion_positive_train</span>
<span id="cb19-15"><a href="#cb19-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-16"><a href="#cb19-16" aria-hidden="true" tabindex="-1"></a>proportion_positive_test <span class="op">=</span> np.<span class="bu">sum</span>(test_labels) <span class="op">/</span> num_test_reviews</span>
<span id="cb19-17"><a href="#cb19-17" aria-hidden="true" tabindex="-1"></a>proportion_negative_test <span class="op">=</span> <span class="dv">1</span> <span class="op">-</span> proportion_positive_test</span>
<span id="cb19-18"><a href="#cb19-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-19"><a href="#cb19-19" aria-hidden="true" tabindex="-1"></a><span class="co"># Number of words in positive/negative reviews</span></span>
<span id="cb19-20"><a href="#cb19-20" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> word_counts(data, labels, positive<span class="op">=</span><span class="va">True</span>):</span>
<span id="cb19-21"><a href="#cb19-21" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> [<span class="bu">len</span>(review) <span class="cf">for</span> review, label <span class="kw">in</span> <span class="bu">zip</span>(data, labels) <span class="cf">if</span> label <span class="op">==</span> positive]</span>
<span id="cb19-22"><a href="#cb19-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-23"><a href="#cb19-23" aria-hidden="true" tabindex="-1"></a>train_data_positive <span class="op">=</span> word_counts(train_data, train_labels, positive<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb19-24"><a href="#cb19-24" aria-hidden="true" tabindex="-1"></a>train_data_negative <span class="op">=</span> word_counts(train_data, train_labels, positive<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb19-25"><a href="#cb19-25" aria-hidden="true" tabindex="-1"></a>test_data_positive <span class="op">=</span> word_counts(test_data, test_labels, positive<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb19-26"><a href="#cb19-26" aria-hidden="true" tabindex="-1"></a>test_data_negative <span class="op">=</span> word_counts(test_data, test_labels, positive<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb19-27"><a href="#cb19-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-28"><a href="#cb19-28" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate min, median, max</span></span>
<span id="cb19-29"><a href="#cb19-29" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> min_median_max(word_counts):</span>
<span id="cb19-30"><a href="#cb19-30" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.<span class="bu">min</span>(word_counts), np.median(word_counts), np.<span class="bu">max</span>(word_counts)</span>
<span id="cb19-31"><a href="#cb19-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-32"><a href="#cb19-32" aria-hidden="true" tabindex="-1"></a>min_words_positive_train, median_words_positive_train, max_words_positive_train <span class="op">=</span> min_median_max(train_data_positive)</span>
<span id="cb19-33"><a href="#cb19-33" aria-hidden="true" tabindex="-1"></a>min_words_negative_train, median_words_negative_train, max_words_negative_train <span class="op">=</span> min_median_max(train_data_negative)</span>
<span id="cb19-34"><a href="#cb19-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-35"><a href="#cb19-35" aria-hidden="true" tabindex="-1"></a>min_words_positive_test, median_words_positive_test, max_words_positive_test <span class="op">=</span> min_median_max(test_data_positive)</span>
<span id="cb19-36"><a href="#cb19-36" aria-hidden="true" tabindex="-1"></a>min_words_negative_test, median_words_negative_test, max_words_negative_test <span class="op">=</span> min_median_max(test_data_negative)</span>
<span id="cb19-37"><a href="#cb19-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-38"><a href="#cb19-38" aria-hidden="true" tabindex="-1"></a><span class="co"># Results</span></span>
<span id="cb19-39"><a href="#cb19-39" aria-hidden="true" tabindex="-1"></a>data <span class="op">=</span> {</span>
<span id="cb19-40"><a href="#cb19-40" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Statistics"</span>: [</span>
<span id="cb19-41"><a href="#cb19-41" aria-hidden="true" tabindex="-1"></a>        <span class="st">"Number of reviews"</span>, <span class="st">"Proportion of positive reviews"</span>, <span class="st">"Proportion of negative reviews"</span>,</span>
<span id="cb19-42"><a href="#cb19-42" aria-hidden="true" tabindex="-1"></a>        <span class="st">"Number of words in positive reviews (Min)"</span>, </span>
<span id="cb19-43"><a href="#cb19-43" aria-hidden="true" tabindex="-1"></a>        <span class="st">"Number of words in positive reviews (Median)"</span>, <span class="st">"Number of words in positive reviews (Max)"</span>,</span>
<span id="cb19-44"><a href="#cb19-44" aria-hidden="true" tabindex="-1"></a>        <span class="st">"Number of words in negative reviews (Min)"</span>, </span>
<span id="cb19-45"><a href="#cb19-45" aria-hidden="true" tabindex="-1"></a>        <span class="st">"Number of words in negative reviews (Median)"</span>, <span class="st">"Number of words in negative reviews (Max)"</span></span>
<span id="cb19-46"><a href="#cb19-46" aria-hidden="true" tabindex="-1"></a>    ],</span>
<span id="cb19-47"><a href="#cb19-47" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Train set"</span>: [</span>
<span id="cb19-48"><a href="#cb19-48" aria-hidden="true" tabindex="-1"></a>        num_train_reviews, proportion_positive_train, proportion_negative_train,</span>
<span id="cb19-49"><a href="#cb19-49" aria-hidden="true" tabindex="-1"></a>        min_words_positive_train, median_words_positive_train, max_words_positive_train,</span>
<span id="cb19-50"><a href="#cb19-50" aria-hidden="true" tabindex="-1"></a>        min_words_negative_train, median_words_negative_train, max_words_negative_train</span>
<span id="cb19-51"><a href="#cb19-51" aria-hidden="true" tabindex="-1"></a>    ],</span>
<span id="cb19-52"><a href="#cb19-52" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Test set"</span>: [</span>
<span id="cb19-53"><a href="#cb19-53" aria-hidden="true" tabindex="-1"></a>        num_test_reviews, proportion_positive_test, proportion_negative_test,</span>
<span id="cb19-54"><a href="#cb19-54" aria-hidden="true" tabindex="-1"></a>        min_words_positive_test, median_words_positive_test, max_words_positive_test,</span>
<span id="cb19-55"><a href="#cb19-55" aria-hidden="true" tabindex="-1"></a>        min_words_negative_test, median_words_negative_test, max_words_negative_test</span>
<span id="cb19-56"><a href="#cb19-56" aria-hidden="true" tabindex="-1"></a>    ]</span>
<span id="cb19-57"><a href="#cb19-57" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb19-58"><a href="#cb19-58" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-59"><a href="#cb19-59" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a DataFrame</span></span>
<span id="cb19-60"><a href="#cb19-60" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> pd.DataFrame(data)</span>
<span id="cb19-61"><a href="#cb19-61" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-62"><a href="#cb19-62" aria-hidden="true" tabindex="-1"></a><span class="co"># Display the DataFrame</span></span>
<span id="cb19-63"><a href="#cb19-63" aria-hidden="true" tabindex="-1"></a>df</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="60">
<div>


<table class="dataframe table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">Statistics</th>
<th data-quarto-table-cell-role="th">Train set</th>
<th data-quarto-table-cell-role="th">Test set</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">0</td>
<td>Number of reviews</td>
<td>25000.0</td>
<td>25000.0</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">1</td>
<td>Proportion of positive reviews</td>
<td>0.5</td>
<td>0.5</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">2</td>
<td>Proportion of negative reviews</td>
<td>0.5</td>
<td>0.5</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">3</td>
<td>Number of words in positive reviews (Min)</td>
<td>13.0</td>
<td>10.0</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">4</td>
<td>Number of words in positive reviews (Median)</td>
<td>178.0</td>
<td>172.0</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">5</td>
<td>Number of words in positive reviews (Max)</td>
<td>2494.0</td>
<td>2315.0</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">6</td>
<td>Number of words in negative reviews (Min)</td>
<td>11.0</td>
<td>7.0</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">7</td>
<td>Number of words in negative reviews (Median)</td>
<td>179.0</td>
<td>176.0</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">8</td>
<td>Number of words in negative reviews (Max)</td>
<td>1571.0</td>
<td>1095.0</td>
</tr>
</tbody>
</table>

</div>
</div>
</div>
<p>The reviews are equally split between positive and negative reviews for both sets. We can see some differences between positive and negative reviews, especially the minimum and maximum number of words per review. Besides understanding the dataset, the reason for looking at some of these key summary statistics is to determine whether we can engineer certain numeric features and build a simple logistic regression or tree-based classifier for sentiment analysis.</p>
<div class="callout callout-style-default callout-tip callout-titled" title="Prompt">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-2-contents" aria-controls="callout-2" aria-expanded="true" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Prompt
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-2" class="callout-2-contents callout-collapse collapse show">
<div class="callout-body-container callout-body">
<p>Generate histogram of word count per review for positive sentiment and negative sentiment in the training data.</p>
</div>
</div>
</div>
<p>In that vein, let’s examine the distribution of the number of words per review, comparing positive and negative sentiments. Are there any differences in the number of words for positive and negative reviews? And if so, are negative reviews usually longer or shorter than positive reviews? We can answer these questions by looking at <a href="#fig-wdcnt">Figure&nbsp;1</a>.</p>
<div id="fig-wdcnt" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb20"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> tensorflow.keras.datasets <span class="im">import</span> imdb</span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb20-4"><a href="#cb20-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb20-5"><a href="#cb20-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-6"><a href="#cb20-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Load the IMDB dataset</span></span>
<span id="cb20-7"><a href="#cb20-7" aria-hidden="true" tabindex="-1"></a>(train_data, train_labels), _ <span class="op">=</span> imdb.load_data()</span>
<span id="cb20-8"><a href="#cb20-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-9"><a href="#cb20-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Function to get word counts</span></span>
<span id="cb20-10"><a href="#cb20-10" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> word_counts(data, labels, positive<span class="op">=</span><span class="va">True</span>):</span>
<span id="cb20-11"><a href="#cb20-11" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> [<span class="bu">len</span>(review) <span class="cf">for</span> review, label <span class="kw">in</span> <span class="bu">zip</span>(data, labels) <span class="cf">if</span> label <span class="op">==</span> positive]</span>
<span id="cb20-12"><a href="#cb20-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-13"><a href="#cb20-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Get word counts for positive and negative reviews</span></span>
<span id="cb20-14"><a href="#cb20-14" aria-hidden="true" tabindex="-1"></a>train_data_positive <span class="op">=</span> word_counts(train_data, train_labels, positive<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb20-15"><a href="#cb20-15" aria-hidden="true" tabindex="-1"></a>train_data_negative <span class="op">=</span> word_counts(train_data, train_labels, positive<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb20-16"><a href="#cb20-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-17"><a href="#cb20-17" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot histogram for positive reviews</span></span>
<span id="cb20-18"><a href="#cb20-18" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">12</span>, <span class="dv">6</span>))</span>
<span id="cb20-19"><a href="#cb20-19" aria-hidden="true" tabindex="-1"></a>sns.histplot(train_data_positive, bins<span class="op">=</span><span class="dv">50</span>, kde<span class="op">=</span><span class="va">True</span>, color<span class="op">=</span><span class="st">'blue'</span>, label<span class="op">=</span><span class="st">'Positive Reviews'</span>)</span>
<span id="cb20-20"><a href="#cb20-20" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Number of Words'</span>)</span>
<span id="cb20-21"><a href="#cb20-21" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Frequency'</span>)</span>
<span id="cb20-22"><a href="#cb20-22" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Histogram of Word Count per Review (Positive Sentiment)'</span>)</span>
<span id="cb20-23"><a href="#cb20-23" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb20-24"><a href="#cb20-24" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb20-25"><a href="#cb20-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-26"><a href="#cb20-26" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot histogram for negative reviews</span></span>
<span id="cb20-27"><a href="#cb20-27" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">12</span>, <span class="dv">6</span>))</span>
<span id="cb20-28"><a href="#cb20-28" aria-hidden="true" tabindex="-1"></a>sns.histplot(train_data_negative, bins<span class="op">=</span><span class="dv">50</span>, kde<span class="op">=</span><span class="va">True</span>, color<span class="op">=</span><span class="st">'red'</span>, label<span class="op">=</span><span class="st">'Negative Reviews'</span>)</span>
<span id="cb20-29"><a href="#cb20-29" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Number of Words'</span>)</span>
<span id="cb20-30"><a href="#cb20-30" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Frequency'</span>)</span>
<span id="cb20-31"><a href="#cb20-31" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Histogram of Word Count per Review (Negative Sentiment)'</span>)</span>
<span id="cb20-32"><a href="#cb20-32" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb20-33"><a href="#cb20-33" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div id="fig-wdcnt-1" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="Natural_Language_Processing_files/figure-html/fig-wdcnt-output-1.png" class="img-fluid figure-img" data-ref-parent="fig-wdcnt"></p>
<figcaption class="figure-caption">(a) positive sentiment</figcaption>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div id="fig-wdcnt-2" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="Natural_Language_Processing_files/figure-html/fig-wdcnt-output-2.png" class="img-fluid figure-img" data-ref-parent="fig-wdcnt"></p>
<figcaption class="figure-caption">(b) negative sentiment</figcaption>
</figure>
</div>
</div>
<figcaption class="figure-caption">Figure&nbsp;1: Historgram of words count</figcaption>
</figure>
</div>
<p>In <a href="#fig-wdcnt">Figure&nbsp;1</a>, we can see no significant differences between positive and negative reviews in terms of the number of words. Therefore, the number of words in a review does not accurately predict whether the review is positive or negative.</p>
<p>Next, we will build a neural network model. Before training a machine learning model, the dataset needs to be preprocessed. This includes removing frequently occurring words that do not contribute much to the meaning of the text, such as stopwords like “the,” “and,” or “a.” Additionally, most machine learning algorithms require the same number of features, i.e., the same length for each review.</p>
<p>To achieve this, the pad_sequences function sets the maximum number of words in each review (<code>maxlen</code>):</p>
<div class="sourceCode" id="cb21"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a>x_train <span class="op">=</span> pad_sequences(x_train, maxlen<span class="op">=</span>maxlen)</span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a>x_test <span class="op">=</span> pad_sequences(x_test, maxlen<span class="op">=</span>maxlen)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>For reviews that have fewer than <code>maxlen</code> words, the above code pads them with “0” For reviews that have more than <code>maxlen</code> words, the code truncates them.</p>
<div class="cell" data-execution_count="73">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb22"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> keras</span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> keras.datasets <span class="im">import</span> imdb</span>
<span id="cb22-3"><a href="#cb22-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> keras <span class="im">import</span> models, layers</span>
<span id="cb22-4"><a href="#cb22-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> keras.utils <span class="im">import</span> pad_sequences</span>
<span id="cb22-5"><a href="#cb22-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> keras <span class="im">import</span> backend <span class="im">as</span> K</span>
<span id="cb22-6"><a href="#cb22-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-7"><a href="#cb22-7" aria-hidden="true" tabindex="-1"></a>max_features <span class="op">=</span> <span class="dv">10000</span></span>
<span id="cb22-8"><a href="#cb22-8" aria-hidden="true" tabindex="-1"></a>maxlen <span class="op">=</span> <span class="dv">20</span></span>
<span id="cb22-9"><a href="#cb22-9" aria-hidden="true" tabindex="-1"></a>(x_train, y_train), (x_test, y_test) <span class="op">=</span> imdb.load_data(num_words<span class="op">=</span>max_features)</span>
<span id="cb22-10"><a href="#cb22-10" aria-hidden="true" tabindex="-1"></a>x_train <span class="op">=</span> pad_sequences(x_train, maxlen<span class="op">=</span>maxlen)</span>
<span id="cb22-11"><a href="#cb22-11" aria-hidden="true" tabindex="-1"></a>x_test <span class="op">=</span> pad_sequences(x_test, maxlen<span class="op">=</span>maxlen)</span>
<span id="cb22-12"><a href="#cb22-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-13"><a href="#cb22-13" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> models.Sequential()</span>
<span id="cb22-14"><a href="#cb22-14" aria-hidden="true" tabindex="-1"></a>model.add(layers.Embedding(input_dim<span class="op">=</span>max_features, output_dim<span class="op">=</span><span class="dv">8</span>))</span>
<span id="cb22-15"><a href="#cb22-15" aria-hidden="true" tabindex="-1"></a>model.add(layers.Flatten())</span>
<span id="cb22-16"><a href="#cb22-16" aria-hidden="true" tabindex="-1"></a>model.add(layers.Dense(<span class="dv">1</span>, activation<span class="op">=</span><span class="st">'sigmoid'</span>))</span>
<span id="cb22-17"><a href="#cb22-17" aria-hidden="true" tabindex="-1"></a>model.<span class="bu">compile</span>(optimizer<span class="op">=</span><span class="st">'rmsprop'</span>, loss<span class="op">=</span><span class="st">'binary_crossentropy'</span>, metrics<span class="op">=</span>[<span class="st">'acc'</span>])</span>
<span id="cb22-18"><a href="#cb22-18" aria-hidden="true" tabindex="-1"></a>history <span class="op">=</span> model.fit(x_train, y_train, epochs<span class="op">=</span><span class="dv">10</span>, batch_size<span class="op">=</span><span class="dv">32</span>, validation_split<span class="op">=</span><span class="fl">0.2</span>, verbose<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb22-19"><a href="#cb22-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-20"><a href="#cb22-20" aria-hidden="true" tabindex="-1"></a><span class="co"># show model summary</span></span>
<span id="cb22-21"><a href="#cb22-21" aria-hidden="true" tabindex="-1"></a>model.summary()</span>
<span id="cb22-22"><a href="#cb22-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-23"><a href="#cb22-23" aria-hidden="true" tabindex="-1"></a><span class="co"># show fitting information</span></span>
<span id="cb22-24"><a href="#cb22-24" aria-hidden="true" tabindex="-1"></a><span class="co"># vars(history)</span></span>
<span id="cb22-25"><a href="#cb22-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-26"><a href="#cb22-26" aria-hidden="true" tabindex="-1"></a><span class="co"># # Create a Keras model with input and all layers' output</span></span>
<span id="cb22-27"><a href="#cb22-27" aria-hidden="true" tabindex="-1"></a><span class="co"># layer_outputs = [layer.output for layer in model.layers]</span></span>
<span id="cb22-28"><a href="#cb22-28" aria-hidden="true" tabindex="-1"></a><span class="co"># model_with_output = keras.Model(inputs=model.inputs, outputs=layer_outputs)</span></span>
<span id="cb22-29"><a href="#cb22-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-30"><a href="#cb22-30" aria-hidden="true" tabindex="-1"></a><span class="co"># # Compute the outputs of all layers for the input tensor</span></span>
<span id="cb22-31"><a href="#cb22-31" aria-hidden="true" tabindex="-1"></a><span class="co"># outputs = model_with_output(x_train)</span></span>
<span id="cb22-32"><a href="#cb22-32" aria-hidden="true" tabindex="-1"></a><span class="co"># weights = [layer.get_weights() for layer in model.layers]</span></span>
<span id="cb22-33"><a href="#cb22-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-34"><a href="#cb22-34" aria-hidden="true" tabindex="-1"></a><span class="co"># Evaluate the model on test data</span></span>
<span id="cb22-35"><a href="#cb22-35" aria-hidden="true" tabindex="-1"></a>test_loss, test_acc <span class="op">=</span> model.evaluate(x_test, y_test)</span>
<span id="cb22-36"><a href="#cb22-36" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Test accuracy: </span><span class="sc">{</span>test_acc<span class="sc">:.4f}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold">Model: "sequential_5"</span>
</pre>
</div>
<div class="cell-output cell-output-display">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓
┃<span style="font-weight: bold"> Layer (type)                    </span>┃<span style="font-weight: bold"> Output Shape           </span>┃<span style="font-weight: bold">       Param # </span>┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩
│ embedding_5 (<span style="color: #0087ff; text-decoration-color: #0087ff">Embedding</span>)         │ (<span style="color: #00af00; text-decoration-color: #00af00">32</span>, <span style="color: #00af00; text-decoration-color: #00af00">20</span>, <span style="color: #00af00; text-decoration-color: #00af00">8</span>)            │        <span style="color: #00af00; text-decoration-color: #00af00">80,000</span> │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ flatten_5 (<span style="color: #0087ff; text-decoration-color: #0087ff">Flatten</span>)             │ (<span style="color: #00af00; text-decoration-color: #00af00">32</span>, <span style="color: #00af00; text-decoration-color: #00af00">160</span>)              │             <span style="color: #00af00; text-decoration-color: #00af00">0</span> │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ dense_5 (<span style="color: #0087ff; text-decoration-color: #0087ff">Dense</span>)                 │ (<span style="color: #00af00; text-decoration-color: #00af00">32</span>, <span style="color: #00af00; text-decoration-color: #00af00">1</span>)                │           <span style="color: #00af00; text-decoration-color: #00af00">161</span> │
└─────────────────────────────────┴────────────────────────┴───────────────┘
</pre>
</div>
<div class="cell-output cell-output-display">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold"> Total params: </span><span style="color: #00af00; text-decoration-color: #00af00">160,324</span> (626.27 KB)
</pre>
</div>
<div class="cell-output cell-output-display">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold"> Trainable params: </span><span style="color: #00af00; text-decoration-color: #00af00">80,161</span> (313.13 KB)
</pre>
</div>
<div class="cell-output cell-output-display">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold"> Non-trainable params: </span><span style="color: #00af00; text-decoration-color: #00af00">0</span> (0.00 B)
</pre>
</div>
<div class="cell-output cell-output-display">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold"> Optimizer params: </span><span style="color: #00af00; text-decoration-color: #00af00">80,163</span> (313.14 KB)
</pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>782/782 ━━━━━━━━━━━━━━━━━━━━ 1s 834us/step - acc: 0.7531 - loss: 0.5272
Test accuracy: 0.7540</code></pre>
</div>
</div>
<p>This very simple model achieves an accuracy of 0.75, meaning it correctly predicts the sentiment of 75% of the test data. There are many ways to improve the model using advanced techniques such as RNN (Recurrent Neural Networks), LSTM (Long Short-Term Memory), GRU (Gated Recurrent Units), or Transformers, which is the most popular architecture used in many AI models today.</p>
<p>Transformers have revolutionized the field of natural language processing (NLP). They use a mechanism called self-attention to process and generate sequences of words, allowing them to handle long-range dependencies in text more effectively than RNNs and LSTMs. Transformers are the backbone of many state-of-the-art models, such as BERT (Bidirectional Encoder Representations from Transformers) and GPT (Generative Pre-trained Transformer), which have achieved remarkable results in various NLP tasks.</p>
</section>
<section id="recommendation-system" class="level2">
<h2 class="anchored" data-anchor-id="recommendation-system">Recommendation System</h2>
<p>Next, we will demonstrate a cool application of embeddings to create a recommendation system. The retail data used to train our model contains the following information:</p>
<ul>
<li><p>InvoiceNo</p></li>
<li><p>StockCode</p></li>
<li><p>Description</p></li>
<li><p>Quantity</p></li>
<li><p>InvoiceDate</p></li>
<li><p>UnitPrice</p></li>
<li><p>CustomerID</p></li>
<li><p>Country</p></li>
</ul>
<p>The data is available at <a href="https://raw.githubusercontent.com/ming-zhao/ming-zhao.github.io/master/AIML_for_Business/data/retail.csv">link</a>.</p>
<p>Upload the data to ChatGPT, and AI can be utilized to generate the summary statistics of the dataset.</p>
<div class="callout callout-style-default callout-tip callout-titled" title="Prompt">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-3-contents" aria-controls="callout-3" aria-expanded="true" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Prompt
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-3" class="callout-3-contents callout-collapse collapse show">
<div class="callout-body-container callout-body">
<p>Show a few rows of the data and generate summary statistics for each column, including measures such as count, mean, standard deviation, minimum, and maximum values.</p>
</div>
</div>
</div>
<div class="cell" data-execution_count="77">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb24"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> random</span>
<span id="cb24-3"><a href="#cb24-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-4"><a href="#cb24-4" aria-hidden="true" tabindex="-1"></a>base_url <span class="op">=</span> (</span>
<span id="cb24-5"><a href="#cb24-5" aria-hidden="true" tabindex="-1"></a>    <span class="st">"https://raw.githubusercontent.com/ming-zhao/ming-zhao.github.io"</span></span>
<span id="cb24-6"><a href="#cb24-6" aria-hidden="true" tabindex="-1"></a>    <span class="st">"/master/AIML_for_Business"</span></span>
<span id="cb24-7"><a href="#cb24-7" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb24-8"><a href="#cb24-8" aria-hidden="true" tabindex="-1"></a>data <span class="op">=</span> pd.read_csv(base_url <span class="op">+</span> <span class="st">"/data/retail.csv"</span>, dtype<span class="op">=</span>{<span class="st">'CustomerID'</span>: <span class="bu">str</span>}, parse_dates<span class="op">=</span>[<span class="st">'InvoiceDate'</span>])</span>
<span id="cb24-9"><a href="#cb24-9" aria-hidden="true" tabindex="-1"></a>data.head()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="77">
<div>


<table class="dataframe table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">InvoiceNo</th>
<th data-quarto-table-cell-role="th">StockCode</th>
<th data-quarto-table-cell-role="th">Description</th>
<th data-quarto-table-cell-role="th">Quantity</th>
<th data-quarto-table-cell-role="th">InvoiceDate</th>
<th data-quarto-table-cell-role="th">UnitPrice</th>
<th data-quarto-table-cell-role="th">CustomerID</th>
<th data-quarto-table-cell-role="th">Country</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">0</td>
<td>536365</td>
<td>85123A</td>
<td>white hanging heart t-light holder</td>
<td>6</td>
<td>2010-12-01 08:26:00</td>
<td>2.55</td>
<td>17850</td>
<td>United Kingdom</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">1</td>
<td>536365</td>
<td>71053</td>
<td>white metal lantern</td>
<td>6</td>
<td>2010-12-01 08:26:00</td>
<td>3.39</td>
<td>17850</td>
<td>United Kingdom</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">2</td>
<td>536365</td>
<td>84406B</td>
<td>cream cupid hearts coat hanger</td>
<td>8</td>
<td>2010-12-01 08:26:00</td>
<td>2.75</td>
<td>17850</td>
<td>United Kingdom</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">3</td>
<td>536365</td>
<td>84029G</td>
<td>knitted union flag hot water bottle</td>
<td>6</td>
<td>2010-12-01 08:26:00</td>
<td>3.39</td>
<td>17850</td>
<td>United Kingdom</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">4</td>
<td>536365</td>
<td>84029E</td>
<td>red woolly hottie white heart.</td>
<td>6</td>
<td>2010-12-01 08:26:00</td>
<td>3.39</td>
<td>17850</td>
<td>United Kingdom</td>
</tr>
</tbody>
</table>

</div>
</div>
</div>
<div class="cell" data-execution_count="78">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb25"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a>data.describe(include<span class="op">=</span><span class="st">'all'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="78">
<div>


<table class="dataframe table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">InvoiceNo</th>
<th data-quarto-table-cell-role="th">StockCode</th>
<th data-quarto-table-cell-role="th">Description</th>
<th data-quarto-table-cell-role="th">Quantity</th>
<th data-quarto-table-cell-role="th">InvoiceDate</th>
<th data-quarto-table-cell-role="th">UnitPrice</th>
<th data-quarto-table-cell-role="th">CustomerID</th>
<th data-quarto-table-cell-role="th">Country</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">count</td>
<td>406829</td>
<td>406829</td>
<td>406829</td>
<td>406829.000000</td>
<td>406829</td>
<td>406829.000000</td>
<td>406829</td>
<td>406829</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">unique</td>
<td>22190</td>
<td>3684</td>
<td>3896</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>4372</td>
<td>37</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">top</td>
<td>576339</td>
<td>85123A</td>
<td>white hanging heart t-light holder</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>17841</td>
<td>United Kingdom</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">freq</td>
<td>542</td>
<td>2077</td>
<td>2070</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>7983</td>
<td>361878</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">mean</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>12.061303</td>
<td>2011-07-10 16:30:57.879207424</td>
<td>3.460471</td>
<td>NaN</td>
<td>NaN</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">min</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>-80995.000000</td>
<td>2010-12-01 08:26:00</td>
<td>0.000000</td>
<td>NaN</td>
<td>NaN</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">25%</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>2.000000</td>
<td>2011-04-06 15:02:00</td>
<td>1.250000</td>
<td>NaN</td>
<td>NaN</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">50%</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>5.000000</td>
<td>2011-07-31 11:48:00</td>
<td>1.950000</td>
<td>NaN</td>
<td>NaN</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">75%</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>12.000000</td>
<td>2011-10-20 13:06:00</td>
<td>3.750000</td>
<td>NaN</td>
<td>NaN</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">max</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>80995.000000</td>
<td>2011-12-09 12:50:00</td>
<td>38970.000000</td>
<td>NaN</td>
<td>NaN</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">std</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>248.693370</td>
<td>NaN</td>
<td>69.315162</td>
<td>NaN</td>
<td>NaN</td>
</tr>
</tbody>
</table>

</div>
</div>
</div>
<p>We will collect the products purchased by each customer (identified by CustomerID). Our goal is to build a model that understands why customers buy certain products together. Once the model is trained, it can recommend products to future customers based on what it has learned about customer behavior. To do this, we will randomly select 90% of the customers for training and 10% for testing.</p>
<p>Both the training and testing data will be in the form of lists of lists, like this: [[a1, a2], [b1, b2, b3], …], where a1 and a2 are StockCode in the data (or products) bought by one customer, and b1, b2, and b3 are products bought by another customer, and so on.</p>
<p>In our model, we treat a1, a2, b1, b2, b3, etc., as words, and [a1, a2], [b1, b2, b3] as sentences. In language, the meaning of individual words can be unclear, but sentences give more context and clarity. Similarly, it is difficult to understand a customer’s behavior from a single product, but by looking at a list of products they bought together, we get a clearer picture of their preferences. Therefore, we use word embedding to convert each StockCode (product) into vectors. We again use the popular pre-trained embedding model, <strong>Word2Vec</strong>, for our task.</p>
<div class="cell">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb26"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> random</span>
<span id="cb26-3"><a href="#cb26-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> gensim.models <span class="im">import</span> Word2Vec</span>
<span id="cb26-4"><a href="#cb26-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-5"><a href="#cb26-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Load the dataset with correct data types</span></span>
<span id="cb26-6"><a href="#cb26-6" aria-hidden="true" tabindex="-1"></a>file_path <span class="op">=</span> base_url <span class="op">+</span> <span class="st">"/data/retail.csv"</span></span>
<span id="cb26-7"><a href="#cb26-7" aria-hidden="true" tabindex="-1"></a>data <span class="op">=</span> pd.read_csv(file_path, dtype<span class="op">=</span>{<span class="st">'CustomerID'</span>: <span class="bu">str</span>}, parse_dates<span class="op">=</span>[<span class="st">'InvoiceDate'</span>])</span>
<span id="cb26-8"><a href="#cb26-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-9"><a href="#cb26-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Shuffle customers and split into training and testing sets</span></span>
<span id="cb26-10"><a href="#cb26-10" aria-hidden="true" tabindex="-1"></a>customers <span class="op">=</span> data.CustomerID.unique().tolist()</span>
<span id="cb26-11"><a href="#cb26-11" aria-hidden="true" tabindex="-1"></a>random.shuffle(customers)</span>
<span id="cb26-12"><a href="#cb26-12" aria-hidden="true" tabindex="-1"></a>split_point <span class="op">=</span> <span class="bu">round</span>(<span class="fl">0.9</span> <span class="op">*</span> <span class="bu">len</span>(customers))</span>
<span id="cb26-13"><a href="#cb26-13" aria-hidden="true" tabindex="-1"></a>customers_train <span class="op">=</span> customers[:split_point]</span>
<span id="cb26-14"><a href="#cb26-14" aria-hidden="true" tabindex="-1"></a>customers_test <span class="op">=</span> customers[split_point:]</span>
<span id="cb26-15"><a href="#cb26-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-16"><a href="#cb26-16" aria-hidden="true" tabindex="-1"></a><span class="co"># List to capture purchase history of the customers</span></span>
<span id="cb26-17"><a href="#cb26-17" aria-hidden="true" tabindex="-1"></a>train_data <span class="op">=</span> []</span>
<span id="cb26-18"><a href="#cb26-18" aria-hidden="true" tabindex="-1"></a>longest <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb26-19"><a href="#cb26-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-20"><a href="#cb26-20" aria-hidden="true" tabindex="-1"></a><span class="co"># Populate the list with the product codes for training data</span></span>
<span id="cb26-21"><a href="#cb26-21" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> customer <span class="kw">in</span> customers_train:</span>
<span id="cb26-22"><a href="#cb26-22" aria-hidden="true" tabindex="-1"></a>    purchase_history <span class="op">=</span> data[data[<span class="st">"CustomerID"</span>] <span class="op">==</span> customer][<span class="st">"StockCode"</span>].tolist()</span>
<span id="cb26-23"><a href="#cb26-23" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="bu">len</span>(purchase_history) <span class="op">&gt;</span> longest:</span>
<span id="cb26-24"><a href="#cb26-24" aria-hidden="true" tabindex="-1"></a>        longest <span class="op">=</span> <span class="bu">len</span>(purchase_history)</span>
<span id="cb26-25"><a href="#cb26-25" aria-hidden="true" tabindex="-1"></a>    train_data.append(purchase_history)</span>
<span id="cb26-26"><a href="#cb26-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-27"><a href="#cb26-27" aria-hidden="true" tabindex="-1"></a><span class="co"># Train Word2Vec model</span></span>
<span id="cb26-28"><a href="#cb26-28" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> Word2Vec(sentences<span class="op">=</span>train_data, </span>
<span id="cb26-29"><a href="#cb26-29" aria-hidden="true" tabindex="-1"></a>                 vector_size<span class="op">=</span><span class="dv">100</span>, </span>
<span id="cb26-30"><a href="#cb26-30" aria-hidden="true" tabindex="-1"></a>                 window<span class="op">=</span>longest, </span>
<span id="cb26-31"><a href="#cb26-31" aria-hidden="true" tabindex="-1"></a>                 min_count<span class="op">=</span><span class="dv">1</span>, </span>
<span id="cb26-32"><a href="#cb26-32" aria-hidden="true" tabindex="-1"></a>                 workers<span class="op">=</span><span class="dv">4</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>After the model is trained, here’s an example showing how one of the products is converted into a vector:</p>
<div class="cell" data-execution_count="89">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb27"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a>product <span class="op">=</span> <span class="st">'21506'</span></span>
<span id="cb27-2"><a href="#cb27-2" aria-hidden="true" tabindex="-1"></a>description <span class="op">=</span> data[data[<span class="st">'StockCode'</span>] <span class="op">==</span> product][<span class="st">'Description'</span>].values[<span class="dv">0</span>]</span>
<span id="cb27-3"><a href="#cb27-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"StockCode: </span><span class="sc">{</span>product<span class="sc">}</span><span class="ss">, Description: </span><span class="sc">{</span>description<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb27-4"><a href="#cb27-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(model.wv.get_vector(product))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>StockCode: 21506, Description: fancy font birthday card, 
[-2.0500734   1.0486172   1.3093923  -1.3016508   6.065899   -2.291378
  1.625014    0.9822313  -1.4804991   2.3123453   4.5394635  -0.3166731
  1.9122493  -0.40571445  2.5464752   0.50067955  1.4677818  -0.40851653
  0.11472506 -1.5560524   2.332762   -1.6304573  -3.975914   -0.08979146
  0.9976462   1.557895    1.6055717  -4.407984   -0.31686464  1.5440747
 -2.906915   -1.0046779   3.1040351   0.88720477 -1.5394729  -0.8074166
 -4.079725    0.16955534 -0.01583755 -1.7557445  -1.0197153   1.8602298
 -0.0090165  -0.29118708  0.75320345 -1.5049409   3.26076    -0.73739696
  0.4466099  -0.15214975  0.96058583  1.5366901   0.02684127 -0.8573799
  0.46368486  1.3442484  -0.34360936 -1.894092    3.6738908   0.3427885
 -0.4298368   0.5521264   1.630709   -2.15073    -0.18831438 -3.8535957
  3.041407    1.6407119  -1.6451669  -0.72795296 -3.4875154  -0.14118685
  0.682361   -0.48886672  2.0331926  -1.2859696   2.0761204  -4.4447527
 -3.6754174  -1.9851053   0.8289878   0.61798036  2.1080215   1.0477064
  2.6741323  -1.321872    3.538619    3.4425843   0.7565658   3.9086444
  1.5500315   1.864594   -0.9064445  -3.9561682   1.4227905   5.174811
  2.102107    0.36084726  3.6684291  -0.29732457]</code></pre>
</div>
</div>
<p>The example example shows a set of similar products suggested by our model, along with their StockCode, Description, and Similarity score:</p>
<div class="cell" data-execution_count="90">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb29"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a>example_product <span class="op">=</span> <span class="st">'21506'</span></span>
<span id="cb29-2"><a href="#cb29-2" aria-hidden="true" tabindex="-1"></a>similar_products <span class="op">=</span> model.wv.most_similar(example_product)</span>
<span id="cb29-3"><a href="#cb29-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Products similar to </span><span class="sc">{</span>example_product<span class="sc">}</span><span class="ss"> (Description: </span><span class="sc">{</span>data[data[<span class="st">'StockCode'</span>] <span class="op">==</span> example_product][<span class="st">'Description'</span>]<span class="sc">.</span>values[<span class="dv">0</span>]<span class="sc">}</span><span class="ss">):"</span>)</span>
<span id="cb29-4"><a href="#cb29-4" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> product, similarity <span class="kw">in</span> similar_products:</span>
<span id="cb29-5"><a href="#cb29-5" aria-hidden="true" tabindex="-1"></a>    description <span class="op">=</span> data[data[<span class="st">'StockCode'</span>] <span class="op">==</span> product][<span class="st">'Description'</span>].values[<span class="dv">0</span>]</span>
<span id="cb29-6"><a href="#cb29-6" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"StockCode: </span><span class="sc">{</span>product<span class="sc">}</span><span class="ss">, Description: </span><span class="sc">{</span>description<span class="sc">}</span><span class="ss">, Similarity: </span><span class="sc">{</span>similarity<span class="sc">:.2f}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Products similar to 21506 (Description: fancy font birthday card, ):
StockCode: 22030, Description: swallows greeting card, Similarity: 0.93
StockCode: 22037, Description: robot birthday card, Similarity: 0.88
StockCode: 22983, Description: card billboard font, Similarity: 0.87
StockCode: 22026, Description: banquet birthday  card  , Similarity: 0.84
StockCode: 22718, Description: card cat and tree , Similarity: 0.82
StockCode: 22047, Description: empire gift wrap, Similarity: 0.80
StockCode: 22029, Description: spaceboy birthday card, Similarity: 0.79
StockCode: 21497, Description: fancy fonts birthday wrap, Similarity: 0.79
StockCode: 22027, Description: tea party birthday card, Similarity: 0.78
StockCode: 22815, Description: card psychedelic apples, Similarity: 0.77</code></pre>
</div>
</div>
<p>Since the target product is a birthday card, the model finds that greeting cards have the highest similarity. This is quite amazing, considering that the training data we fed into the model is simply the StockCode, which to us (humans) has no inherent meaning. The model learns the relationships between products purely through the sets of StockCodes that are purchased together.</p>
<p>With this embedding model, we can build a recommendation system. For a basic example, we can predict the next purchase based on a given purchase history. The example shows that for a customer in the testing data, we use his/herIt is certainly debatable if this is the best logic to use for recommendations. It is also worth noting that this model is probably too simplistic to be deployed in a real-world scenario. Nevertheless, this example effectively demonstrates the application of embeddings, and many recommendation systems in real businesses are indeed built using embedding methods. first purchase to predict a future purchase, and this predicted purchase is indeed among the items the customer bought.</p>
<div class="cell" data-execution_count="122">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb31"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Example: Predicting products for a given purchase history</span></span>
<span id="cb31-2"><a href="#cb31-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> predict_next_purchase(purchase_history):</span>
<span id="cb31-3"><a href="#cb31-3" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Use the model to predict the next likely product, excluding the product itself</span></span>
<span id="cb31-4"><a href="#cb31-4" aria-hidden="true" tabindex="-1"></a>    vector <span class="op">=</span> model.wv[purchase_history]</span>
<span id="cb31-5"><a href="#cb31-5" aria-hidden="true" tabindex="-1"></a>    similar_products <span class="op">=</span> model.wv.most_similar(positive<span class="op">=</span>vector, topn<span class="op">=</span><span class="dv">10</span>)</span>
<span id="cb31-6"><a href="#cb31-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> product, similarity <span class="kw">in</span> similar_products:</span>
<span id="cb31-7"><a href="#cb31-7" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> product <span class="kw">not</span> <span class="kw">in</span> purchase_history:</span>
<span id="cb31-8"><a href="#cb31-8" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> product</span>
<span id="cb31-9"><a href="#cb31-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="va">None</span> </span>
<span id="cb31-10"><a href="#cb31-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-11"><a href="#cb31-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Predicting the next product for the first customer in the test set</span></span>
<span id="cb31-12"><a href="#cb31-12" aria-hidden="true" tabindex="-1"></a>test_customer_index <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb31-13"><a href="#cb31-13" aria-hidden="true" tabindex="-1"></a>first_product <span class="op">=</span> test_data[test_customer_index][<span class="dv">0</span>]</span>
<span id="cb31-14"><a href="#cb31-14" aria-hidden="true" tabindex="-1"></a>first_description <span class="op">=</span> data[data[<span class="st">'StockCode'</span>]<span class="op">==</span>first_product][<span class="st">'Description'</span>].values[<span class="dv">0</span>]</span>
<span id="cb31-15"><a href="#cb31-15" aria-hidden="true" tabindex="-1"></a>first_test_customer_history <span class="op">=</span> test_data[test_customer_index]</span>
<span id="cb31-16"><a href="#cb31-16" aria-hidden="true" tabindex="-1"></a>predicted_product <span class="op">=</span> predict_next_purchase(first_product)</span>
<span id="cb31-17"><a href="#cb31-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-18"><a href="#cb31-18" aria-hidden="true" tabindex="-1"></a><span class="co"># Show the product description for the predicted product</span></span>
<span id="cb31-19"><a href="#cb31-19" aria-hidden="true" tabindex="-1"></a>predicted_description <span class="op">=</span> data[data[<span class="st">'StockCode'</span>] <span class="op">==</span> predicted_product][<span class="st">'Description'</span>].values[<span class="dv">0</span>]</span>
<span id="cb31-20"><a href="#cb31-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-21"><a href="#cb31-21" aria-hidden="true" tabindex="-1"></a><span class="co"># Check if the predicted product was actually purchased by the customer</span></span>
<span id="cb31-22"><a href="#cb31-22" aria-hidden="true" tabindex="-1"></a>actual_purchase <span class="op">=</span> predicted_product <span class="kw">in</span> first_test_customer_history</span>
<span id="cb31-23"><a href="#cb31-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-24"><a href="#cb31-24" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"For customerID = </span><span class="sc">{</span>customers_test[test_customer_index]<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb31-25"><a href="#cb31-25" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"The first purchased product is StockCode=</span><span class="sc">{</span>first_product<span class="sc">}</span><span class="ss"> (Description: </span><span class="sc">{</span>first_description<span class="sc">}</span><span class="ss">)"</span>)</span>
<span id="cb31-26"><a href="#cb31-26" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Predicted product is StockCode=</span><span class="sc">{</span>predicted_product<span class="sc">}</span><span class="ss"> (Description: </span><span class="sc">{</span>predicted_description<span class="sc">}</span><span class="ss">)"</span>)</span>
<span id="cb31-27"><a href="#cb31-27" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Was the predicted product actually purchased by the customer? </span><span class="sc">{</span><span class="st">'Yes'</span> <span class="cf">if</span> actual_purchase <span class="cf">else</span> <span class="st">'No'</span><span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>For customerID = 13261
The first purchased product is StockCode=48129 (Description: doormat topiary)
Predicted product is StockCode=48188 (Description: doormat welcome puppies)
Was the predicted product actually purchased by the customer? Yes</code></pre>
</div>
</div>
<p>It is certainly debatable if this is the best logic to use for recommendations. It is also worth noting that this model is probably too simplistic to be deployed in a real-world scenario, especially considering that we only trained the model for about 10 minutes. Nevertheless, this example effectively demonstrates the application of embeddings, and many recommendation systems in real businesses are indeed built using embedding methods.</p>
</section>
</section>

</main>
<!-- /main column -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->



</body></html>